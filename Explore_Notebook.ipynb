{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27054106-5f21-4f2a-a320-a24b0e6d0b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import astropy.units as u\n",
    "from scipy import ndimage, optimize, stats, interpolate\n",
    "from skimage import morphology, filters, exposure, measure\n",
    "from astropy.io import fits\n",
    "from datetime import datetime\n",
    "from matplotlib import colormaps, ticker, colors\n",
    "\n",
    "import sunpy.map\n",
    "from sunpy.coordinates import frames\n",
    "from astropy.coordinates import SkyCoord\n",
    "from sunpy.map.maputils import all_coordinates_from_map, coordinate_is_on_solar_disk\n",
    "\n",
    "\n",
    "import prepare_data\n",
    "import detect\n",
    "import plot_detection\n",
    "from settings import *\n",
    "\n",
    "pio.renderers.default = 'vscode'\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1324d19-b50d-43d3-8875-f4f0ea184bcb",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f7089cf",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdf30f18",
   "metadata": {},
   "source": [
    "Extract Data from File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6598a-c954-4bcc-9383-d6ab03b6364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract He I observation datetimes from FITS files\n",
    "HE_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    ")\n",
    "\n",
    "# Extract magnetogram datetimes from 6302l FITS files\n",
    "MAG_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, ALL_MAG_DIR, SELECT_MAG_DIR\n",
    ")\n",
    "\n",
    "# Extract EUV datetimes from FITS files\n",
    "EUV_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, ALL_EUV_DIR, SELECT_EUV_DIR\n",
    ")\n",
    "\n",
    "date_strs = [HE_DATE_LIST[0], HE_DATE_LIST[-1]]\n",
    "file_date_str = f'{date_strs[0]}_{date_strs[-1]}'\n",
    "\n",
    "num_maps = len(HE_DATE_LIST)\n",
    "datetimes = [datetime.strptime(date_str, DICT_DATE_STR_FORMAT)\n",
    "             for date_str in date_strs]\n",
    "title_date_strs = [datetime.strftime(d, '%m/%d/%Y') for d in datetimes]\n",
    "DATE_RANGE_SUPTITLE = (f'{num_maps} Maps Evaluated from '\n",
    "                       + f'{title_date_strs[0]} to {title_date_strs[-1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6181f045-b741-4915-a11b-e6e127064fab",
   "metadata": {},
   "source": [
    "## Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53e85c-08e3-4ea4-b224-2f95076b967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for He I Observations:')\n",
    "prepare_data.display_dates(HE_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for Magnetograms:')\n",
    "prepare_data.display_dates(MAG_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for EUV Observations:')\n",
    "prepare_data.display_dates(EUV_DATE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPVT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nispdata.nso.edu/ftp/kpvt/daily/medres/ \n",
    "fits_path = TEST_HE_DIR + '011219mag.fits'\n",
    "kpvt_mag_disk_med_res_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/daily/raw/\n",
    "kpvt_he_fits_path = TEST_HE_DIR + '01dec19h.fits'\n",
    "kpvt_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    kpvt_he_fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/daily/raw/\n",
    "kpvt_mag_fits_path = TEST_HE_DIR + '01dec19m.fits'\n",
    "kpvt_mag_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    kpvt_mag_fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/synoptic/hel.hires/\n",
    "fits_path = TEST_HE_DIR + 'hB1984.fits'\n",
    "kpvt_he_synoptic_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE9'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# /ftp/kpvt/synoptic/helium\n",
    "fits_path = TEST_HE_DIR + 'h1984.fits'\n",
    "kpvt_he_synoptic_low_res_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['2001_12_19', 'CR1984']\n",
    "plot_detection.plot_hists(\n",
    "    [kpvt_he_disk_img, kpvt_he_synoptic_img], titles, semilogy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_he_disk_img, vmin=-200, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_mag_disk_img, vmin=-50, vmax=50, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_he_synoptic_low_res_img, vmin=-300, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_path = ALL_HE_DIR + '2009_10_21__17_50.fts'\n",
    "rockwell_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = ALL_HE_DIR + '2011_03_28__17_35.fts'\n",
    "sarnoff_2011_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = SELECT_HE_DIR + '2015_03_31__18_13.fts'\n",
    "sarnoff_2015_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = TEST_HE_DIR + 'kbv2g150410t1408c2162_000_int-mas_dim-900.fits'\n",
    "sarnoff_2015_he_synoptic_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['2015_03_31__18_13', 'CR2162']\n",
    "plot_detection.plot_hists(\n",
    "    [sarnoff_2015_he_disk_img, sarnoff_2015_he_synoptic_img],\n",
    "    titles, semilogy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detection.plot_images(\n",
    "    image_list=[rockwell_he_disk_img, sarnoff_2011_he_disk_img],\n",
    "    title_list=['2009_10_21__17_50', '2011_03_28__17_35']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rockwell_he_disk_img, vmin=-100, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Single Map Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPVT: H&H\n",
    "# he_date_str = '2003_07_14__18_07'\n",
    "\n",
    "# Rockwell: Null detection\n",
    "# he_date_str = '2004_11_20__17_07'\n",
    "\n",
    "# Rockwell: Smiley\n",
    "# he_date_str = '2004_12_03__16_36'\n",
    "\n",
    "# Sarnoff: Pre-updated FITS\n",
    "# he_date_str = '2012_04_01__17_03'\n",
    "\n",
    "# Sarnoff: Start of Updated FITS in May\n",
    "# he_date_str = '2012_05_01__18_08'\n",
    "\n",
    "# Sarnoff: Disk center neutral line candidate\n",
    "he_date_str = '2012_06_11__18_01'\n",
    "\n",
    "# Sarnoff: East limb hammer\n",
    "# he_date_str = '2012_06_28__16_44'\n",
    "\n",
    "# Failed limb detection\n",
    "# he_date_str = '2012_07_08__19_37'\n",
    "\n",
    "# Sarnoff: Greatest area in 06/2012\n",
    "# he_date_str = '2012_06_09__19_20'\n",
    "\n",
    "# he_date_str = HE_DATE_LIST[0]\n",
    "\n",
    "he_fits_file = prepare_data.get_fits_path(\n",
    "    he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    ")\n",
    "he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "he_map_data = np.flipud(he_map.data)\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "# fig = plt.figure(figsize=(12,12))\n",
    "ax = plot_detection.plot_he_map(fig, (1, 1, 1), he_map, he_date_str)\n",
    "# ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = np.where(he_map_data.flatten() == 0, np.nan, he_map_data.flatten())\n",
    "edges = np.arange(-100, 100, 12.5)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "plt.figure(figsize=(4,3))\n",
    "data = plt.hist(\n",
    "    hist_data, edges, histtype='step',\n",
    "    color='white', edgecolor='black', linewidth=3,\n",
    ")\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Equivalent Width (mAngstroms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_date_str = prepare_data.get_nearest_date_str(\n",
    "    MAG_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Extract magnetogram\n",
    "mag_fits_file = prepare_data.get_fits_path(\n",
    "   mag_date_str, DATE_RANGE, ALL_MAG_DIR, SELECT_MAG_DIR\n",
    ")\n",
    "mag_map = prepare_data.get_nso_sunpy_map(mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=mag_map)\n",
    "mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "\n",
    "# fig = plt.figure(figsize=(7,7))\n",
    "# ax = fig.add_subplot(111, projection=mag_map)\n",
    "# display_mag_data = np.where(mag_map.data == 0, 50, mag_map.data)\n",
    "# display_mag_map = sunpy.map.Map(display_mag_data, mag_map.meta)\n",
    "# display_mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euv_date_str = prepare_data.get_nearest_date_str(\n",
    "    EUV_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Extract magnetogram\n",
    "euv_fits_file = prepare_data.get_fits_path(\n",
    "   euv_date_str, DATE_RANGE, ALL_EUV_DIR, SELECT_EUV_DIR\n",
    ")\n",
    "euv_map = sunpy.map.Map(euv_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plot_detection.plot_euv_map(fig, (1, 1, 1), euv_map, euv_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processed Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.5.1: Extract FITS file pre-processed map\n",
    "pre_process_fits_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "pre_processed_map = sunpy.map.Map(pre_process_fits_file)\n",
    "pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=pre_processed_map)\n",
    "pre_processed_map.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract differentially rotated magnetogram map\n",
    "reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                         f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "\n",
    "# fig = plt.figure(figsize=(12,12))\n",
    "# ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "# ax.set_axis_off()\n",
    "# display_mag_data = np.where(np.isnan(reprojected_mag_map.data), -50, reprojected_mag_map.data)\n",
    "# display_mag_map = sunpy.map.Map(display_mag_data, reprojected_mag_map.meta)\n",
    "# display_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.1-v0.5: Extract saved pre-processed image array\n",
    "pre_process_file = (PREPROCESS_SAVE_DIR + he_date_str\n",
    "                    + '_pre_processed_map.npy')\n",
    "pre_processed_map_data = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "pre_processed_map = sunpy.map.Map(np.flipud(pre_processed_map_data), he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=pre_processed_map)\n",
    "pre_processed_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Heliographic coordinate reprojected magnetogram map\n",
    "hg_mag_fits_file = (f'{HELIOGRAPH_MAG_SAVE_DIR}'\n",
    "                    f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "hg_mag_map = sunpy.map.Map(hg_mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=hg_mag_map)\n",
    "hg_mag_map.plot(axes=ax, title='', vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.5.1+: Extract saved ensemble map\n",
    "ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "ensemble_map = sunpy.map.Map(ensemble_file)\n",
    "ensemble_map_data = np.flipud(ensemble_map.data)\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='', cmap='magma')\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.2-0.5: Extract saved ensemble map array and convert to Sunpy map\n",
    "ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing ensemble map from a single segmentation\n",
    "percent_of_peak = 100\n",
    "morph_radius_dist = 15\n",
    "\n",
    "ch_mask_data = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, [percent_of_peak], [morph_radius_dist]\n",
    ")[0]\n",
    "ensemble_map_data = np.where(~np.isnan(np.flipud(pre_processed_map.data)), 0, np.nan)\n",
    "ensemble_map_data = np.where(ch_mask_data, 100, ensemble_map_data)\n",
    "\n",
    "ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), pre_processed_map.meta)\n",
    "ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved single mask array and convert to Sunpy map\n",
    "mask_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "mask_data = np.load(mask_file, allow_pickle=True)[-1]\n",
    "mask_map = sunpy.map.Map(np.flipud(mask_data), he_map.meta)\n",
    "mask_map.plot_settings['cmap'] = colormaps['gray']\n",
    "\n",
    "he_base_data = np.where(he_map.data == he_map.data[0,0], np.nan, he_map.data)\n",
    "he_base_map = sunpy.map.Map(he_base_data, he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5))\n",
    "\n",
    "plot_detection.plot_he_map(fig, (1, 4, 1), he_map, he_date_str)\n",
    "\n",
    "# Plot He I observation with overlayed detection contours\n",
    "ax = fig.add_subplot(142, projection=he_map)\n",
    "he_base_map.plot(axes=ax, vmin=-100, vmax=100, title=he_date_str,\n",
    "                    cmap='afmhot')\n",
    "for contour in mask_map.contour(0):\n",
    "    ax.plot_coord(contour, color='black', linewidth=1)\n",
    "\n",
    "plot_detection.plot_euv_map(fig, (1, 4, 3), euv_map, euv_date_str)\n",
    "\n",
    "ax = fig.add_subplot(144, projection=he_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50, title=mag_date_str)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d82b32f-7347-4351-94c2-22c2f8f1786d",
   "metadata": {},
   "source": [
    "# Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_1_he, he_high_cut, he_nan = detect.pre_process_v0_1(\n",
    "    he_map_data, peak_count_cutoff_percent=0.1\n",
    ")\n",
    "\n",
    "arrays = [he_map_data, he_nan, he_high_cut, pre_process_v0_1_he]\n",
    "titles = ['he', 'he NaN', 'he High Cut', 'he Band Cut']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = prepare_data.get_fits_path(\n",
    "        he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    pre_process_v0_1_he = detect.pre_process_v0_1(raw_he)[0]\n",
    "    arrays = [raw_he, pre_process_v0_1_he]\n",
    "    titles = [he_date_str, 'Pre-Processed']\n",
    "    plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, pre_process_v0_4_he]\n",
    "titles = ['L2 Observation', 'Pre-Processed Observation']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = prepare_data.get_fits_path(\n",
    "        he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    pre_process_v0_4_he = detect.pre_process_v0_4(raw_he)\n",
    "    arrays = [raw_he, pre_process_v0_4_he]\n",
    "    titles = [he_date_str, 'Pre-Processed']\n",
    "    plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_map_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "arrays = [he_map_data, np.flipud(pre_processed_map_v0_5_1_map.data)]\n",
    "titles = ['L2 Observation', 'Pre-Processed Observation']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "fig = plt.figure(figsize=(11, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1, projection=he_map)\n",
    "he_map.plot(axes=ax, title=he_map.date)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2, projection=he_map)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100, title='+/-100 mAngstrom Saturation')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, (3,4), projection=pre_processed_vY_map)\n",
    "pre_processed_vY_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_peak_counts(array):\n",
    "    \"\"\"Retrieve an array with the value of peak counts replaced with NaN.\n",
    "    \"\"\"\n",
    "    peak_counts_val = detect.get_peak_counts_loc(array, bins_as_percent=False)\n",
    "    zero_vals = (array > peak_counts_val - 1e-2) & (array < peak_counts_val + 1e-2)\n",
    "    \n",
    "    return np.where(zero_vals, np.NaN, array)\n",
    "\n",
    "def band_pass(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by setting background to NaN\n",
    "    and a simple brightness band pass.\n",
    "    \"\"\"\n",
    "    he_nan = np.where(raw_he == 0, np.NaN, raw_he)\n",
    "    \n",
    "    he_high_cut = np.where(he_nan > 100, np.NaN, he_nan)\n",
    "    # he_band_cut = np.where(he_high_cut < -100, np.NaN, he_high_cut)\n",
    "    he_band_cut = np.clip(he_high_cut, -100, 100)\n",
    "    \n",
    "    return he_band_cut, he_high_cut, he_nan\n",
    "\n",
    "\n",
    "def equalize(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by setting background to NaN\n",
    "    and a simple brightness band pass.\n",
    "    \"\"\"\n",
    "    # Histogram equalization\n",
    "    he1 = exposure.equalize_hist(raw_he)\n",
    "    he1 = detect.remove_background(he1)\n",
    "    \n",
    "    # Shift nonzero values into positive range and equalize histogram\n",
    "    he2 = np.where(raw_he == 0, 0, raw_he + np.abs(np.min(raw_he)))\n",
    "    he3 = exposure.equalize_hist(he2)\n",
    "    \n",
    "    he3 = np.where(he3 == np.min(he3), np.NaN, he3)\n",
    "    \n",
    "    return he3, he2, he1\n",
    "\n",
    "\n",
    "def rescale(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by applying linear rescaling\n",
    "    to normalize the contrast and setting background to NaN. Linear\n",
    "    rescaling between 2-98 percentiles produces a less harsh contrast\n",
    "    enhancement than histogram equalization.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(raw_he[~np.isnan(raw_he)], (2, 98))\n",
    "    \n",
    "    # Shift nonzero values into positive range and normalize\n",
    "    he1 = np.where(raw_he == 0, 0, raw_he + np.abs(np.min(raw_he)))\n",
    "    he2 = exposure.rescale_intensity(he1, in_range=(p2, p98))\n",
    "    \n",
    "    # Normalize directly\n",
    "    he3 = exposure.rescale_intensity(raw_he, in_range=(p2, p98))\n",
    "    he3 = detect.remove_background(he3)\n",
    "        \n",
    "    return he3, he2, he1\n",
    "\n",
    "\n",
    "def rescale_center(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by applying linear rescaling\n",
    "    to normalize the contrast, set background to NaN, and centering mode\n",
    "    to zero.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(raw_he, (2, 98))\n",
    "    \n",
    "    # Linearly rescale\n",
    "    he1 = exposure.rescale_intensity(raw_he, in_range=(p2, p98))    \n",
    "    he2 = detect.remove_background(he1)\n",
    "    \n",
    "    # Center mode to zero\n",
    "    peak_counts_val = detect.get_peak_counts_loc(he2, bins_as_percent=False)\n",
    "    he3 = he2 - peak_counts_val + 1\n",
    "\n",
    "    return he3, he2, he1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Band Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e418b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_band_cut, he_high_cut, he_nan = band_pass(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he_nan, he_high_cut, he_band_cut]\n",
    "titles = ['he', 'he NaN', 'he High Cut', 'he Band Cut']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5300ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = equalize(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he1, he2, he3]\n",
    "titles = ['he', 'Equalized', 'Shifted', 'Shifted & Equalized']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = detect.get_hist(he3, bins_as_percent=True, n=1000)\n",
    "plt.semilogy(edges[0:-1], hist)\n",
    "detect.get_peak_counts_loc(he3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bf6e056",
   "metadata": {},
   "source": [
    "Rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e692a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = rescale(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he3, he1, he2]\n",
    "titles = ['he', 'Stretched', 'Shifted', 'Shifted & Stretched']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95058682",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = rescale_center(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he1, he2, he3]\n",
    "titles = ['he', 'Stretched', 'Shifted', 'Removed Background']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "raw_ratio = prepare_data.get_image_from_fits(ratio_fits_file)\n",
    "ratio = detect.pre_process_v0_4(raw_ratio)\n",
    "\n",
    "arrays = [raw_ratio, ratio]\n",
    "titles = ['Raw Ratio', 'Pre-Processed Ratio']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-Limb Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hp_coords = all_coordinates_from_map(he_map)\n",
    "mask = coordinate_is_on_solar_disk(all_hp_coords)\n",
    "limb_removed_he_map = sunpy.map.Map(\n",
    "    np.where(mask, he_map.data, np.nan), he_map.meta\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=limb_removed_he_map)\n",
    "limb_removed_he_map.plot(axes=ax, title='', vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hp_coords = all_coordinates_from_map(mag_map)\n",
    "mask = coordinate_is_on_solar_disk(all_hp_coords)\n",
    "limb_removed_mag_map = sunpy.map.Map(mag_map.data, mag_map.meta, mask=~mask)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=limb_removed_mag_map)\n",
    "limb_removed_mag_map.plot(axes=ax, title='', vmin=-50, vmax=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_date_str = HE_DATE_LIST[3]\n",
    "mag_date_str = MAG_DATE_LIST[4]\n",
    "\n",
    "he_fits_file = prepare_data.get_fits_path(\n",
    "   he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    ")\n",
    "mag_fits_file = prepare_data.get_fits_path(\n",
    "   mag_date_str, DATE_RANGE, ALL_MAG_DIR, SELECT_MAG_DIR\n",
    ")\n",
    "he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "mag_map = prepare_data.get_nso_sunpy_map(mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=mag_map)\n",
    "mag_map.plot(axes=ax1, vmin=-50, vmax=50,\n",
    "               title=f'Original: {mag_map.date}')\n",
    "\n",
    "smoothed_map = prepare_data.get_smoothed_map(mag_map, smooth_size_percent=10)\n",
    "plot_detection.plot_map_contours(ax1, smoothed_map)\n",
    "\n",
    "reprojected_map = prepare_data.diff_rotate(\n",
    "   input_map=mag_map, target_map=he_map\n",
    ")\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection=reprojected_map)\n",
    "reprojected_map.plot(axes=ax2, vmin=-50, vmax=50,\n",
    "                     title=f'Reprojection: {reprojected_map.date}')\n",
    "\n",
    "reprojected_smooth_map = prepare_data.diff_rotate(\n",
    "   input_map=smoothed_map, target_map=he_map\n",
    ")\n",
    "plot_detection.plot_map_contours(ax2, reprojected_smooth_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helioprojective Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec)/pix > (HP Tx, Ty arcsec)\n",
    "Tx_scale = he_map.scale.axis1.to(u.arcsec/u.pix) * u.pix\n",
    "Ty_scale = he_map.scale.axis2.to(u.arcsec/u.pix) * u.pix\n",
    "\n",
    "f'Tx: {Tx_scale.value:.5f} Ty: {Ty_scale.value:.5f} arcsec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    he_map.scale.axis1*u.pix,\n",
    "    he_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=he_map.date)\n",
    ")\n",
    "f'x: {hc_delta_coords.x.to(u.Mm).value:.5f} y: {hc_delta_coords.x.to(u.Mm).value:.5f} Mm'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrington Non-CEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection map shape scaling factors\n",
    "# Increase to increase map resolution and reduce distance scale / pixel\n",
    "# Aim to match Helioprojective scale to preserve resolution\n",
    "NON_CEA_LON_FACTOR = 1.55\n",
    "NON_CEA_LAT_FACTOR = 1.55\n",
    "\n",
    "# Obtain dimension  in image pixel number of the solar radius\n",
    "Rs_hp_coord = SkyCoord(\n",
    "    he_map.rsun_obs, 0*u.arcsec, frame='helioprojective',\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "Rs_pixel_pair = he_map.world_to_pixel(Rs_hp_coord)\n",
    "ref_pixel_pair = he_map.world_to_pixel(he_map.reference_coordinate)\n",
    "Rs_dim = int((Rs_pixel_pair.x - ref_pixel_pair.x).value)\n",
    "\n",
    "new_rows = int(2*Rs_dim*NON_CEA_LAT_FACTOR)\n",
    "new_cols = int(4*Rs_dim*NON_CEA_LON_FACTOR)\n",
    "\n",
    "hg_header = sunpy.map.header_helper.make_heliographic_header(\n",
    "    he_map.date, he_map.observer_coordinate,\n",
    "    shape=(new_rows, new_cols), frame='stonyhurst'\n",
    ")\n",
    "\n",
    "# Convert to 180 deg center longitude for Carrington map visualization\n",
    "hg_header['crval1'] = 180\n",
    "non_cea_map = he_map.reproject_to(\n",
    "    hg_header#, algorithm='adaptive'\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1, projection=he_map)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100, title=he_map.date)\n",
    "\n",
    "ax = fig.add_subplot(1, 3, (2,3), projection=non_cea_map)\n",
    "non_cea_map.plot(axes=ax, vmin=-100, vmax=100, title='')\n",
    "\n",
    "(new_rows, new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-CEA Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg)/pix > (HG lon, lat deg)\n",
    "lon_scale = non_cea_map.scale.axis1.to(u.deg/u.pix) * u.pix\n",
    "lat_scale = non_cea_map.scale.axis2.to(u.deg/u.pix) * u.pix\n",
    "\n",
    "f'lon: {lon_scale.value:.5f} lat: {lat_scale.value:.5f} deg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg) > (HC Mm)\n",
    "x_scale = (non_cea_map.rsun_meters * lon_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "y_scale = (non_cea_map.rsun_meters * lat_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG\n",
    "# (HG lon, lat deg) > (HC Mm)\n",
    "hg_delta_coords = frames.HeliographicStonyhurst(lon_scale, lat_scale)\n",
    "hc_delta_coords = hg_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=non_cea_map.date)\n",
    ")\n",
    "\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'\n",
    "\n",
    "# WRONG\n",
    "# (HP??? lon, lat deg) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    lon_scale, lat_scale,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=non_cea_map.date)\n",
    ")\n",
    "\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stonyhurst CEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection map shape scaling factors\n",
    "# Increase to increase map resolution and reduce distance scale / pixel\n",
    "# Aim to match Helioprojective scale within 0.01 tolerance to preserve resolution\n",
    "CEA_X_SCALE_FACTOR = np.pi/2\n",
    "CEA_Y_SCALE_FACTOR = 1\n",
    "\n",
    "# Obtain dimension  in image pixel number of the solar radius\n",
    "Rs_hp_coord = SkyCoord(\n",
    "    he_map.rsun_obs, 0*u.arcsec, frame='helioprojective',\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "Rs_pixel_pair = he_map.world_to_pixel(Rs_hp_coord)\n",
    "ref_pixel_pair = he_map.world_to_pixel(he_map.reference_coordinate)\n",
    "Rs_dim = int((Rs_pixel_pair.x - ref_pixel_pair.x).value)\n",
    "\n",
    "new_row_num = int(2*Rs_dim*CEA_Y_SCALE_FACTOR)\n",
    "new_col_num = int(4*Rs_dim*CEA_X_SCALE_FACTOR)\n",
    "\n",
    "hg_header = sunpy.map.header_helper.make_heliographic_header(\n",
    "    he_map.date,\n",
    "    he_map.observer_coordinate,\n",
    "    # observer,\n",
    "    shape=(new_row_num, new_col_num), frame='stonyhurst',\n",
    "    projection_code='CEA'\n",
    ")\n",
    "\n",
    "# Specify Earth-based observer for solar radius, distance to Sun,\n",
    "# and Heliographic coordinates to avoid warning messages due to\n",
    "# missing keywords\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    Tx=0*u.arcsec, Ty=0*u.arcsec,\n",
    "    observer='earth', obstime=he_map.date,\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header((1,1), earth_hp_coords)\n",
    "for earth_coord_key in ['RSUN_REF', 'DSUN_OBS', 'HGLN_OBS', 'HGLT_OBS']:\n",
    "    hg_header[earth_coord_key] = earth_header[earth_coord_key]\n",
    "\n",
    "cea_he_map = he_map.reproject_to(\n",
    "    hg_header, #algorithm='adaptive'\n",
    ")\n",
    "\n",
    "# Crop map to within 90 degrees of the central meridian\n",
    "top_right = SkyCoord(\n",
    "    lon=90*u.deg, lat=90*u.deg, frame=cea_he_map.coordinate_frame\n",
    ")\n",
    "bottom_left = SkyCoord(\n",
    "    lon=-90*u.deg, lat=-90*u.deg, frame=cea_he_map.coordinate_frame\n",
    ")\n",
    "cea_he_map = cea_he_map.submap(bottom_left, top_right=top_right)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111, projection=cea_he_map)\n",
    "cea_he_map.plot(axes=ax, vmin=-100, vmax=100)\n",
    "\n",
    "(new_row_num, new_col_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec)/pix > (HP Tx, Ty arcsec)\n",
    "Tx_scale = he_map.scale.axis1.to(u.arcsec/u.pix) * u.pix\n",
    "Ty_scale = he_map.scale.axis2.to(u.arcsec/u.pix) * u.pix\n",
    "\n",
    "f'Tx: {Tx_scale.value:.5f} Ty: {Ty_scale.value:.5f} arcsec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    he_map.scale.axis1*u.pix,\n",
    "    he_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=he_map.date)\n",
    ")\n",
    "f'x: {hc_delta_coords.x.to(u.Mm).value:.5f} y: {hc_delta_coords.x.to(u.Mm).value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_mag_map = detect.reproject_to_cea(mag_map)\n",
    "\n",
    "reprojected_mag_map = cea_mag_map.reproject_to(\n",
    "    cea_he_map.wcs, #algorithm='adaptive'\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEA Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Available: (HG lon, lat deg)/pix > (HG lon, lat deg)\n",
    "lon_scale = cea_he_map.scale.axis1.to(u.deg/u.pix) * u.pix\n",
    "lat_scale = cea_he_map.scale.axis2.to(u.deg/u.pix) * u.pix\n",
    "\n",
    "f'lon: {lon_scale.value:.5f} lat: {lat_scale.value:.5f} deg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg) > (HC Mm)\n",
    "x_scale = (cea_he_map.rsun_meters * lon_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "y_scale = (cea_he_map.rsun_meters * lat_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG\n",
    "# (HG lon, lat deg) > (HC Mm)\n",
    "hg_delta_coords = frames.HeliographicStonyhurst(\n",
    "    pre_processed_map.scale.axis1*u.pix,\n",
    "    pre_processed_map.scale.axis2*u.pix,\n",
    ")\n",
    "hc_delta_coords = hg_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'\n",
    "\n",
    "# WRONG\n",
    "# (HP??? lon, lat deg) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    pre_processed_map.scale.axis1*u.pix,\n",
    "    pre_processed_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07ac5ef7-6c04-4363-aba1-2aa4766b2e9a",
   "metadata": {},
   "source": [
    "# Design Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '#6ece58'\n",
    "BLUE = '#3e4989'\n",
    "ORANGE = '#fd9668'\n",
    "PURPLE = '#721f81'\n",
    "\n",
    "\n",
    "def get_thresh_px_percent_list(array, percent_of_peak_list):\n",
    "    \"\"\"Retrieve the area percentage of pixels accepted by varied thresholds.\n",
    "    \"\"\"\n",
    "    thresh_bound_list = [\n",
    "        detect.get_thresh_bound(array, percent_of_peak)\n",
    "        for percent_of_peak in percent_of_peak_list\n",
    "    ]\n",
    "    px_percent_list = [\n",
    "        np.count_nonzero(array > thresh_bound)*100/array.size\n",
    "        for thresh_bound in thresh_bound_list\n",
    "    ]\n",
    "    return px_percent_list\n",
    "\n",
    "\n",
    "def get_parameter_stats(outcome_list):\n",
    "    \"\"\"Retrieve maximum difference between segmentations in area percentage\n",
    "    detected, the average area percentage at the max difference for a cutoff,\n",
    "    the number selected below this cutoff, and differences in area percentage.\n",
    "    \"\"\"    \n",
    "    outcome_diffs = np.abs(np.diff(outcome_list))\n",
    "\n",
    "    max_diff_i = np.argmax(outcome_diffs)\n",
    "    max_diff = np.max(outcome_diffs)*100/outcome_list[max_diff_i]\n",
    "    \n",
    "    cutoff = np.mean([outcome_list[max_diff_i], \n",
    "                      outcome_list[max_diff_i + 1]])\n",
    "\n",
    "    selected_parameter_num = np.count_nonzero(outcome_list > cutoff)\n",
    "    \n",
    "    return max_diff, cutoff, selected_parameter_num, outcome_diffs\n",
    "\n",
    "\n",
    "def plot_pixel_percent_bars(ax, parameter_list, pixel_percent_list,\n",
    "                            max_diff, cutoff, selected_parameter_num,\n",
    "                            step, title, unit, xlabel, thresh=True):\n",
    "    bar_width = 0.8*step\n",
    "    selected_parameters = parameter_list[selected_parameter_num:]\n",
    "    \n",
    "    ax.set_title(f'{title}\\n Cutoff: {selected_parameters[0]}{unit} | ' +\n",
    "                 f'Max Difference: {max_diff:.1f}%' , fontsize=28)\n",
    "    ax.set_xlabel(xlabel, fontsize=24)\n",
    "    \n",
    "    ax.set_ylabel('Pixel Percentage (%)', fontsize=24)\n",
    "    \n",
    "    ax.plot([parameter_list[0] - step/2, parameter_list[-1] + step/2], [cutoff, cutoff], \n",
    "               linestyle='--', color='k', linewidth=3)\n",
    "    \n",
    "    if thresh:\n",
    "        ax.bar(parameter_list, pixel_percent_list, \n",
    "               width=bar_width, color=BLUE)\n",
    "        ax.bar(selected_parameters, \n",
    "               pixel_percent_list[selected_parameter_num:], \n",
    "               width=bar_width, color=GREEN)\n",
    "    else:\n",
    "        ax.bar(parameter_list, pixel_percent_list,\n",
    "               width=bar_width, color=PURPLE)\n",
    "        ax.bar(selected_parameters,\n",
    "               pixel_percent_list[selected_parameter_num:], \n",
    "               width=bar_width, color=ORANGE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ec0780b",
   "metadata": {},
   "source": [
    "## Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    pre_process_v0_1_he, bounds=[75, 90, 105], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = prepare_data.get_fits_path(\n",
    "        he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    he = detect.pre_process_v0_1(raw_he)[0]\n",
    "\n",
    "    plot_detection.plot_thresholds(he, bounds=[75, 85, 100], bounds_as_percent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweep\n",
    "step = 5\n",
    "percent_of_peak_lists = [\n",
    "    list(np.arange(0,200,step)), list(np.arange(80,130,step))\n",
    "]\n",
    "\n",
    "for percent_of_peak_list in percent_of_peak_lists:\n",
    "    px_percent_list = get_thresh_px_percent_list(pre_process_v0_1_he, percent_of_peak_list)\n",
    "    \n",
    "    parameter_stats = get_parameter_stats(px_percent_list)\n",
    "    max_diff, cutoff, selected_parameter_num, pixel_percent_diffs = parameter_stats\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    plot_pixel_percent_bars(\n",
    "        ax, percent_of_peak_list, px_percent_list, max_diff, cutoff, selected_parameter_num,\n",
    "        step, title='Threshold', unit='%', xlabel='Percent of Peak Pixel Count (%)', thresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    pre_process_v0_4_he, bounds=[75, 90, 105], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    np.flipud(pre_processed_v0_5_1_map.data), bounds=[75, 90, 105],\n",
    "    bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    np.flipud(pre_processed_vY_map.data), bounds=[75, 90, 105],\n",
    "    bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7886423-ef31-43ec-9020-025001037628",
   "metadata": {},
   "source": [
    "## Structuring Element Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_varied_morph_radius(pre_processed_map_data, percent_of_peak_list,\n",
    "                             morph_radius_list, ch_mask_list, px=False):\n",
    "    plot_detection.plot_images([pre_processed_map_data], image_size=4)\n",
    "\n",
    "    image_list = [pre_processed_map_data for _ in range(len(ch_mask_list))]\n",
    "    axes = plot_detection.plot_image_grid(\n",
    "        image_list, num_cols=3, cmap='afmhot', image_size=7\n",
    "    )\n",
    "    zipped_items = zip(axes.values(), percent_of_peak_list,\n",
    "                       morph_radius_list, ch_mask_list)\n",
    "\n",
    "    for ax, percent_of_peak, radius, ch_mask in zipped_items:\n",
    "        if px:\n",
    "            ax.set_title(f'{percent_of_peak:d}% of Peak | {radius:d}px Radius')\n",
    "        else:\n",
    "            ax.set_title((f'{percent_of_peak:d}% of Mode Threshold | '\n",
    "                          f'{radius:d}Mm SE Disk Radius'))\n",
    "        \n",
    "        ax.tick_params(left=False, right=False, labelleft=False,\n",
    "                       labelbottom=False, bottom=False)\n",
    "            \n",
    "        ax.contour(ch_mask, cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b327def-2b9e-43e3-b5f4-69dea76a5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [12,16,20]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "plot_varied_morph_radius(pre_process_v0_1_he, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, px=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3660c-3cb1-46f3-94ed-fa9e35134099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweep\n",
    "step = 2\n",
    "morph_radius_lists = [\n",
    "    list(np.arange(1,21,step)), list(np.arange(8,15,step))\n",
    "]\n",
    "\n",
    "for morph_radius_list in morph_radius_lists:\n",
    "    percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "    ch_mask_list = [\n",
    "        detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    px_percent_list = detect.get_px_percent_list(ch_mask_list)\n",
    "    \n",
    "    parameter_stats = get_parameter_stats(px_percent_list)\n",
    "    max_diff, cutoff, selected_parameter_num, pixel_percent_diffs = parameter_stats\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    plot_pixel_percent_bars(\n",
    "        ax, morph_radius_list, px_percent_list, max_diff, cutoff, selected_parameter_num,\n",
    "        step, title='SE Disk Radius', unit='px', xlabel='SE Disk Radius (px)', thresh=False\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [12,16,20]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_process_v0_4_he, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "plot_varied_morph_radius(pre_process_v0_4_he, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, px=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [8, 12, 16]\n",
    "percent_of_peak_list = [80 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "\n",
    "pre_processed_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_v0_5_1_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_v0_5_1_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_date_str = HE_DATE_LIST[1]\n",
    "morph_radius_list = [8, 12, 16]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "\n",
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_vY(\n",
    "    pre_processed_vY_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_vY_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial distance definition of SE disk radius ideas\n",
    "\n",
    "# Area square: (HP Tx, Ty arcsec) > (HC Mm)\n",
    "# Get HG coords: (pixel) > (HP Tx, Ty arcsec) > (HG lon, lat deg)\n",
    "# Reproject CEA: (HP Tx, Ty arcsec) > (pixel)\n",
    "\n",
    "# SE Disk Radius: (HC Mm) > (pixel)\n",
    "# HC to HP/HG to pixel\n",
    "# To pixel step fails\n",
    "empty_dim_list = [0*u.km for _ in morph_radius_list]\n",
    "morph_radius_hc_coords = SkyCoord(\n",
    "    x=[morph_radius_dist*u.Mm for morph_radius_dist in morph_radius_list],\n",
    "    y=empty_dim_list, z=empty_dim_list, frame='heliocentric',\n",
    "    observer='earth', obstime=pre_processed_map.date\n",
    ")\n",
    "morph_radius_hp_coords = morph_radius_hc_coords.transform_to(\n",
    "    frames.Helioprojective(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "morph_radius_pixel_coord = pre_processed_map.world_to_pixel(\n",
    "    morph_radius_hc_coords\n",
    ").x\n",
    "ref_pixel_coord = pre_processed_map.world_to_pixel(\n",
    "    pre_processed_map.reference_coordinate\n",
    ").x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill and Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_of_peak_list = [100, 90]\n",
    "# morph_radius_list = [14, 8]\n",
    "percent_of_peak_list = [90, 70]\n",
    "morph_radius_list = [13, 15]\n",
    "\n",
    "pre_processed_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_v0_5_1_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_v0_5_1_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [90, 90]\n",
    "morph_radius_list = [10, 15]\n",
    "\n",
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_vY(\n",
    "    pre_processed_vY_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_vY_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Variable Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_design_var_grid(pre_processed_map_data, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, num_cols):\n",
    "    image_list = [pre_processed_map_data for _ in range(len(ch_mask_list))]\n",
    "    axes = plot_detection.plot_image_grid(\n",
    "        image_list, num_cols, cmap='afmhot', image_size=7\n",
    "    )\n",
    "    zipped_items = zip(axes.values(), percent_of_peak_list,\n",
    "                    morph_radius_list, ch_mask_list)\n",
    "\n",
    "    for ax, percent_of_peak, radius, ch_mask in zipped_items:\n",
    "        ax.set_title(f'{percent_of_peak:d}% of Peak | {radius:d}Mm Radius')\n",
    "        \n",
    "        ax.tick_params(left=False, right=False, labelleft=False,\n",
    "                        labelbottom=False, bottom=False)\n",
    "            \n",
    "        ax.contour(ch_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peaks = [80, 90, 100]\n",
    "morph_radii = [      9, 13, 17] # Mm\n",
    "percent_of_peak_list = [percent_of_peak \n",
    "                        for _ in morph_radii\n",
    "                        for percent_of_peak in percent_of_peaks]\n",
    "morph_radius_list = [morph_radius \n",
    "                     for morph_radius in reversed(morph_radii)\n",
    "                     for _ in percent_of_peaks]\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "plot_design_var_grid(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list,\n",
    "    ch_mask_list, num_cols=len(percent_of_peaks)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # v0.5.1 Design\n",
    "# percent_of_peak_list = [70, 70, 80, 90]\n",
    "# morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# v0.5.1 KPVT Design\n",
    "percent_of_peak_list = [85, 105, 85, 95]\n",
    "morph_radius_list = [   17, 13, 15, 13] # Mm\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "plot_design_var_grid(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list,\n",
    "    ch_mask_list, num_cols=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57ad4835",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versions\n",
    "\n",
    "Appropriate pre-processed products must be extracted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f95f843",
   "metadata": {},
   "source": [
    "v0.2: Detected Pixel Percentage Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1148e83-6a17-4503-8814-053692d910b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80,90,100,100]\n",
    "morph_radius_list = [13,17,15,13,17] # px\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_2(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map_data, ch_mask_list, confidence_list, px_percent_list = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, ch_mask_list,\n",
    "    confidence_list, px_percent_list, mask_contour=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.3: Evenly Assigned Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1db2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [   13,17, 15, 13, 17] # px\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_3(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5: Directly Assigned Unipolarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [   18,20, 16, 16, 20] # px\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_5(\n",
    "    pre_processed_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list, unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, map_data_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, map_data_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative Design\n",
    "# percent_of_peak_list = [80, 80, 90, 100]\n",
    "# morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# Aggressive Design\n",
    "percent_of_peak_list = [70, 70, 80, 90]\n",
    "morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_5_1(\n",
    "    pre_processed_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, masks_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [85, 73, 95, 85]\n",
    "morph_radius_list = [   10, 14, 10, 14]\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "out = detect.get_ensemble_vY(\n",
    "    pre_processed_map, hg_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, masks_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d947bd7",
   "metadata": {},
   "source": [
    "v0.3b: Percentile Assigned Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_ensemble(pre_processed_map, percent_of_peak_list,\n",
    "                        morph_radius_list, even_confidence=True):\n",
    "    \"\"\"Retrieve an ensemble of segmentations sorted by CH smoothness.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "        even_confidence: boolean to specify confidence assignment\n",
    "            True to assign confidence by even ranking in (0,100]%\n",
    "            False to assign confidence by percentile of gradient\n",
    "                median among values from other candidate CHs in [0,100]%\n",
    "    Returns\n",
    "        Ensemble greyscale coronal holes mask sorted by mean gradient.\n",
    "        List of binary coronal holes masks.\n",
    "        List of confidence levels in mask layers.\n",
    "    \"\"\"\n",
    "    # Create global segmentations for varied design variable combinations\n",
    "    ch_masks = [\n",
    "        detect.get_ch_mask(pre_processed_map, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    # Lists to hold pre processed map and gradient data respectively\n",
    "    # for distinct CHs from all segmentations\n",
    "    map_data_by_ch = []\n",
    "    grad_data_by_ch = []\n",
    "    \n",
    "    for ch_mask in ch_masks:\n",
    "        # Masked array of candidate CHs\n",
    "        masked_candidates = detect.get_masked_candidates(pre_processed_map, ch_mask)\n",
    "        \n",
    "        # Compute spatial gradient\n",
    "        gradient_candidates = filters.sobel(masked_candidates)\n",
    "        \n",
    "        map_data_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(pre_processed_map, ch_mask)\n",
    "        )\n",
    "        grad_data_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(gradient_candidates, ch_mask)\n",
    "        )\n",
    "    \n",
    "     # Obtain sorting indixes from greatest to least gradient median\n",
    "    gradient_medians = [np.median(grad_data[~np.isnan(grad_data)])\n",
    "                        for grad_data in grad_data_by_ch]\n",
    "    sorted_idxs = np.flip(np.argsort(gradient_medians))\n",
    "    \n",
    "    # Sort candidate CHs from greatest to least gradient median\n",
    "    map_data_by_ch = [map_data_by_ch[i] for i in sorted_idxs]\n",
    "    gradient_medians = [gradient_medians[i] for i in sorted_idxs]\n",
    "    \n",
    "    # Assign confidence by percentile or direct ranking\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    if even_confidence:\n",
    "        confidence_list = [(c + 1)*100/num_ch\n",
    "                           for c in range(num_ch)]\n",
    "    else:\n",
    "        percent_conversion = 100 / (np.max(gradient_medians)\n",
    "                                    - np.min(gradient_medians))\n",
    "        confidence_list = [\n",
    "            100 - (grad_median - np.min(gradient_medians)) *percent_conversion\n",
    "            for grad_median in gradient_medians\n",
    "        ]\n",
    "\n",
    "    # Construct ensemble map by adding distinct CHs with assigned\n",
    "    # confidence level values to an empty base disk\n",
    "    ensemble_map = np.where(~np.isnan(pre_processed_map), 0, np.nan)\n",
    "    \n",
    "    for distinct_ch, confidence in zip(map_data_by_ch, confidence_list):\n",
    "        ensemble_map = np.where(\n",
    "            ~np.isnan(distinct_ch), confidence, ensemble_map\n",
    "        )\n",
    "    return ensemble_map, map_data_by_ch, confidence_list, gradient_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9add29",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80,90,100,100]\n",
    "morph_radius_list = [13,17,15,13,17]\n",
    "\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "out = get_smooth_ensemble(\n",
    "    pre_process_v0_1_he, percent_of_peak_list, morph_radius_list,\n",
    "    even_confidence=False\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_1_he, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5b: Evenly Assigned Unipolarity, No threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unipol_ensemble(pre_processed_map, reprojected_mag_map,\n",
    "                        percent_of_peak_list, morph_radius_list,\n",
    "                        even_confidence=True):\n",
    "    \"\"\"Retrieve an ensemble of segmentations sorted by CH unipolarity.\n",
    "    \n",
    "    Args\n",
    "        pre_processed_map: Sunpy map object to segment\n",
    "        reprojected_mag_map: Sunpy map object of magnetogram reprojected\n",
    "            to align with the ensemble map\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "        even_confidence: boolean to specify confidence assignment\n",
    "            True to assign confidence by even ranking in (0,100]%\n",
    "            False to assign confidence as 100% for unipolarity of 0\n",
    "                and 0% for unipolarity of 1\n",
    "    Returns\n",
    "        Ensemble greyscale coronal holes mask sorted by unipolarity.\n",
    "        List of coronal holes masks.\n",
    "        List of confidence levels in mask layers.\n",
    "    \"\"\"\n",
    "    pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "    \n",
    "    # Create global segmentations for varied design variable combinations\n",
    "    ch_masks = [\n",
    "        detect.get_ch_mask(pre_processed_map_data, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    # List to be extended by masks for distinct CHs from all segmentations\n",
    "    masks_by_ch = []\n",
    "    \n",
    "    ones_array = np.ones_like(pre_processed_map_data)\n",
    "    \n",
    "    for ch_mask in ch_masks:\n",
    "        masks_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(ones_array, ch_mask)\n",
    "        )\n",
    "    \n",
    "    num_ch = len(masks_by_ch)\n",
    "    \n",
    "    # Compute constant area per square pixel once for all CHs\n",
    "    A_per_square_px = detect.get_A_per_square_px(pre_processed_map)\n",
    "    \n",
    "    # List to hold unipolarity for distinct CHs from all segmentations\n",
    "    unipolarity_by_ch = []\n",
    "    \n",
    "    for ch_label in range(num_ch):\n",
    "        distinct_ch_mask = masks_by_ch[ch_label]\n",
    "        \n",
    "        # Not flipping works right\n",
    "        distinct_ch_map = sunpy.map.Map(\n",
    "            distinct_ch_mask, pre_processed_map.meta\n",
    "        )\n",
    "        # ax = fig.add_subplot(num_rows, num_cols, ch_label + 1, projection=pre_processed_map)\n",
    "        # distinct_ch_map.plot(cmap='magma')\n",
    "        \n",
    "        # fake_mag_map_data = np.where(~np.isnan(np.flipud(distinct_ch_mask)), 25,\n",
    "        #                              reprojected_mag_map.data)\n",
    "        # fake_mag_map = sunpy.map.Map(fake_mag_map_data, reprojected_mag_map.meta)\n",
    "        # outcomes = get_outcomes(\n",
    "        #     distinct_ch_map, fake_mag_map, A_per_square_px\n",
    "        # )\n",
    "        \n",
    "        outcome_dict = detect.get_outcomes(\n",
    "            distinct_ch_map, pre_processed_map_data, reprojected_mag_map,\n",
    "            A_per_square_px\n",
    "        )\n",
    "        unipolarity_by_ch.append(outcome_dict['unipolarity'])\n",
    "    \n",
    "    # Sort unipolarities from greatest to least\n",
    "    sorted_idxs = np.argsort(unipolarity_by_ch)\n",
    "    \n",
    "    # Sort candidate CHs from greatest to least gradient median\n",
    "    masks_by_ch = [masks_by_ch[i] for i in sorted_idxs]\n",
    "    unipolarity_by_ch = [unipolarity_by_ch[i] for i in sorted_idxs]\n",
    "    \n",
    "    # Assign confidence by direct ranking or by unipolarity value\n",
    "    if even_confidence:\n",
    "        confidence_list = [(c + 1)*100/num_ch\n",
    "                           for c in range(num_ch)]\n",
    "    else:\n",
    "        confidence_list = [100 - unipolarity*100\n",
    "                           for unipolarity in unipolarity_by_ch]\n",
    "\n",
    "    # Construct ensemble map by adding distinct CHs with assigned\n",
    "    # confidence level values to an empty base disk    \n",
    "    ensemble_map_data = np.where(\n",
    "        ~np.isnan(pre_processed_map_data), 0, np.nan\n",
    "    )\n",
    "    for distinct_ch, confidence in zip(masks_by_ch, confidence_list):\n",
    "        ensemble_map_data = np.where(\n",
    "            ~np.isnan(distinct_ch), confidence, ensemble_map_data\n",
    "        )\n",
    "    return ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "\n",
    "\n",
    "pre_process_v0_4_he_map_data = detect.pre_process_v0_4(he_map_data)\n",
    "pre_process_v0_4_he_map = sunpy.map.Map(\n",
    "    np.flipud(pre_process_v0_4_he), he_map.meta\n",
    ")\n",
    "\n",
    "out = get_unipol_ensemble(\n",
    "    pre_process_v0_4_he_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    even_confidence=True\n",
    ")\n",
    "ensemble_map_data, map_data_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_4_he_map_data, ensemble_map_data, map_data_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5c: EUV Ratio, v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "\n",
    "\n",
    "ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "raw_ratio = prepare_data.get_image_from_fits(ratio_fits_file)\n",
    "\n",
    "pre_process_v0_4_ratio_map_data = detect.pre_process_v0_4(raw_ratio)\n",
    "\n",
    "out = detect.get_ensemble_v0_3(\n",
    "    pre_process_v0_4_ratio_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_4_ratio_map_data, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96894956",
   "metadata": {},
   "source": [
    "# Outcomes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He I Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ch_band_widths(map_data_by_ch):\n",
    "    \"\"\"Retrieve a list of 5th to 95th percentile band widths for each\n",
    "    detected CH.\n",
    "    \n",
    "    Args\n",
    "        map_data_by_ch: list of isolated CH images from a segmentation\n",
    "    \"\"\"\n",
    "    percentiles = [5, 95]\n",
    "    bound_list = [np.percentile(map_data[~np.isnan(map_data)], percentiles)\n",
    "                  for map_data in map_data_by_ch]\n",
    "    \n",
    "    hole_band_widths = [bounds[1] - bounds[0]\n",
    "                        for bounds in bound_list]\n",
    "    return hole_band_widths\n",
    "\n",
    "\n",
    "def get_ch_lower_tail_widths(map_data_by_ch):\n",
    "    \"\"\"Retrieve a list of lower tail widths for each detected CH.\n",
    "    \n",
    "    Args\n",
    "        map_data_by_ch: list of isolated CH images from a segmentation\n",
    "    \"\"\"\n",
    "    filt_map_data_by_ch = [map_data[~np.isnan(map_data)]\n",
    "                         for map_data in map_data_by_ch]\n",
    "    \n",
    "    # List of the 1st percentile brightness value of each CH\n",
    "    first_percentile_list = [np.percentile(map_data, 1)\n",
    "                             for map_data in filt_map_data_by_ch]\n",
    "        \n",
    "    # List of peak count of each CH\n",
    "    peak_counts_value_list = [\n",
    "        detect.get_peak_counts_loc(map_data, bins_as_percent=False)\n",
    "        for map_data in filt_map_data_by_ch\n",
    "    ]\n",
    "\n",
    "    # List of lower tail widths of each CH\n",
    "    ch_lower_tail_width_list = [\n",
    "        peak_count - first_percentile\n",
    "        for peak_count, first_percentile \n",
    "        in zip(peak_counts_value_list, first_percentile_list)]\n",
    "    \n",
    "    return ch_lower_tail_width_list\n",
    "\n",
    "\n",
    "def plot_sorted_ch_hists(array, ch_mask, apply_gradient, hist_stat,\n",
    "                         descend_sort=False):\n",
    "    \"\"\"Plot segmented CH histograms sorted by histogram statistics.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask: binary coronal holes mask\n",
    "        apply_gradient: boolean to specify taking spatial gradient of image\n",
    "        hist_stat: str to specify histogram sorting statistic\n",
    "            'median', 'width', 'tail_width'\n",
    "        descend_sort: boolean to specify sorting CHs from greatest to least\n",
    "            statistic\n",
    "    \"\"\"\n",
    "    # Masked array of candidate CHs\n",
    "    masked_candidates = detect.get_masked_candidates(array, ch_mask)\n",
    "    if apply_gradient:\n",
    "        masked_candidates = filters.sobel(masked_candidates)\n",
    "    \n",
    "    # Isolated images of detected CHs\n",
    "    map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        masked_candidates, ch_mask\n",
    "    )\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    \n",
    "    # Compute statistics for each CH\n",
    "    medians = [np.nanmedian(map_data) for map_data in map_data_by_ch]\n",
    "    ch_band_widths = get_ch_band_widths(map_data_by_ch)\n",
    "    \n",
    "    # Histogram x limit bounds\n",
    "    hist_xlim_min = np.mean(medians)\n",
    "    if not apply_gradient:\n",
    "        hist_xlim_min = hist_xlim_min - 2*np.max(ch_band_widths)\n",
    "    hist_xlim_max = np.mean(medians) + 2*np.max(ch_band_widths)\n",
    "    \n",
    "    # Obtain indices of candidates sorted by specifed mode\n",
    "    if hist_stat == 'median':\n",
    "        sorted_candidate_idxs = np.argsort(medians)\n",
    "        titles = [f'Median: {median:.2f}'\n",
    "                  for median in medians]\n",
    "    elif hist_stat == 'width':\n",
    "        sorted_candidate_idxs = np.argsort(ch_band_widths)\n",
    "        titles = [f'90% Band Width: {ch_band_width:.1f}'\n",
    "                  for ch_band_width in ch_band_widths]\n",
    "    elif hist_stat == 'tail_width':\n",
    "        ch_lower_tail_width_list = get_ch_lower_tail_widths(\n",
    "            map_data_by_ch\n",
    "        )\n",
    "        sorted_candidate_idxs = np.argsort(ch_lower_tail_width_list)\n",
    "        titles = [f'1% to Peak Width: {ch_lower_tail_width:.1f}'\n",
    "                  for ch_lower_tail_width in ch_lower_tail_width_list]\n",
    "    \n",
    "    if descend_sort:\n",
    "        sorted_candidate_idxs = np.flip(sorted_candidate_idxs)\n",
    "\n",
    "    for r in range(int(np.ceil(num_ch/2))):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(60, 10))\n",
    "        ax = axes.ravel()\n",
    "        \n",
    "        for c in range(2):\n",
    "            i = 2*r + c\n",
    "            ax_i = 3*c\n",
    "            if i + 1 > num_ch:\n",
    "                return\n",
    "            \n",
    "            # Retrieve isolated CH image and contour\n",
    "            ch_num = sorted_candidate_idxs[i]\n",
    "            ch_im = map_data_by_ch[ch_num]\n",
    "            ch_contour = np.where(~np.isnan(ch_im), 1, 0)\n",
    "\n",
    "            # Zoom in on an isolated CH\n",
    "            y, x = np.where(~np.isnan(ch_im))\n",
    "            ch_zoom = ch_im[np.min(y) - 10:np.max(y) + 10,\n",
    "                             np.min(x) - 10:np.max(x) + 10]\n",
    "                \n",
    "            hist, edges = detect.get_hist(\n",
    "                ch_zoom[~np.isnan(ch_zoom)], bins_as_percent=False, n=200\n",
    "            )\n",
    "            \n",
    "            ax[ax_i].set_title(f'Hole {ch_num + 1}', fontsize=32)\n",
    "            ax[ax_i].imshow(array, cmap='gray', vmin=-100, vmax=100)\n",
    "            ax[ax_i].contour(ch_contour, cmap='plasma')\n",
    "            \n",
    "            if apply_gradient:\n",
    "                cmap = plt.cm.viridis\n",
    "            else:\n",
    "                cmap = plt.cm.magma\n",
    "\n",
    "            ax[ax_i + 1].imshow(ch_zoom, cmap)\n",
    "\n",
    "            ax[ax_i + 2].set_title(titles[ch_num], fontsize=32)\n",
    "            ax[ax_i + 2].bar(edges[0:-1], hist)\n",
    "            ax[ax_i + 2].set_xlim([hist_xlim_min, hist_xlim_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires single mask ensemble map\n",
    "ch_mask = np.where(ensemble_map_data > 0, 1, 0)\n",
    "plot_sorted_ch_hists(\n",
    "    he_map_data, ch_mask, apply_gradient=True,\n",
    "    hist_stat='median'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all CHs and histograms for a single date.\n",
    "# Display ranked maps for all dates.\n",
    "def get_ranked_map(array, ch_mask, apply_gradient, hist_stat,\n",
    "                   ascend_sort=True):\n",
    "    \"\"\"Retrieve a map of CHs from a single segmentation as ranked by a\n",
    "    histogram statistic.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask: binary coronal holes mask\n",
    "        apply_gradient: boolean to specify taking spatial gradient of image\n",
    "        hist_stat: str to specify histogram sorting statistic\n",
    "            'median', 'width', 'tail_width'\n",
    "        ascend_sort: boolean to specify sorting CHs from least to greatest\n",
    "            statistic\n",
    "    Returns\n",
    "        List of isolated CH images from a segmentation.\n",
    "    \"\"\"\n",
    "    # Masked array of candidate CHs\n",
    "    masked_candidates = detect.get_masked_candidates(array, ch_mask)\n",
    "    if apply_gradient:\n",
    "        masked_candidates = filters.sobel(masked_candidates)\n",
    "    \n",
    "    # Isolated images of detected CHs\n",
    "    map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        masked_candidates, ch_mask\n",
    "    )\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    \n",
    "    # Rank candidates by histogram statistic\n",
    "    if hist_stat == 'median':\n",
    "        medians = [np.nanmedian(map_data) for map_data in map_data_by_ch]\n",
    "        sorted_candidate_idxs = np.argsort(medians)\n",
    "    elif hist_stat == 'width':\n",
    "        ch_band_widths = get_ch_band_widths(map_data_by_ch)\n",
    "        sorted_candidate_idxs = np.argsort(ch_band_widths)\n",
    "    elif hist_stat == 'tail_width':\n",
    "        ch_lower_tail_width_list = get_ch_lower_tail_widths(\n",
    "            map_data_by_ch\n",
    "        )\n",
    "        sorted_candidate_idxs = np.argsort(ch_lower_tail_width_list)\n",
    "    \n",
    "    if ascend_sort:\n",
    "        sorted_candidate_idxs = np.flip(sorted_candidate_idxs)\n",
    "    \n",
    "    map_data_by_ch = np.array(map_data_by_ch)\n",
    "    ranked_ch_ims = map_data_by_ch[sorted_candidate_idxs]\n",
    "    \n",
    "    ranked_map = np.where(\n",
    "        ~np.isnan(array), 0, np.nan\n",
    "    )\n",
    "    for isolated_ch_im, ch_num in zip(ranked_ch_ims, range(num_ch)):\n",
    "        ranked_map = np.where(\n",
    "            ~np.isnan(isolated_ch_im), (ch_num + 1)*100/num_ch, ranked_map\n",
    "        )\n",
    "    return ranked_map\n",
    "\n",
    "\n",
    "def plot_ensemble_comparison(he_map, ensemble_map, euv_map):\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Plot He observation\n",
    "    ax = fig.add_subplot(1, 3, 1, projection=he_map)\n",
    "    he_map.plot(axes=ax, vmin=-100, vmax=100)\n",
    "    \n",
    "    # Plot ensemble map with overlayed neutral lines\n",
    "    ax = fig.add_subplot(1, 3, 2, projection=he_map)\n",
    "    ensemble_map.plot(axes=ax, title='', cmap='magma')\n",
    "    \n",
    "    # Plot EUV observation\n",
    "    ax = fig.add_subplot(1, 3, 3, projection=euv_map)\n",
    "    euv_map.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brightness Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=False, hist_stat='width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brightness Tail Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=False, hist_stat='tail_width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=True, hist_stat='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=True, hist_stat='width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ranking by different smoothness statistics for a single date\n",
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "apply_gradient_list = [False, False, True, True]\n",
    "hist_stat_list = ['width', 'tail_width', 'median', 'width']\n",
    "\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "    \n",
    "for apply_gradient, hist_stat in zip(apply_gradient_list, hist_stat_list):\n",
    "    ranked_map_data = get_ranked_map(\n",
    "        pre_process_v0_1_he, ch_mask, apply_gradient, hist_stat\n",
    "    )\n",
    "    \n",
    "    ranked_map = sunpy.map.Map(np.flipud(ranked_map_data), he_map.meta)\n",
    "    plot_ensemble_comparison(he_map, ranked_map, euv_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates: Missing Helioprojective Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without modifying when reading FITS file\n",
    "with fits.open(he_fits_file) as hdu_list:\n",
    "    header = hdu_list[-1].header\n",
    "    num_data_arrays = header.get('NAXIS3')\n",
    "    \n",
    "    if not num_data_arrays:\n",
    "        data = hdu_list[-1].data\n",
    "    else:\n",
    "        data = hdu_list[-1].data[0]\n",
    "\n",
    "he_map = sunpy.map.Map(data, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup Header Heliocentric delt/pix To Helioprojective delt/pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian distance change per pixel\n",
    "hc_delta_coords = frames.Heliocentric(\n",
    "    header['CDELT1A']*u.Unit(header['CUNIT1A']),\n",
    "    header['CDELT2A']*u.Unit(header['CUNIT2A']),\n",
    "    z=0*u.m, observer='earth', obstime=header['DATE-OBS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunpy coordinate transform\n",
    "hp_delta_coords = hc_delta_coords.transform_to(\n",
    "    frames.Helioprojective(\n",
    "        header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    ")\n",
    "print(f'HP Scale: {hp_delta_coords.Tx.value:.11f}, {hp_delta_coords.Ty.value:.11f} arcsec/pix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thompson 2005 method section 4.1\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "\n",
    "# Sun-Earth distance\n",
    "Ds = earth_header['dsun_obs']*u.m\n",
    "\n",
    "hp_delta_Tx = (hc_delta_coords.x/Ds).to(u.dimensionless_unscaled)\n",
    "hp_delta_Tx = (hp_delta_Tx*u.rad).to(u.arcsec)\n",
    "hp_delta_Ty = (hc_delta_coords.y/Ds).to(u.dimensionless_unscaled)\n",
    "hp_delta_Ty = (hp_delta_Ty*u.rad).to(u.arcsec)\n",
    "\n",
    "print(f'HP Scale: {hp_delta_Tx.value:.11f}, {hp_delta_Ty.value:.11f} arcsec/pix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Map Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(35):\n",
    "    ax.plot_coord(contour, color='white')\n",
    "    \n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(65):\n",
    "    ax.plot_coord(contour, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out detections below a CL threshold\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ensemble_map.mask = (ensemble_map.data > 50)\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(np.flipud(ensemble_map.mask), cmap='magma')\n",
    "\n",
    "ensemble_map.mask = (ensemble_map.data < 50)\n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "\n",
    "ensemble_map.mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold Map by Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 20\n",
    "\n",
    "\n",
    "confidence_map = np.where(ensemble_map_data >= confidence_level, ensemble_map_data, 0)\n",
    "labeled_map, num_ch = ndimage.label(confidence_map)\n",
    "\n",
    "confidence_map = np.where(np.isnan(ensemble_map_data), np.nan, confidence_map)\n",
    "labeled_map = np.where(np.isnan(ensemble_map_data), np.nan, labeled_map)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 10))\n",
    "\n",
    "ax[0].set_title(he_date_str)\n",
    "ax[0].imshow(he, cmap=plt.cm.gray)\n",
    "\n",
    "ax[1].imshow(ensemble_map_data, cmap=plt.cm.magma)\n",
    "ax[2].imshow(labeled_map, cmap=plt.cm.bone)\n",
    "print(num_ch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ensemble_data = np.where(~np.isnan(np.flipud(ensemble_map.data)), 1, np.nan)\n",
    "ensemble_map = sunpy.map.Map(fake_ensemble_data, pre_processed_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Area: Compare official function with decomposed to investigate failed retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLAR_AREA = 4*np.pi *(1*u.solRad).to(u.Mm)**2\n",
    "\n",
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    hp_delta_coord = frames.Helioprojective(\n",
    "        ensemble_map.scale.axis1*u.pix,\n",
    "        ensemble_map.scale.axis2*u.pix,\n",
    "        observer='earth', obstime=ensemble_map.date\n",
    "    )\n",
    "    hc_delta_coord = hp_delta_coord.transform_to(\n",
    "        frames.Heliocentric(observer='earth', obstime=ensemble_map.date)\n",
    "    )\n",
    "    A_per_square_px = np.abs(\n",
    "        hc_delta_coord.x.to(u.Mm)*hc_delta_coord.y.to(u.Mm)\n",
    "    )\n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "\n",
    "    x_scale, y_scale = detect.get_hg_map_dist_scales(pre_processed_map)\n",
    "    A_per_square_px = x_scale*y_scale\n",
    "else:\n",
    "    raise Exception(('Coordinate frame not recognized for obtaining '\n",
    "                     'area per square pixel.'))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Detected pixels at a confidence level\n",
    "# Flip upside down to align Sunpy coordinates and Numpy indices\n",
    "detected_px_coords = np.where(\n",
    "    np.flipud(ensemble_map.data) >= confidence_level\n",
    ")\n",
    "\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    \n",
    "    # Convert detected pixels to Helioprojective Tx, Ty\n",
    "    detected_hp_coords = ensemble_map.pixel_to_world(\n",
    "        detected_px_coords[1]*u.pix, detected_px_coords[0]*u.pix\n",
    "    )\n",
    "\n",
    "    # Convert detected Helioprojective Tx, Ty to Heliographic lon, lat\n",
    "    raw_detected_hg_coords = detected_hp_coords.transform_to(\n",
    "        frames.HeliographicStonyhurst(obstime=ensemble_map.date)\n",
    "    )\n",
    "\n",
    "    # Remove pixels with failed conversion and longitudes outside (-90,90)\n",
    "    failed_coord_idxs = np.where(\n",
    "        np.isnan(raw_detected_hg_coords.lon) \n",
    "        | (np.abs(raw_detected_hg_coords.lon.to(u.deg).value) >= 90)\n",
    "    )\n",
    "    detected_hg_coords = np.delete(\n",
    "        raw_detected_hg_coords, failed_coord_idxs\n",
    "    )\n",
    "    \n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "    \n",
    "    # Convert detected pixels to Heliographic lon, lat\n",
    "    detected_hg_coords = ensemble_map.pixel_to_world(\n",
    "        detected_px_coords[1]*u.pix, detected_px_coords[0]*u.pix\n",
    "    )\n",
    "    failed_coord_idxs = np.array([], dtype=np.int8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    \n",
    "    # B-angle to subtract from latitude\n",
    "    B0 = ensemble_map.observer_coordinate.lat\n",
    "\n",
    "    pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "    pixel_lats = detected_hg_coords.lat.to(u.rad).value - B0.to(u.rad).value\n",
    "    pixel_areas = A_per_square_px/(np.cos(pixel_lons)*np.cos(pixel_lats))\n",
    "\n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "    pixel_areas = np.ones(detected_hg_coords.shape)*A_per_square_px\n",
    "\n",
    "# Sum area detected in all pixels\n",
    "area = np.sum(pixel_areas)\n",
    "area_percent = area/SOLAR_AREA*100\n",
    "area_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect.get_open_area(ensemble_map, confidence_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(detected_hg_coords.lat > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import CylindricalRepresentation\n",
    "\n",
    "# WRONG BC RHO IS CYLINDRICAL AS OPPOSED TO SPHERICAL\n",
    "# # Convert detected Helioprojective Sky Coords to Heliocentric radial rho, psi\n",
    "# raw_pixel_hc_coords = pixel_hp_coords.transform_to(\n",
    "#     frames.Heliocentric(observer='earth', obstime=obstime)\n",
    "# )\n",
    "# # raw_pixel_hc_coords = raw_pixel_hc_coords.represent_as(CylindricalRepresentation)\n",
    "# pixel_hc_coords = raw_pixel_hc_coords[np.where(~np.isnan(raw_pixel_hc_coords.x))]\n",
    "\n",
    "# # Compute area per pixel while accounting for foreshortening\n",
    "# rho = np.sqrt(pixel_hc_coords.x**2 + pixel_hc_coords.y**2 + pixel_hc_coords.z**2)\n",
    "# pixel_sol_rad_ratios = (pixel_hc_coords.rho/u.solRad).to(u.dimensionless_unscaled)\n",
    "# pixel_angles_to_limb = pixel_sol_rad_ratios*np.pi/2 *u.rad\n",
    "# pixel_areas = A_per_square_px/np.cos(pixel_angles_to_limb.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed Coordinate Conversion Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(failed_coord_idxs):\n",
    "    failed_hp_coords = detected_hp_coords[failed_coord_idxs]\n",
    "    failed_pixel_pairs = ensemble_map.world_to_pixel(failed_hp_coords)\n",
    "else:\n",
    "    print('No points matched condition')\n",
    "\n",
    "ensemble_map.mask = None\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "failed_point_color = '#1ed950'\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(ensemble_map.data, cmap='magma')\n",
    "if np.any(failed_coord_idxs):\n",
    "    ax.scatter(failed_pixel_pairs.x.value, failed_pixel_pairs.y.value,\n",
    "               color=failed_point_color)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(confidence_level):\n",
    "    ax.plot_coord(contour, color='white')\n",
    "\n",
    "if np.any(failed_coord_idxs):\n",
    "    ax.plot_coord(failed_hp_coords, 'o', color=failed_point_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heliographic pixels with too large longitude\n",
    "large_lon_pixel_hg_coords = raw_detected_hg_coords[\n",
    "    np.where(~np.isnan(raw_detected_hg_coords.lon)\n",
    "             & (raw_detected_hg_coords.lon.to(u.deg).value >= 90))\n",
    "]\n",
    "large_lon_pixel_hg_coords\n",
    "# large_lon_pixel_hg_coords[np.argsort(large_lon_pixel_hg_coords.lon.to(u.deg).value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All failed conversion pixels\n",
    "pixel_hg_coords = raw_detected_hg_coords[failed_detect_idxs]\n",
    "pixel_hg_coords\n",
    "\n",
    "# HG\n",
    "# nan_idx = np.argmax(pixel_hg_coords.lon.to(u.deg).value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct B-Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPH_B0 keyword: [deg] Disk center solar latitude at DATE-AVG\n",
    "# Yields a lat offset in HG Stonyhurst coordinates\n",
    "he_map.center\n",
    "he_map.reference_coordinate\n",
    "he_map.center.observer.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth observer HGLT_OBS keyword\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "earth_header['HGLT_OBS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limb Size Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonyhurst_frame = frames.HeliographicStonyhurst(obstime=he_map.date)\n",
    "\n",
    "num_points = 100\n",
    "lon_value = -50 * u.deg\n",
    "lat_value = 0 * u.deg\n",
    "constant_lon = SkyCoord(lon_value, np.linspace(-90, 90, num_points) * u.deg,\n",
    "                        frame=stonyhurst_frame)\n",
    "constant_lat = SkyCoord(np.linspace(-90, 90, num_points) * u.deg, lat_value,\n",
    "                        frame=stonyhurst_frame)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection=he_map)\n",
    "\n",
    "# north = SkyCoord(0 * u.deg, 10 * u.deg, frame=\"heliographic_stonyhurst\")\n",
    "# offset_frame = NorthOffsetFrame(north=north)\n",
    "# overlay = ax.get_coords_overlay(offset_frame)\n",
    "# overlay[0].set_ticks(spacing=30. * u.deg)\n",
    "# overlay.grid(ls='--', color='blue')\n",
    "\n",
    "ax.plot_coord(constant_lon, color=\"lightblue\")\n",
    "ax.plot_coord(constant_lat, color=\"tomato\")\n",
    "# he_map.draw_grid(axes=ax, grid_spacing=10*u.deg)\n",
    "he_map.draw_limb(axes=ax)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "pixel_lats = detected_hg_coords.lat.to(u.rad).value - B0.to(u.rad).value\n",
    "pixel_areas = A_per_square_px/(np.cos(pixel_lons)*np.cos(pixel_lats))\n",
    "pixel_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Off Limb Coords: \n",
    "# https://docs.sunpy.org/en/stable/generated/api/sunpy.coordinates.utils.get_limb_coordinates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMB_FACTOR = 1.5\n",
    "\n",
    "# def get_nso_sunpy_map(fits_file):\n",
    "#     \"\"\"Retrieve a Sunpy map with a Helioprojective Cartesian\n",
    "#     coordinate system and the first data array in a SOLIS VSM FITS file.\n",
    "    \n",
    "#     Args\n",
    "#         fits_file: path to FITS file\n",
    "#     Returns\n",
    "#         Sunpy map object.\n",
    "#     \"\"\"\n",
    "with fits.open(fits_file) as hdu_list:\n",
    "    header = hdu_list[-1].header\n",
    "    num_data_arrays = header.get('NAXIS3')\n",
    "    \n",
    "    if not num_data_arrays:\n",
    "        data = hdu_list[-1].data\n",
    "    else:\n",
    "        data = hdu_list[-1].data[0]\n",
    "\n",
    "# # Apply absolute value of coordinate change per pixel such that\n",
    "# # Solar-X is positive\n",
    "# header['CDELT1'] = abs(header['CDELT1'])\n",
    "\n",
    "# Helioprojective Cartesian coordinates must have\n",
    "# arcsec units for further processing. Warning messages\n",
    "# will appear but the map will be produced successfully.\n",
    "if (header['WCSNAME'] == 'Helioprojective-cartesian'\n",
    "    and header['CUNIT1'] != 'arcsec'):\n",
    "    pass\n",
    "    \n",
    "# Heliocentric Cartesian coordinates must have zero\n",
    "# centered coordinates\n",
    "if (header['WCSNAME'] == 'Heliocentric-cartesian (approximate)'\n",
    "    and (header['CRVAL1'] != 0 or header['CRVAL2'] != 0)):\n",
    "    print((f'Failed to convert {fits_file} into a Sunpy map.')\n",
    "            + ('Coordinates were Heliocentric but were not ')\n",
    "            + ('zero centered.'))\n",
    "    pass\n",
    "    \n",
    "# Specify Earth-based observer for solar radius, distance to Sun,\n",
    "# and Heliographic coordinates to avoid warning messages due to\n",
    "# missing keywords\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "for earth_coord_key in ['DSUN_OBS', 'HGLN_OBS', 'HGLT_OBS']:\n",
    "    header[earth_coord_key] = earth_header[earth_coord_key]\n",
    "\n",
    "# Enlarge solar radius by a factor to account for larger apparent solar\n",
    "# limb in He I observations\n",
    "header['RSUN_REF'] = (100 + LIMB_FACTOR)/100 * earth_header['RSUN_REF']\n",
    "\n",
    "# Change primary World Coordinate System from Heliocentric Cartesian\n",
    "# to Helioprojective Cartesian for Sunpy to create map\n",
    "if header['WCSNAME'] == 'Heliocentric-cartesian (approximate)':\n",
    "    \n",
    "    # Cartesian coordinate units\n",
    "    coord_u1 = u.Unit(header['CUNIT1'])\n",
    "    coord_u2 = u.Unit(header['CUNIT2'])\n",
    "    \n",
    "    # Convert center pixel coordinates from distance to angle\n",
    "    hc_coords = frames.Heliocentric(\n",
    "        header['CRVAL1']*coord_u1,\n",
    "        header['CRVAL2']*coord_u2, z=0*u.m,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    "    hp_coords = hc_coords.transform_to(earth_hp_coords)\n",
    "    header['CRVAL1'] = hp_coords.Tx.value\n",
    "    header['CRVAL2'] = hp_coords.Ty.value\n",
    "    \n",
    "    # Convert change per pixel from distance to angle\n",
    "    hc_delta_coords = frames.Heliocentric(\n",
    "        header['CDELT1']*coord_u1,\n",
    "        header['CDELT2']*coord_u2, z=0*u.m,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    "    hp_delta_coords = hc_delta_coords.transform_to(earth_hp_coords)\n",
    "    header['CDELT1'] = hp_delta_coords.Tx.value\n",
    "    header['CDELT2'] = hp_delta_coords.Ty.value\n",
    "    \n",
    "    # Modify keywords\n",
    "    header['WCSNAME'] = 'Helioprojective-cartesian'\n",
    "    header['CTYPE1'] = 'HPLN-TAN'\n",
    "    header['CTYPE2'] = 'HPLT-TAN'\n",
    "    header['CUNIT1'] = 'arcsec'\n",
    "    header['CUNIT2'] = 'arcsec'\n",
    "\n",
    "    # Remove error causing keywords indicate presence of\n",
    "    # coordinate transformation\n",
    "    header.pop('PC1_1')\n",
    "    header.pop('PC2_2')\n",
    "        \n",
    "he_map = sunpy.map.Map(data, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(he_map.rsun_meters/u.solRad).to(u.dimensionless_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnetogram Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = fig.add_subplot(121, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "for contour in ensemble_map.contour(0):\n",
    "    ax.plot_coord(contour, color='yellow')\n",
    "\n",
    "# Mask out data\n",
    "reprojected_mag_map.mask = (ensemble_map.data < 1)\n",
    "ax = fig.add_subplot(122, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-0.1, vmax=0.1, cmap='coolwarm')\n",
    "\n",
    "reprojected_mag_map.mask = None\n",
    "# Red: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Flux: Compare official function with decomposed to investigate failed retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_flux = detect.get_unsigned_open_flux(\n",
    "    ensemble_map, reprojected_mag_map, confidence_level=0\n",
    ")\n",
    "f'{open_flux:.7e} Wb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "    \n",
    "A_per_square_px = detect.get_A_per_square_px(ensemble_map)\n",
    "        \n",
    "detected_hg_coords, failed_coord_idxs = detect.get_detected_hg_coords(\n",
    "    ensemble_map, confidence_level\n",
    ")\n",
    "\n",
    "pixel_areas = detect.get_pixel_areas(\n",
    "    ensemble_map, A_per_square_px, detected_hg_coords\n",
    ")\n",
    "\n",
    "# Magnetic field strength per detected pixel\n",
    "# Flip upside down to align Sunpy coordinates and Numpy indices\n",
    "detected_idxs = np.where(np.flipud(ensemble_map.data) >= confidence_level)\n",
    "pixel_B_LOS = reprojected_mag_map.data[detected_idxs]*u.G\n",
    "\n",
    "# Remove pixels with failed coordinate conversion\n",
    "pixel_B_LOS = np.delete(pixel_B_LOS, failed_coord_idxs)\n",
    "\n",
    "# Remove pixels with failed magnetic data retrieval\n",
    "failed_mag_idxs = np.where(np.isnan(pixel_B_LOS))\n",
    "pixel_B_LOS = np.delete(pixel_B_LOS, failed_mag_idxs)\n",
    "\n",
    "\n",
    "pixel_areas = np.delete(pixel_areas, failed_mag_idxs)\n",
    "    \n",
    "unsigned_open_flux = np.sum(np.abs(pixel_B_LOS)*pixel_areas).to(u.Wb)\n",
    "\n",
    "unsigned_open_flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed Magnetic Retrieval Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(failed_mag_idxs):\n",
    "    failed_hp_coords = detected_hp_coords[failed_mag_idxs]\n",
    "    failed_pixel_pairs = ensemble_map.world_to_pixel(failed_hp_coords)\n",
    "else:\n",
    "    print('No points matched condition')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "failed_point_color = '#1ed950'\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(reprojected_mag_map.data, vmin=-50, vmax=50, cmap='gray')\n",
    "if np.any(failed_mag_idxs):\n",
    "    ax.scatter(failed_pixel_pairs.x.value, failed_pixel_pairs.y.value,\n",
    "               color=failed_point_color)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = fig.add_subplot(122, projection=he_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "reprojected_mag_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(confidence_level):\n",
    "    ax.plot_coord(contour, color='yellow')\n",
    "\n",
    "if np.any(failed_mag_idxs):\n",
    "    ax.plot_coord(failed_hp_coords, 'o', color=failed_point_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties Per CH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Magnetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set uniform value to detected regions to verify 0 unipolarity observed\n",
    "fake_mag_map_data = np.where(ensemble_map.data > 1, 25, reprojected_mag_map.data)\n",
    "fake_mag_map = sunpy.map.Map(fake_mag_map_data, reprojected_mag_map.meta)\n",
    "fake_mag_map.plot(vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "outcome_by_ch_dict = detect.get_outcomes_by_ch(\n",
    "    ensemble_map, pre_processed_map, fake_mag_map, confidence_level\n",
    ")\n",
    "\n",
    "# Mask of detected CHs at the given confidence level\n",
    "confidence_ch_mask = np.where(\n",
    "    ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    ")\n",
    "\n",
    "# List of ensemble map data for distinct CHs\n",
    "ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    ensemble_map.data, confidence_ch_mask\n",
    ")\n",
    "\n",
    "[print(f'{unipolarity:.4f}', end='\\t')\n",
    " for unipolarity in outcome_by_ch_dict['unipolarity']]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify vs Global Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLAR_AREA = 4*np.pi*(1*u.solRad).to(u.Mm)**2\n",
    "\n",
    "summed_area = np.sum(outcome_by_ch_dict['area'])*u.Mm**2 /SOLAR_AREA*100\n",
    "global_area = detect.get_open_area(ensemble_map, confidence_level=0)[0]\n",
    "\n",
    "f'Summed: {summed_area:.6f} % | Global: {global_area:.6f} %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_flux = np.sum(outcome_by_ch_dict['unsigned_flux'])\n",
    "global_flux = detect.get_unsigned_open_flux(\n",
    "    ensemble_map, reprojected_mag_map, confidence_level=0\n",
    ")\n",
    "f'{summed_flux:.6e} Wb | Global: {global_flux:.6e} Wb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "# Compute outcomes by CH and sort from greatest to least\n",
    "outcome_by_ch_dict = detect.get_outcomes_by_ch(\n",
    "    ensemble_map, he_map_data, reprojected_mag_map, confidence_level\n",
    ")\n",
    "sorted_idxs = np.flip(np.argsort(outcome_by_ch_dict['unipolarity']))\n",
    "\n",
    "sorted_outcome_by_ch_dict = {}\n",
    "for key, outcome_by_ch in zip(outcome_by_ch_dict, outcome_by_ch_dict.values()):\n",
    "    sorted_outcome_by_ch_dict[key] = [outcome_by_ch[i] for i in sorted_idxs]\n",
    "\n",
    "    \n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "\n",
    "# Mask of detected CHs at the given confidence level\n",
    "confidence_ch_mask = np.where(\n",
    "    ensemble_map.data >= confidence_level, 1, 0\n",
    ")\n",
    "\n",
    "# List of ensemble map data for distinct CHs\n",
    "ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    ensemble_map.data, confidence_ch_mask\n",
    ")\n",
    "ensemble_map_data_by_ch = [ensemble_map_data_by_ch[i] for i in sorted_idxs]\n",
    "\n",
    "\n",
    "[print(f'{area:.1e} Mm^2', end='\\t')\n",
    " for area in sorted_outcome_by_ch_dict['area']]\n",
    "print()\n",
    "[print(f'Lat: {cm_lat:.1f} deg', end='\\t')\n",
    " for cm_lat in sorted_outcome_by_ch_dict['cm_lat']]\n",
    "print()\n",
    "[print(f'Lon: {cm_lon:.1f} deg', end='\\t')\n",
    " for cm_lon in sorted_outcome_by_ch_dict['cm_lon']]\n",
    "print()\n",
    "[print(f'{signed_flux:.4e} Mx', end='\\t')\n",
    " for signed_flux in sorted_outcome_by_ch_dict['signed_flux']]\n",
    "print()\n",
    "[print(f'Skew: {mag_skew:.4f}', end='\\t')\n",
    " for mag_skew in sorted_outcome_by_ch_dict['mag_skew']]\n",
    "print()\n",
    "[print(f'U: {unipolarity:.4f}', end='\\t')\n",
    " for unipolarity in sorted_outcome_by_ch_dict['unipolarity']]\n",
    "print()\n",
    "[print(f'Grad: {grad_median:.4f}', end='\\t')\n",
    " for grad_median in sorted_outcome_by_ch_dict['grad_median']]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_idx = np.argmax(np.abs(sorted_outcome_by_ch_dict['mag_skew']))\n",
    "# ch_idx = np.argmax(sorted_outcome_by_ch_dict['unipolarity'])\n",
    "# ch_idx = np.argmax(sorted_outcome_by_ch_dict['area'])\n",
    "ch_idx = np.argmax(sorted_outcome_by_ch_dict['grad_median'])\n",
    "\n",
    "# area = area_by_ch[ch_idx]\n",
    "signed_flux = sorted_outcome_by_ch_dict['signed_flux'][ch_idx]\n",
    "mag_skew = sorted_outcome_by_ch_dict['mag_skew'][ch_idx]\n",
    "unipolarity = sorted_outcome_by_ch_dict['unipolarity'][ch_idx]\n",
    "grad_median = sorted_outcome_by_ch_dict['grad_median'][ch_idx]\n",
    "# title = f'{area:.2e} Mm^2 | {signed_flux:.2e} Mx | {mag_skew:.2f} Skew'\n",
    "# title = f'{signed_flux:.2e} Mx | {unipolarity:.2f} Unipolarity | {mag_skew:.2f} Skew'\n",
    "title = f'{unipolarity:.2f} Unipolarity | {grad_median:.2f} Gradient Median'\n",
    "\n",
    "A_per_square_px = detect.get_A_per_square_px(ensemble_map)        \n",
    "\n",
    "\n",
    "selected_ch_map_data = ensemble_map_data_by_ch[ch_idx]\n",
    "selected_ch_map_data = np.where(np.isnan(selected_ch_map_data), -100,\n",
    "                                selected_ch_map_data)\n",
    "selected_ch_map = sunpy.map.Map(selected_ch_map_data, he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.suptitle(he_date_str)\n",
    "\n",
    "ax = fig.add_subplot(projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title=title)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in selected_ch_map.contour(0):\n",
    "    ax.plot_coord(contour, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank each CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_mag_map(ensemble_map, confidence_level, sorted_idxs):\n",
    "    \"\"\"Retrieve a map of CHs from a single segmentation as ranked by a\n",
    "    histogram statistic.\n",
    "    \n",
    "    Args\n",
    "        ensemble_map_data: map data which was segmented\n",
    "        ch_mask: binary coronal holes mask\n",
    "    Returns\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Mask of detected CHs at the given confidence level\n",
    "    confidence_ch_mask = np.where(\n",
    "        ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    "    )\n",
    "\n",
    "    # List of ensemble map data for distinct CHs\n",
    "    ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        ensemble_map.data, confidence_ch_mask\n",
    "    )\n",
    "    num_ch = len(ensemble_map_data_by_ch)\n",
    "    ensemble_map_data_by_ch = [ensemble_map_data_by_ch[i]\n",
    "                               for i in np.flip(sorted_idxs)]\n",
    "    \n",
    "    ranked_map_data = np.where(\n",
    "        ~np.isnan(ensemble_map.data), 0, np.nan\n",
    "    )\n",
    "    for map_data, ch_num in zip(ensemble_map_data_by_ch, range(num_ch)):\n",
    "        ranked_map_data = np.where(\n",
    "            ~np.isnan(map_data), (ch_num + 1)*100/num_ch, ranked_map_data\n",
    "        )\n",
    "    \n",
    "    ranked_map = sunpy.map.Map(ranked_map_data, he_map.meta)\n",
    "    return ranked_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = len(ensemble_map_data_by_ch)\n",
    "image_list = [pre_processed_map_data for _ in range(num_ch)]\n",
    "axes = plot_detection.plot_image_grid(image_list, num_cols=3, cmap='gray')\n",
    "\n",
    "for ax, i, ch_data in zip(axes.values(), range(num_ch), ensemble_map_data_by_ch):\n",
    "    mask = np.where(np.isnan(ch_data), 0, 1)\n",
    "    \n",
    "    ax.set_title((f'{sorted_outcome_by_ch_dict[\"unipolarity\"][i]:.2f} Unipolarity | '\n",
    "                  f'{sorted_outcome_by_ch_dict[\"mag_skew\"][i]:.2f} Skew'))\n",
    "    ax.contour(np.flipud(mask), cmap=plt.cm.plasma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save maps thresholded above a confidence level and then sorted by that outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEUTRAL LINE COMPARISON NEEDS UPDATE\n",
    "overwrite = False\n",
    "confidence_level = 0\n",
    "\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Unipolarity_Rank/'\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}He{he_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'EUV {euv_date_str} comparison already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_map = prepare_data.get_nso_sunpy_map(ALL_HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    # Extract saved ensemble map array and convert to Sunpy map\n",
    "    ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "    ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "    # Extract saved processed magnetograms\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    mag_fits_name = f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}_He{he_date_str}'\n",
    "    reprojected_fits_file = f'{mag_fits_name}.fits'\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "    # Compute outcomes by CH and sort from greatest to least\n",
    "    # TODO: Update get_outcomes_by_ch call to dict format\n",
    "    outcomes_by_ch = detect.get_outcomes_by_ch(\n",
    "        ensemble_map, pre_processed_map, reprojected_mag_map, confidence_level\n",
    "    )\n",
    "    unipolarity_by_ch = outcomes_by_ch[4]\n",
    "    sorted_idxs = np.argsort(unipolarity_by_ch)\n",
    "\n",
    "    # Obtain ranked map\n",
    "    ranked_map = get_ranked_mag_map(\n",
    "        ensemble_map, confidence_level, sorted_idxs\n",
    "    )\n",
    "    ranked_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "    euv_date_str = prepare_data.get_nearest_date_str(\n",
    "        EUV_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    plot_detection.plot_he_neutral_lines_euv_comparison(\n",
    "        fig, he_date_str, mag_date_str, euv_date_str, ranked_map\n",
    "    )\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(comparison_img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{euv_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness Histogram in Single CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sym_log_hist(data_list, num_bins):\n",
    "    # Symmetric log bins\n",
    "    neg_outcomes = np.where(data_list < 0, np.abs(data_list), 0)\n",
    "    bin_min = np.ceil(np.log10(np.max(neg_outcomes)))\n",
    "    bin_max = np.ceil(np.log10(np.max(np.abs(data_list))))\n",
    "    bins = np.hstack((-np.logspace(bin_min, 0, num_bins//2),\n",
    "                    np.logspace(0, bin_max, num_bins//2)))\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xscale('symlog')\n",
    "    ax.hist(data_list, bins)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = sorted_outcome_by_ch_dict['pixel_signed_fluxes'][ch_idx]\n",
    "fig, ax = plot_sym_log_hist(data_list, num_bins=500)\n",
    "fig.suptitle(f'CH Index: {ch_idx}')\n",
    "ax.set_title('Pixel Magnetic Flux Histogram')\n",
    "ax.set_xlabel('Magnetic Flux (Wb)')\n",
    "ax.set_ylim([0,3500])\n",
    "f'Summed: {np.sum(data_list):.3e} | Pre-Computed {sorted_outcome_by_ch_dict[\"signed_flux\"][ch_idx]:.3e}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line of Sight B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "    \n",
    "percent_unipolar_by_ch = []\n",
    "signed_B_by_ch = []\n",
    "unsigned_B_by_ch = []\n",
    "unipolarity_by_ch = []\n",
    "\n",
    "# Thresholded array at the given confidence level\n",
    "confidence_map_data = np.where(\n",
    "    ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    ")\n",
    "\n",
    "# Array with number labels per distinct CH and number of labels\n",
    "labeled_map_data, num_labels = ndimage.label(confidence_map_data)\n",
    "num_ch = num_labels - 1\n",
    "\n",
    "# List of magnetic field data images for distinct CHs.\n",
    "# Equivalent to obtain Helioprojective coordinates of pixels per CH\n",
    "# and then obtaining magnetic data at HP coordinates\n",
    "mag_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    reprojected_mag_map.data, confidence_map_data\n",
    ")\n",
    "\n",
    "# # Obtain masked magnetic data per CH > Compute on magnetic data per CH\n",
    "# mask_idxs_by_ch = [np.where(labeled_map_data == label)\n",
    "#                    for label in range(1, num_labels)]\n",
    "# mag_data_by_ch = [reprojected_mag_map.data[mask_idxs]\n",
    "#                   for mask_idxs in mask_idxs_by_ch]\n",
    "\n",
    "# List of masks for distinct CHs\n",
    "masks_by_ch = [np.where(labeled_map_data == label, 1, 0)\n",
    "               for label in range(1, num_labels)]\n",
    "\n",
    "for ch_label in range(num_ch):\n",
    "    mag_map_data = mag_map_data_by_ch[ch_label]\n",
    "    mag_data = mag_map_data[~np.isnan(mag_map_data)]\n",
    "    \n",
    "    # Pixel percent unipolarity\n",
    "    num_positive = np.count_nonzero(mag_data > 0)\n",
    "    num_negative = np.count_nonzero(mag_data < 0)\n",
    "    num_px = np.count_nonzero(mag_data)\n",
    "    percent_unipolarity = max(num_positive, num_negative)*100/num_px\n",
    "    percent_unipolar_by_ch.append(percent_unipolarity)\n",
    "    \n",
    "    # Signed average magnetic field\n",
    "    signed_B = np.abs(np.mean(mag_data))\n",
    "    signed_B_by_ch.append(signed_B)\n",
    "\n",
    "    # Unsigned average magnetic field\n",
    "    unsigned_B = np.mean(np.abs(mag_data))\n",
    "    unsigned_B_by_ch.append(unsigned_B)\n",
    "    \n",
    "    # Unipolarity\n",
    "    unipolarity = (unsigned_B - signed_B)/unsigned_B\n",
    "    unipolarity_by_ch.append(unipolarity)\n",
    "\n",
    "\n",
    "# Sort outcomes by CH from greatest to least\n",
    "sorted_idxs = np.flip(np.argsort(percent_unipolar_by_ch))\n",
    "percent_unipolar_by_ch.sort(reverse=True)\n",
    "\n",
    "# Sort candidate CHs from greatest to least outcome median\n",
    "masks_by_ch = [masks_by_ch[i] for i in sorted_idxs]\n",
    "signed_B_by_ch = [signed_B_by_ch[i] for i in sorted_idxs]\n",
    "unsigned_B_by_ch = [unsigned_B_by_ch[i] for i in sorted_idxs]\n",
    "unipolarity_by_ch = [unipolarity_by_ch[i] for i in sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = [pre_processed_map for _ in range(num_ch)]\n",
    "axes = plot_detection.plot_image_grid(image_list, num_cols=3, cmap='gray')\n",
    "\n",
    "for ax, i, mask in zip(axes.values(), range(num_ch), masks_by_ch):\n",
    "    ax.set_title((f'{percent_unipolar_by_ch[i]:.1f}% Unipolar '\n",
    "                  + f'| {unipolarity_by_ch[i]:.2f} Unipolarity'))\n",
    "    ax.contour(np.flipud(mask), cmap=plt.cm.plasma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.CDM.fracstat import *\n",
    "\n",
    "def get_fractal_D(img, scale_range):\n",
    "    # Compute fractal dimension\n",
    "    \n",
    "    scales, n_filled = box_counting(img, scale_range, f=1.1)[:2]\n",
    "    if np.any(n_filled == 0):\n",
    "        D = np.nan\n",
    "    else:\n",
    "        fit = power_law(scales, n_filled, scale_range)[0]\n",
    "        D = -fit[0]\n",
    "    \n",
    "    return D\n",
    "    \n",
    "\n",
    "def plot_fractal_D(img, scale_range, title_var, contours=False):\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    if contours:\n",
    "        ax.contour(img, cmap='gray')\n",
    "    \n",
    "    ax.set_title(title_var)\n",
    "    \n",
    "    # Plot full range\n",
    "    img_row_num = img.shape[0]\n",
    "    full_range = [min([5, scale_range[0]]), max(img_row_num, scale_range[1])]\n",
    "    full_scales, full_n_filled = box_counting(img, scale_range=full_range, f=1.1)[:2]\n",
    "    \n",
    "    ax = fig.add_subplot(212)\n",
    "    full_X = full_scales/img_row_num\n",
    "    ax.loglog(full_X, full_n_filled, c='k')\n",
    "    \n",
    "    # Compute fractal dimension in specified range and plot selected range\n",
    "    scales, n_filled = box_counting(img, scale_range, f=1.1)[:2]\n",
    "    X = scales/img_row_num\n",
    "    fit, cov = power_law(X, n_filled)\n",
    "    predict = np.poly1d(fit)\n",
    "    D = -fit[0]\n",
    "    D_err = np.sqrt(cov[0,0])\n",
    "    \n",
    "    X_range = np.array(scale_range)/img_row_num\n",
    "    ax.loglog(X_range, 10**predict(np.log10(X_range)), c='C0', linewidth=2)\n",
    "    ax.vlines(X_range, ymin=0, ymax=1e6, linestyles='--', colors='k')\n",
    "    \n",
    "    ax.set_xlabel(r'$\\epsilon$')\n",
    "    ax.set_ylabel('Box Number Containing Boundary')\n",
    "    ax.set_title(f'Fractal Dimension: {D:.3f} +/- {D_err:.4f}')\n",
    "    ax.set_ylim([1,1e5])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = measure.find_contours(ch_mask_data)\n",
    "ch_boundary_data = np.zeros(ch_mask_data.shape)\n",
    "\n",
    "for contour in contours:\n",
    "    contour = contour.astype(int)\n",
    "    ch_boundary_data[contour[:,0], contour[:,1]] = 1\n",
    "\n",
    "fig = plot_fractal_D(\n",
    "    ch_boundary_data,\n",
    "    scale_range=[10, 500],\n",
    "    title_var=f'SE Disk Radius: {morph_radius_dist} Mm',\n",
    "    contours=True\n",
    ")\n",
    "# plt.savefig(f'{OUTPUT_DIR}Fractal/{percent_of_peak}_{morph_radius_dist}_Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fractal_D(ch_boundary_data, scale_range=[10, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_fractal_D(\n",
    "    ch_mask_data,\n",
    "    scale_range=[10, 500],\n",
    "    # scale_range=[10, np.min(ch_mask_data.shape)],\n",
    "    title_var=f'SE Disk Radius: {morph_radius_dist} Mm'\n",
    ")\n",
    "plt.savefig(f'{OUTPUT_DIR}Fractal/{morph_radius_dist}_Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process Outcomes vs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80, 90, 100, 110]\n",
    "area_percent_df_by_method_list = []\n",
    "mad_by_thresh_by_method_list = []\n",
    "\n",
    "for pre_process_save_dir in ['v0_1/', 'v0_4/']:\n",
    "    pre_process_save_dir = out_dir + PREPROCESS_DIR + pre_process_save_dir\n",
    "    \n",
    "    area_percent_df = detect.get_thresh_outcome_time_series_dfs(\n",
    "        HE_DATE_LIST, percent_of_peak_list, ALL_HE_DIR, pre_process_save_dir\n",
    "    )[1]\n",
    "    area_percent_df_by_method_list.append(area_percent_df)\n",
    "    mad_by_thresh_by_method_list.append(\n",
    "        detect.get_mad_by_confidences(area_percent_df, percent_of_peak_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(percent_of_peak_list))\n",
    "threshold_label_list = [\n",
    "    f'{thresh_level}% of Peak Threshold'\n",
    "    for thresh_level in percent_of_peak_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(8,6))\n",
    "\n",
    "plt.bar(x_ticks - 0.2, mad_by_thresh_by_method_list[0], width=0.2, label='v0.3')\n",
    "plt.bar(x_ticks, mad_by_thresh_by_method_list[1], width=0.2, label='Band Pass')\n",
    "plt.bar(x_ticks + 0.2, mad_by_thresh_by_method_list[2], width=0.2, label='Rescaling')\n",
    "plt.xticks(x_ticks, threshold_label_list, rotation=10)\n",
    "plt.ylabel(f'MAD of Detected Area Percentage (%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Outcomes vs Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare outcomes between confidence levels and/or methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = DETECT_DIR + '_Outcome_Comparison/' + DATE_DIR\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "confidence_level_list = [0, 35, 65, 95]\n",
    "# confidence_level_list = list(range(0,96,5))\n",
    "\n",
    "# version_dirs = ['v0_3/', 'Band_Pass/', 'Rescale/', 'Rescale_Center/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/', 'v0_4/']\n",
    "# version_dirs = ['v0_3/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Single/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Unipolar']\n",
    "version_dirs = ['v0_1', 'v0_2', 'v0_3', 'v0_4', 'v0_5']\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "method_list = ['Bright & Coherent Mask', 'Ensemble', 'Smoothness',\n",
    "               'Consistency', 'Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.9,0.91,0.2)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(0,1,0.05)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Band Pass', 'v0.3 Design + Rescale',\n",
    "# #               'v0.3 Design + Rescale & Center']\n",
    "# method_list = ['v0.1', 'v0.2', 'v0.3', 'v0.4']\n",
    "\n",
    "# cl_dx_list = [-0.1, 0.1]\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Rescale']\n",
    "# # method_list = ['v0.3', 'v0.4']\n",
    "# method_list = ['v0.4 Single', 'v0.4 Ensemble']\n",
    "\n",
    "# cl_dx_list = [-0.2, 0, 0.2]\n",
    "# method_list = ['v0.3', 'v0.3 Design + Rescale', 'v0.4']\n",
    "\n",
    "cmap = colormaps['viridis']\n",
    "color_list = cmap(np.linspace(0, 0.75, len(confidence_level_list)))\n",
    "# cmap = colormaps['plasma_r']\n",
    "# color_list = cmap(np.linspace(0.25, 1, len(confidence_level_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.5 Compute Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_df_by_method_list = []\n",
    "autocorr_by_conf_by_method_list = []\n",
    "mad_by_conf_by_method_list = []\n",
    "norm_mad_by_conf_by_method_list = []\n",
    "\n",
    "\n",
    "for version_dir in version_dirs:\n",
    "    detection_save_dir = os.path.join(DETECT_DIR, version_dir, 'Saved_npy_Files/')\n",
    "    \n",
    "    outcome_time_series_dict = detect.get_outcome_time_series_dict(\n",
    "        HE_DATE_LIST, confidence_level_list, detection_save_dir\n",
    "    )\n",
    "    area_percent_df_by_method_list.append(\n",
    "        outcome_time_series_dict['area_percent']\n",
    "    )\n",
    "    \n",
    "    autocorr_by_confidences = [\n",
    "        outcome_time_series_dict['area'][cl].autocorr()\n",
    "        for cl in confidence_level_list\n",
    "    ]\n",
    "    autocorr_by_conf_by_method_list.append(autocorr_by_confidences)\n",
    "    out = detect.get_mad_by_confidences(\n",
    "        outcome_time_series_dict['area'], confidence_level_list\n",
    "    )\n",
    "    mad_by_confidences, norm_mad_by_confidences = out\n",
    "    mad_by_conf_by_method_list.append(mad_by_confidences)\n",
    "    norm_mad_by_conf_by_method_list.append(norm_mad_by_confidences)\n",
    "    print(f'Outcomes computed for {version_dir}')\n",
    "\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]\n",
    "autocorr_file = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}.npy'\n",
    "np.save(autocorr_file, np.array(autocorr_by_conf_by_method_list),\n",
    "        allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(confidence_level_list))\n",
    "confidence_label_list = [\n",
    "    f'{confidence_level}% Confidence'\n",
    "    for confidence_level in confidence_level_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(9,6))\n",
    "for mad_by_confidences, cl_dx, method, color in zip(\n",
    "    mad_by_conf_by_method_list, cl_dx_list, method_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, mad_by_confidences, width=0.2,\n",
    "            label=method, color=color)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Method Comparison')\n",
    "plt.xticks(x_ticks, confidence_label_list, rotation=10)\n",
    "plt.ylabel(f'MAD of Detected Area (Mm^2)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(confidence_level_list))\n",
    "confidence_label_list = [\n",
    "    f'{confidence_level}% Confidence'\n",
    "    for confidence_level in confidence_level_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(9,6))\n",
    "for norm_mad_by_confidences, cl_dx, method, color in zip(\n",
    "    norm_mad_by_conf_by_method_list, cl_dx_list, method_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, norm_mad_by_confidences, width=0.2,\n",
    "            label=method, color=color)\n",
    "    \n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Method Comparison')\n",
    "plt.xticks(x_ticks, confidence_label_list, rotation=10)\n",
    "plt.ylim([0, 50])\n",
    "plt.ylabel(f'Normalized MAD of Detected Area (%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level_list = [1, 50, 75, 95]\n",
    "num_ch_df, area_percent_df, px_percent_df = detect.get_outcome_time_series_dfs(\n",
    "    HE_DATE_LIST[:5], confidence_level_list, DETECTION_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = px_percent_df\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_detection.plot_outcome_df_vs_time(\n",
    "    ax, outcome_df, he_date_str, cmap='plasma', ylabel='Detected Pixel Percentage (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = area_percent_df\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_detection.plot_outcome_df_vs_time(\n",
    "    ax, outcome_df, he_date_str, cmap='bone', ylabel='Detected Area Percentage (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram Moments vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_stat_list = []\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    pre_process_file = (PREPROCESS_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map_data = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    peak_counts_val = detect.get_peak_counts_loc(\n",
    "        pre_processed_map_data, bins_as_percent=False\n",
    "    )\n",
    "    hist_stat_list.append(\n",
    "        [peak_counts_val, np.nanstd(pre_processed_map_data)]\n",
    "    )\n",
    "\n",
    "# Convert to dataframes\n",
    "datetime_list = [datetime.strptime(he_date_str, DICT_DATE_STR_FORMAT)\n",
    "                 for he_date_str in HE_DATE_LIST]\n",
    "hist_df = pd.DataFrame(\n",
    "    hist_stat_list, columns=['Peak', 'StDev'],\n",
    "    index=datetime_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Histogram_Moments/'\n",
    "cmap = 'plasma'\n",
    "ylabel = 'Histogram Moments'\n",
    "\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    img_file = f'{out_dir}{he_date_str}.jpg'\n",
    "    if os.path.isfile(img_file) and not overwrite:\n",
    "        print((f'He {he_date_str} map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    hist, edges = detect.get_hist(pre_processed_map,\n",
    "                                         bins_as_percent=False)\n",
    "    \n",
    "    ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(231)\n",
    "    ax.set_title(he_date_str)\n",
    "    ax.imshow(pre_processed_map, cmap=plt.cm.gray)\n",
    "    \n",
    "    ax = fig.add_subplot(232)\n",
    "    ax.set_title('Semilog Histogram')\n",
    "    ax.semilogy(edges[1:], hist)\n",
    "    if 'Rescale' in DETECTION_VERSION_DIR:\n",
    "        ax.set_xlim([-1.3, 1.1])\n",
    "        ax.set_ylim([1E2, 5E4])\n",
    "    else:\n",
    "        ax.set_xlim([-110, 110])\n",
    "        ax.set_ylim([1E1, 5E4])\n",
    "    \n",
    "    ax = fig.add_subplot(233)\n",
    "    ax.imshow(ensemble_map, cmap=plt.cm.magma)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))\n",
    "    datetimes = hist_df.index\n",
    "    ax.plot(hist_df['StDev'], label='Standard Deviation', linewidth=3)\n",
    "    ax.plot(hist_df['Peak'], label='Mode', linewidth=3)\n",
    "    \n",
    "    # Vertical line for datetime indicator\n",
    "    vline_datetime = datetime.strptime(he_date_str, DICT_DATE_STR_FORMAT)\n",
    "    min_moment = min(hist_df.min())\n",
    "    max_moment = max(hist_df.max())\n",
    "    ax.vlines(x=[vline_datetime, vline_datetime], ymax=2*max_moment, ymin=0,\n",
    "              colors='k', linestyles='dashed')\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    ax.set_xlim([datetimes[0], datetimes[-1]])\n",
    "    if 'Rescale' in DETECTION_VERSION_DIR:\n",
    "        ax.set_ylim([0.4, 0.8])\n",
    "    else:\n",
    "        ax.set_ylim([0.9*min_moment, 1.1*max_moment])\n",
    "    \n",
    "    ax.legend(reverse=True)\n",
    "\n",
    "    plt.savefig(img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{he_date_str} map saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Process Outcomes vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80, 90, 100, 110]\n",
    "num_ch_df, area_percent_df, area_df, px_percent_df = detect.get_thresh_outcome_time_series_dfs(\n",
    "    HE_DATE_LIST, percent_of_peak_list, ALL_HE_DIR, PREPROCESS_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Thresh_Area_Percentage/'\n",
    "outcome_df = area_percent_df\n",
    "cmap = 'plasma'\n",
    "ylabel = 'Detected Area Percentage (%)'\n",
    "\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST[24:25]:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    img_file = f'{out_dir}{he_date_str}.jpg'\n",
    "    if os.path.isfile(img_file) and not overwrite:\n",
    "        print((f'He {he_date_str} map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    hist, edges = detect.get_hist(pre_processed_map,\n",
    "                                         bins_as_percent=False)\n",
    "    \n",
    "    ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(231)\n",
    "    ax.set_title(he_date_str)\n",
    "    ax.imshow(pre_processed_map, cmap=plt.cm.gray)\n",
    "    \n",
    "    ax = fig.add_subplot(232)\n",
    "    ax.set_title('Semilog Histogram')\n",
    "    ax.semilogy(edges[1:], hist)\n",
    "    if 'Rescale/' in DETECTION_VERSION_DIR:\n",
    "        ax.set_ylim([1E2, 5E4])\n",
    "    else:\n",
    "        ax.set_xlim([-110, 110])\n",
    "        ax.set_ylim([1E1, 5E4])\n",
    "    \n",
    "    ax = fig.add_subplot(233)\n",
    "    ax.imshow(ensemble_map, cmap=plt.cm.magma)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))    \n",
    "    plot_detection.plot_thresh_outcome_vs_time(\n",
    "        ax, outcome_df, he_date_str, cmap, ylabel)\n",
    "\n",
    "    plt.savefig(img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{he_date_str} map saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Outcomes on Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "\n",
    "# v0.5.1\n",
    "p_start, p_step, p_num = (70, 10, 5)\n",
    "r_start, r_step, r_num = (8, 4, 4)\n",
    "\n",
    "# # vY\n",
    "# p_start, p_step, p_num = (60, 7.5, 7)\n",
    "# r_start, r_step, r_num = (8, 2, 6)\n",
    "\n",
    "PERCENTS_OF_PEAK = np.arange(p_start, p_start + p_num*p_step, p_step)\n",
    "MORPH_RADII = np.arange(r_start, r_start + r_num*r_step, r_step)\n",
    "\n",
    "# thresh_step = 5\n",
    "# radius_step = 2\n",
    "# PERCENTS_OF_PEAK = np.arange(50,86,thresh_step)\n",
    "# # PERCENTS_OF_PEAK = np.array(range(65,101,thresh_step))\n",
    "# # PERCENTS_OF_PEAK = np.array(range(45,101,thresh_step))\n",
    "# MORPH_RADII = np.arange(14,25,radius_step)\n",
    "\n",
    "print(PERCENTS_OF_PEAK)\n",
    "print(MORPH_RADII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_area_for_date_str_1D(percent_of_peak, he_date_str):\n",
    "#     \"\"\"Retrieve detected area percentages for the specified date.\n",
    "#     \"\"\"\n",
    "#     he_map = prepare_data.get_nso_sunpy_map(ALL_HE_DIR + he_date_str + '.fts')\n",
    "#     he = detect.pre_process_he_v0_4(he_map.data)\n",
    "#     ch_mask_data = detect.get_ch_mask(\n",
    "#         he, percent_of_peak, MORPH_RADIUS\n",
    "#     )\n",
    "#     ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "#     return detect.get_area(ch_mask_map, 0)[0]\n",
    "\n",
    "\n",
    "def get_area_for_date_str(percent_of_peak, morph_radius, he_date_str):\n",
    "    \"\"\"Retrieve detected area percentages for specified dates.\n",
    "    \"\"\"\n",
    "    he_map = prepare_data.get_nso_sunpy_map(ALL_HE_DIR + he_date_str + '.fts')\n",
    "    he = detect.pre_process_v0_4(he_map.data)\n",
    "    ch_mask_data = detect.get_ch_mask(\n",
    "        he, percent_of_peak, morph_radius\n",
    "    )\n",
    "    ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "    return detect.get_open_area(ch_mask_map, 0)\n",
    "    \n",
    "\n",
    "def get_outcomes(percent_of_peak, morph_radius):\n",
    "    \"\"\"Retrieve segmentation map outcomes with specified design\n",
    "    variables over time.\n",
    "    \n",
    "    Args\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "    Returns\n",
    "        Array of median area percentage.\n",
    "        Autocorrelation and normalized MAD of the detected area time series.\n",
    "    \"\"\"\n",
    "    print((percent_of_peak, morph_radius), end='  ')\n",
    "    area_tuple_list = [\n",
    "        get_area_for_date_str(percent_of_peak, morph_radius, he_date_str)\n",
    "        for he_date_str in HE_DATE_LIST\n",
    "    ]\n",
    "    area_percent_list = [\n",
    "        area_tuple[0] for area_tuple in area_tuple_list\n",
    "    ]\n",
    "    area_list = [\n",
    "        area_tuple[1] for area_tuple in area_tuple_list\n",
    "    ]\n",
    "    # percent_of_peak = design_vars\n",
    "    # print((design_vars, MORPH_RADIUS))\n",
    "    # area_percent_list = [\n",
    "    #     get_area_for_date_str_1D(percent_of_peak, he_date_str)\n",
    "    #     for he_date_str in HE_DATE_LIST\n",
    "    # ]\n",
    "    # area_percent_list = OPTIMIZER.get_area_list(design_vars)\n",
    "    \n",
    "    area_percent_median = np.median(area_percent_list)\n",
    "    autocorr = pd.Series(area_list).autocorr()\n",
    "    \n",
    "    # Compute normalized MAD of detected area\n",
    "    mad = np.median(np.abs(area_list - np.median(area_list)))\n",
    "    if np.median(area_list) == 0:\n",
    "        norm_mad = 0\n",
    "    else:\n",
    "        norm_mad = mad/np.median(area_list)*100\n",
    "    \n",
    "    return np.array([area_percent_median, autocorr, norm_mad])\n",
    "\n",
    "get_vect_outcomes = np.vectorize(get_outcomes, signature='(),()->(3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Expensive computation: 31 min for 1 month, 48 nodes)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Optionally overwrite existing files\n",
    "if os.path.isfile(save_file) and not overwrite:\n",
    "    sys.exit(f'{file_date_str} data already exists.')\n",
    "\n",
    "num_nodes = len(PERCENTS_OF_PEAK)*len(PERCENTS_OF_PEAK)\n",
    "print(f'Evaluating outcomes for {num_nodes} nodes')\n",
    "\n",
    "# ~1min/step\n",
    "X, Y = np.meshgrid(PERCENTS_OF_PEAK, MORPH_RADII)\n",
    "outcome_array = get_vect_outcomes(X, Y)\n",
    "\n",
    "np.save(save_file, outcome_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcome maps\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps_L/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "area_percent_median_array_L = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps_R/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "area_percent_median_array_R = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "\n",
    "area_percent_median_array = np.hstack([\n",
    "    area_percent_median_array_L[:,:4], area_percent_median_array_R\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_outcomes_for_date_str(he_date_str, percent_of_peak, morph_radius_dist):\n",
    "    \"\"\"Retrieve detected area for specified dates.\n",
    "    \"\"\"\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    pre_processed_map = sunpy.map.Map(pre_process_file)\n",
    "\n",
    "    # Obtain segmentation mask, sunpy map, and boundaries --------------------\n",
    "    ch_mask_data = detect.get_ch_mask_list_vY(\n",
    "        pre_processed_map, [percent_of_peak], [morph_radius_dist]\n",
    "    )[0]\n",
    "    ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), pre_processed_map.meta)\n",
    "    \n",
    "    contours = measure.find_contours(ch_mask_data)\n",
    "    ch_boundary_data = np.zeros(ch_mask_data.shape)\n",
    "\n",
    "    for contour in contours:\n",
    "        contour = contour.astype(int)\n",
    "        ch_boundary_data[contour[:,0], contour[:,1]] = 1\n",
    "    \n",
    "    # Compute outcomes -------------------------------------------------------\n",
    "    area_percent, open_area = detect.get_open_area(ch_mask_map, 0)\n",
    "    fractal_D = get_fractal_D(ch_boundary_data, scale_range=[10, 500])\n",
    "    \n",
    "    return area_percent, open_area, fractal_D\n",
    "\n",
    "\n",
    "def get_outcomes_v0_5_1(percent_of_peak, morph_radius_dist):\n",
    "    \"\"\"Retrieve segmentation map outcomes with specified design\n",
    "    variables over time.\n",
    "    \n",
    "    Args\n",
    "        percent_of_peak: float percentage measured from the zero\n",
    "            value up to or beyond the histogram value\n",
    "        morph_radius_dist: float distances in Mm for radius of\n",
    "            disk structuring element in morphological operations\n",
    "    Returns\n",
    "        Array of median area percentage and autocorrelation of the detected\n",
    "            area time series.\n",
    "    \"\"\"\n",
    "    print((percent_of_peak, morph_radius_dist), end='  ')\n",
    "    outcomes_by_dates = [\n",
    "        get_contour_outcomes_for_date_str(\n",
    "            he_date_str, percent_of_peak, morph_radius_dist\n",
    "        )\n",
    "        for he_date_str in HE_DATE_LIST\n",
    "    ]\n",
    "    area_percent_list = [\n",
    "        date_outcomes[0] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    area_list = [\n",
    "        date_outcomes[1] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    fractal_D_list = [\n",
    "        date_outcomes[2] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    \n",
    "    area_percent_median = np.median(area_percent_list)\n",
    "    autocorr = pd.Series(area_list).autocorr()\n",
    "    fractal_D_median = np.nanmedian(fractal_D_list)\n",
    "    \n",
    "    return np.array([area_percent_median, autocorr, fractal_D_median])\n",
    "\n",
    "get_vect_outcomes_v0_5_1 = np.vectorize(get_outcomes_v0_5_1, signature='(),()->(3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Optionally overwrite existing files\n",
    "if os.path.isfile(save_file) and not overwrite:\n",
    "    sys.exit(f'{file_date_str} data already exists.')\n",
    "\n",
    "num_nodes = len(PERCENTS_OF_PEAK)*len(PERCENTS_OF_PEAK)\n",
    "print(f'Evaluating outcomes for {num_nodes} nodes and {num_maps} dates')\n",
    "\n",
    "# ~1min/step\n",
    "X, Y = np.meshgrid(PERCENTS_OF_PEAK, MORPH_RADII)\n",
    "outcome_array = get_vect_outcomes_v0_5_1(X, Y)\n",
    "\n",
    "np.save(save_file, outcome_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "upper_level = np.ceil(np.max(area_percent_median_array))\n",
    "# levels = [0, 0.1, 0.5]\n",
    "# levels.extend(list(np.linspace()))\n",
    "step = 2\n",
    "levels = np.arange(0,upper_level + step,step)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII,\n",
    "             area_percent_median_array, levels, cmap='plasma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Median Detected Area Percentage (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Area_fill_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.5.1 Design\n",
    "percent_of_peak_design = [70, 70, 80, 90]\n",
    "morph_radius_design = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # v0.5.1 KPVT Design\n",
    "# percent_of_peak_design = [80, 80, 90, 100]\n",
    "# morph_radius_design = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # vY Design 1\n",
    "# percent_of_peak_design = [62, 68, 73, 80]\n",
    "# morph_radius_design = [   11, 13,  8, 10]\n",
    "\n",
    "# # vY Design 2\n",
    "# percent_of_peak_design = [85, 73, 95, 85]\n",
    "# morph_radius_design = [   10, 14, 10, 14]\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "upper_level = np.ceil(np.max(area_percent_median_array))\n",
    "step = 1\n",
    "levels_1 = np.arange(0,5 + step,step)\n",
    "step = 2\n",
    "levels_2 = np.arange(6,upper_level + step,step)\n",
    "levels = np.hstack((levels_1, levels_2))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "cp = plt.contour(PERCENTS_OF_PEAK, MORPH_RADII,\n",
    "                 area_percent_median_array, levels, cmap='plasma')\n",
    "plt.clabel(cp, fontsize=14)\n",
    "\n",
    "plt.title('Median Detected Area Percentage (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "# plt.savefig(f'{output_dir}Area_{file_date_str}.jpg')\n",
    "\n",
    "plt.scatter(percent_of_peak_design, morph_radius_design, color='k')\n",
    "plt.savefig(f'{output_dir}Area_Point_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "d_area_d_thresh = np.diff(area_percent_median_array, axis=1)/p_step\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK[:-1], MORPH_RADII,\n",
    "             d_area_d_thresh, cmap='bone')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('$\\partial(Median\\ Area)/\\partial(Threshold)$ (%/%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Partial_Thresh_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "d_area_d_radius = np.diff(area_percent_median_array, axis=0)/r_step\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII[:-1],\n",
    "             d_area_d_radius, cmap='bone')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('$\\partial(Median\\ Area)/\\partial(Radius)$ (%/px)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Partial_Radius_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_area_d_radius[np.where(MORPH_RADII == 10), np.where(PERCENTS_OF_PEAK == 115)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not available in updated versions\n",
    "norm_mad_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "upper_level = np.ceil(np.max(norm_mad_array))\n",
    "levels = [0, 1, 10]\n",
    "levels.extend(list(np.linspace(15,upper_level,7)))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, norm_mad_array, levels)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Normalized MAD of Detected Area (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')\n",
    "plt.savefig(f'{output_dir}Norm_MAD_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_array = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, -autocorr_array)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Negative Autocorrelation')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Autocorr_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal_D_bound_median_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "# fractal_D_bound_median_array = np.load(save_file, allow_pickle=True)[:,:,3]\n",
    "lower_level = np.floor(np.nanmin(fractal_D_bound_median_array)*10)/10\n",
    "upper_level = np.ceil(np.nanmax(fractal_D_bound_median_array)*10)/10\n",
    "step = 0.025\n",
    "levels = np.arange(lower_level, upper_level + step, step)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, fractal_D_bound_median_array, levels)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Fractal Dimension')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Fractal_Dim_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), fractal_D_bound_median_array\n",
    ")\n",
    "f((8,110))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs verification\n",
    "interp_points = 500\n",
    "num_bins = 17\n",
    "\n",
    "area_interp = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), area_percent_median_array\n",
    ")\n",
    "fractal_D_interp = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), fractal_D_bound_median_array\n",
    ")\n",
    "\n",
    "interp_percents_of_peak = np.linspace(np.min(PERCENTS_OF_PEAK), np.max(PERCENTS_OF_PEAK), interp_points)\n",
    "interp_morph_radii = np.linspace(np.min(MORPH_RADII), np.max(MORPH_RADII), interp_points)\n",
    "XX, YY = np.meshgrid(interp_percents_of_peak, interp_morph_radii)\n",
    "\n",
    "interpolated_area = area_interp((YY, XX))\n",
    "interpolated_fractal_D = fractal_D_interp((YY, XX))\n",
    "\n",
    "bins = np.linspace(np.min(area_percent_median_array), np.max(area_percent_median_array), num_bins)\n",
    "binned_idx_array = np.digitize(interpolated_area, bins) - 1\n",
    "\n",
    "X = np.zeros_like(binned_idx_array, dtype=float)\n",
    "\n",
    "for bin_num in range(num_bins):\n",
    "    bin_idxs = np.where(binned_idx_array == bin_num)\n",
    "    bin_fractal_D = interpolated_fractal_D[bin_idxs]\n",
    "    \n",
    "    if not np.any(bin_fractal_D):\n",
    "        continue\n",
    "\n",
    "    X[bin_idxs] = bin_fractal_D - np.mean(bin_fractal_D)\n",
    "    \n",
    "# X = X\n",
    "\n",
    "bound = max([np.max(X), np.abs(np.min(X))])\n",
    "levels = np.linspace(-bound, bound, 15)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(interp_percents_of_peak, interp_morph_radii, X,\n",
    "             levels, cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Binned Difference Fractal Boundary Dimension')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func_v0_5_1(norm_mad_array, area_percent_median_array, mu, M):\n",
    "    penalty = mu*(area_percent_median_array - M)**2 + 1/area_percent_median_array\n",
    "    obj_func_array = norm_mad_array + penalty\n",
    "    return obj_func_array\n",
    "\n",
    "def get_optim_vars(mu, M):\n",
    "    obj_func_array = obj_func(NORM_MAD_ARRAY, AREA_PERCENT_MEDIAN_ARRAY, mu, M)\n",
    "    xi, yi = np.unravel_index(np.argmin(obj_func_array), obj_func_array.shape)\n",
    "    return np.array([PERCENTS_OF_PEAK[yi], MORPH_RADII[xi]])\n",
    "\n",
    "get_vect_optim_vars = np.vectorize(get_optim_vars, signature='(),()->(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area penalty weight\n",
    "mu = 1\n",
    "\n",
    "# Target median area\n",
    "M = 3\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "norm_mad_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "obj_func_array = obj_func_v0_5_1(norm_mad_array, area_percent_median_array, mu, M)\n",
    "\n",
    "z = obj_func_array[~np.isinf(obj_func_array)]\n",
    "lev_exp = np.linspace(\n",
    "    np.log10(z.min()), np.log10(z.max()), 10\n",
    ")\n",
    "levels = np.power(10, lev_exp)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "cs = plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, obj_func_array, levels, \n",
    "                  norm=colors.LogNorm(), cmap='gray')\n",
    "plt.colorbar(cs, format=ticker.FuncFormatter(lambda x, pos: f'{x:.1f}'))\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title(f'Objective Function | $\\mu$: {mu} $M$: {M}%')\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')\n",
    "plt.savefig(f'{output_dir}Obj_Func_Mu{mu}_M{M}_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func_array[np.where(MORPH_RADII == 20), np.where(PERCENTS_OF_PEAK == 115)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_step = 5\n",
    "M_step = 0.1\n",
    "mu = np.arange(0, 500 + mu_step, mu_step)\n",
    "M = np.arange(2, 6 + M_step, M_step)\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(mu, M)\n",
    "AREA_PERCENT_MEDIAN_ARRAY = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "NORM_MAD_ARRAY = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "optim_var_array = get_vect_optim_vars(X, Y)\n",
    "optim_var_pairs = optim_var_array.reshape((np.prod(optim_var_array.shape[:2]), 2))\n",
    "unique_optim_var_pairs, optim_counts = np.unique(\n",
    "    optim_var_pairs,axis=0, return_counts=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(unique_optim_var_pairs[:,0], unique_optim_var_pairs[:,1], s=250,\n",
    "            c=optim_counts, cmap='turbo')\n",
    "plt.colorbar()\n",
    "plt.xticks(PERCENTS_OF_PEAK)\n",
    "plt.yticks(MORPH_RADII)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Optimal Design Variables with Varied $\\mu,\\ M$')\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(mu, M, optim_var_array[:,:,0], cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Optimal Threshold Relative to Mode (%)')\n",
    "plt.xlabel('Penalty Weight $mu$')\n",
    "plt.ylabel('Target Median Area Percentage $M$ (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func_v0_5_2(autocorr_array, area_percent_median_array, mu, M):\n",
    "    C = mu*(area_percent_median_array - M)**2\n",
    "    obj_func_array = -autocorr_array + C\n",
    "    return obj_func_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target median area\n",
    "M = 3\n",
    "\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps/Obj_Func_v0_5_2/'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "autocorr_array = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "# Area penalty weight\n",
    "for mu in np.logspace(-5,0,20):\n",
    "    obj_func_array = obj_func_v0_5_2(autocorr_array, area_percent_median_array, mu, M)\n",
    "    if np.min(obj_func_array) < 0:\n",
    "        obj_func_array = obj_func_array + np.abs(np.min(obj_func_array)) + 0.1\n",
    "\n",
    "    z = obj_func_array[~np.isinf(obj_func_array)]\n",
    "    lev_exp = np.linspace(\n",
    "        np.log10(z.min()), np.log10(z.max()), 10\n",
    "    )\n",
    "    levels = np.power(10, lev_exp)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    \n",
    "    cs = plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, obj_func_array, levels, \n",
    "                    norm=colors.LogNorm(), cmap='gray')\n",
    "    plt.colorbar(cs, format=ticker.FuncFormatter(lambda x, pos: f'{x:.1f}'))\n",
    "\n",
    "    plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "    plt.title(f'Objective Function | $\\mu$: {mu:.2e} $M$: {M}%')\n",
    "    plt.xlabel('Threshold Relative to Mode (%)')\n",
    "    plt.ylabel('SE Disk Radius (px)')\n",
    "    plt.savefig(f'{output_dir}Mu{mu:.6f}_M{M}_{file_date_str}'.replace('.','_') + '.jpg')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f'Mu: {mu:.2e} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect.write_ensemble_video(output_dir, fps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v0.1 Heat Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CH Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = f'{DETECTION_IMAGE_DIR}Outcome_Maps/{he_date_str}_maps.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CH Mask List (Expensive computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower threshold accepts more and lets morphology carry the load in selection and removal\n",
    "thresh_step = 5\n",
    "radius_step = 1\n",
    "percent_of_peak_list = list(np.arange(70,106,thresh_step))\n",
    "morph_radius_list = list(np.arange(6,21,radius_step))\n",
    "\n",
    "# List of CHS masks for different files with varied parameters\n",
    "all_ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_processed_map_data, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "\n",
    "save_list = [he_date_str, percent_of_peak_list, morph_radius_list, all_ch_mask_list]\n",
    "np.save(save_file, np.array(save_list, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CH Mask List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list = np.load(save_file, allow_pickle=True)\n",
    "date_str = save_list[0]\n",
    "percent_of_peak_list = save_list[1]\n",
    "morph_radius_list = save_list[2]\n",
    "all_ch_mask_list = save_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [f'{percent_of_peak:d}% of Peak | {radius:d}px Radius'\n",
    "              for percent_of_peak in percent_of_peak_list\n",
    "              for radius in morph_radius_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_map(outcome_list, title, percent_of_peak_list, morph_radius_list, \n",
    "                  color_scale='Magma'):\n",
    "    # Reverse order to facilitate plotting\n",
    "    y_axis_list = morph_radius_list.copy()\n",
    "    y_axis_list.reverse()\n",
    "    \n",
    "    outcome_map = np.flipud(np.reshape(\n",
    "        outcome_list, (len(percent_of_peak_list),len(morph_radius_list))).T)\n",
    "\n",
    "    fig = px.imshow(outcome_map, \n",
    "                    labels=dict(x='Threshold Level as Percent of Peak (%)',\n",
    "                                y='SE Disk Radius (px)'),\n",
    "                    x=percent_of_peak_list, y=y_axis_list,\n",
    "                    aspect='auto', color_continuous_scale=color_scale)\n",
    "    fig.update_layout(title=title, width=700)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def plot_heat_map_band(outcome_list, heat_map_title, lower_bound, upper_bound,\n",
    "                       percent_of_peak_list, morph_radius_list, \n",
    "                       array, all_ch_masks_list, title_list, color_scale='Magma'):\n",
    "    edit_outcome_list = outcome_list.copy()\n",
    "    \n",
    "    # Index list of outcomes within bounds \n",
    "    idx_list = [i for i in range(len(edit_outcome_list)) \n",
    "              if edit_outcome_list[i] >= lower_bound and edit_outcome_list[i] <= upper_bound]\n",
    "    num_ch_masks = len(idx_list)\n",
    "    \n",
    "    if not num_ch_masks:\n",
    "        print('No masks in outcome range')\n",
    "        return\n",
    "    \n",
    "    max_outcome = max(edit_outcome_list)\n",
    "    # Highlight outcomes within bounds\n",
    "    for i in idx_list:\n",
    "        edit_outcome_list[i] = 2*max_outcome\n",
    "\n",
    "    plot_heat_map(edit_outcome_list, heat_map_title,\n",
    "                  percent_of_peak_list, morph_radius_list, color_scale)\n",
    "    \n",
    "    if num_ch_masks == 1:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot()\n",
    "        \n",
    "        outcome_idx = idx_list[0]\n",
    "        ax.imshow(array, cmap=plt.cm.afmhot)\n",
    "        ax.contour(all_ch_masks_list[outcome_idx], linewidths=0.5, cmap=plt.cm.gray)\n",
    "        ax.set_title(title_list[outcome_idx], fontsize=18)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=num_ch_masks, figsize=(6*num_ch_masks, 6))\n",
    "        ax = axes.ravel()\n",
    "    \n",
    "        for i in range(num_ch_masks):\n",
    "            outcome_idx = idx_list[i]\n",
    "            ax[i].imshow(array, cmap=plt.cm.afmhot)\n",
    "            ax[i].contour(all_ch_masks_list[outcome_idx], linewidths=0.5, cmap=plt.cm.gray)\n",
    "            ax[i].set_title(title_list[outcome_idx], fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_list = detect.get_px_percent_list(all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Segmented Area Percentage'\n",
    "\n",
    "plot_heat_map(\n",
    "    area_percent_list, heat_map_title, percent_of_peak_list, morph_radius_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(area_percent_list[:-1], bins=30)\n",
    "ax.set_title(f'{date_str} Segmented Area Bins', fontsize=20)\n",
    "ax.set_xlabel('Area Percentage', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Area Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 16\n",
    "upper_bound = 18\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 6.8\n",
    "upper_bound = 7.3\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 5.4\n",
    "upper_bound = 6\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 3.2\n",
    "upper_bound = 3.3\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CH Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch_list = detect.get_num_CH_list(all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Segmented Hole Number'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map(\n",
    "    num_ch_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(num_ch_list, bins=30, range=(0,25))\n",
    "ax.set_title(f'{date_str} Segmented Hole Number Bins', fontsize=20)\n",
    "ax.set_xlabel('Segemented Hole Number', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Hole Number Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 17\n",
    "upper_bound = 17\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 11\n",
    "upper_bound = 11\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 9\n",
    "upper_bound = 9\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Tail Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ch_lower_tail_width_list(array, ch_mask_list): \n",
    "    \"\"\"Retrieve the average of the histogram lower tail width across CHs\n",
    "    for each segmentation in a list.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask_list: binary coronal holes mask list\n",
    "    Returns\n",
    "        List of mean histogram tail widths of CHs detected in segmentations.\n",
    "    \"\"\"\n",
    "    labeled_ch_list = [ndimage.label(ch_mask)[0]\n",
    "                          for ch_mask in ch_mask_list]\n",
    "    num_ch_list = [get_num_ch(ch_mask) for ch_mask in ch_mask_list]\n",
    "    \n",
    "    # List of average lower tail widths across all CH's of each segmentaion\n",
    "    lower_tail_width_list = []\n",
    "    \n",
    "    count = 0\n",
    "    for labeled_ch_mask, num_ch in zip(labeled_ch_list, num_ch_list):\n",
    "\n",
    "        map_data_by_ch = get_map_data_by_ch(array, labeled_ch_mask, num_ch)\n",
    "    \n",
    "        ch_mask_lower_tail_width_list = get_ch_lower_tail_widths(map_data_by_ch)\n",
    "        \n",
    "        mean_lower_tail_width = np.mean(ch_mask_lower_tail_width_list)\n",
    "        \n",
    "        lower_tail_width_list.append(mean_lower_tail_width)\n",
    "        count = count + 1\n",
    "        \n",
    "    return lower_tail_width_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tail_width_list = detect.get_ch_lower_tail_width_list(he, all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map(\n",
    "    lower_tail_width_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(lower_tail_width_list, bins=30, range=(10,40))\n",
    "ax.set_title(f'{date_str} Mean CH Tail Width Bins', fontsize=20)\n",
    "ax.set_xlabel('Mean CH Tail Width', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Tail Width Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 29\n",
    "upper_bound = 29.5\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    lower_tail_width_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 24.5\n",
    "upper_bound = 25\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    lower_tail_width_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Multiplied Metrics'\n",
    "color_scale = 'dense_r'\n",
    "\n",
    "outcome_list = list(np.array(area_percent_list)*np.array(num_ch_list)*np.array(lower_tail_width_list))\n",
    "\n",
    "plot_heat_map(\n",
    "    outcome_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes_by_all_ch(cl_list):\n",
    "    \"\"\"Retrieve outcomes per CH in ensemble maps in all datetimes\n",
    "    at specified confidence levels from ensemble maps.\n",
    "    \n",
    "    See get_outcomes for retrieved outcomes.\n",
    "    \n",
    "    Args\n",
    "        cl_list: list of float confidence levels at which\n",
    "            to threshold ensemble maps for computing outcomes\n",
    "    Returns\n",
    "        Dataframes of outcomes by confidence level over time.\n",
    "    \"\"\"\n",
    "    # Dictionaries for outcomes of distinct CHs at varied confidence levels\n",
    "    area_dict = {cl:[] for cl in cl_list}\n",
    "    lat_dict = {cl:[] for cl in cl_list}\n",
    "    lon_dict = {cl:[] for cl in cl_list}\n",
    "    unsigned_flux_dict = {cl:[] for cl in cl_list}\n",
    "    signed_flux_dict = {cl:[] for cl in cl_list}\n",
    "    mag_skew_dict = {cl:[] for cl in cl_list}\n",
    "    unipolarity_dict = {cl:[] for cl in cl_list}\n",
    "\n",
    "    for he_date_str in HE_DATE_LIST:\n",
    "        \n",
    "        # Extract He I observation\n",
    "        he_file = f'{ALL_HE_DIR}{he_date_str}.fts'\n",
    "        he_map = prepare_data.get_nso_sunpy_map(he_file)\n",
    "        if not he_map:\n",
    "            print(f'{he_date_str} He I observation extraction failed.')\n",
    "            continue\n",
    "        \n",
    "        # Extract saved ensemble map\n",
    "        ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "        ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "        ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "        \n",
    "        # Extract saved processed magnetograms\n",
    "        mag_date_str = prepare_data.get_nearest_date_str(\n",
    "            MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "        )\n",
    "        reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                                 + f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "        reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "        \n",
    "        # Outcomes per CH detected at the given or greater\n",
    "        # confidence levels\n",
    "        outcome_by_ch_dict_by_cl = [\n",
    "            detect.get_outcomes_by_ch(ensemble_map, pre_processed_map,\n",
    "                                      reprojected_mag_map, cl)\n",
    "            for cl in cl_list\n",
    "        ]\n",
    "        area_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['area'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        lat_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['cm_lat'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        lon_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['cm_lon'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        unsigned_flux_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['unsigned_flux'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        signed_flux_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['signed_flux'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        mag_skew_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['mag_skew'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        unipolarity_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['unipolarity'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        \n",
    "        # Extend outcomes per CH\n",
    "        for cl in cl_list:\n",
    "            area_dict[cl].extend(\n",
    "                area_single_date_dict[cl]\n",
    "            )\n",
    "            lat_dict[cl].extend(\n",
    "                lat_single_date_dict[cl]\n",
    "            )\n",
    "            lon_dict[cl].extend(\n",
    "                lon_single_date_dict[cl]\n",
    "            )\n",
    "            unsigned_flux_dict[cl].extend(\n",
    "                unsigned_flux_single_date_dict[cl]\n",
    "            )\n",
    "            signed_flux_dict[cl].extend(\n",
    "                signed_flux_single_date_dict[cl]\n",
    "            )\n",
    "            mag_skew_dict[cl].extend(\n",
    "                mag_skew_single_date_dict[cl]\n",
    "            )\n",
    "            unipolarity_dict[cl].extend(\n",
    "                unipolarity_single_date_dict[cl]\n",
    "            )\n",
    "            \n",
    "    return area_dict, lat_dict, lon_dict, \\\n",
    "        unsigned_flux_dict, signed_flux_dict, \\\n",
    "        mag_skew_dict, unipolarity_dict\n",
    "\n",
    "# def get_outcomes_by_all_date_range_ch(cl_list):\n",
    "#     \"\"\"Retrieve outcomes per CH in ensemble maps in all datetimes\n",
    "#     at specified confidence levels from ensemble maps.\n",
    "    \n",
    "#     See get_outcomes for retrieved outcomes.\n",
    "    \n",
    "#     Args\n",
    "#         cl_list: list of float confidence levels at which\n",
    "#             to threshold ensemble maps for computing outcomes\n",
    "#     Returns\n",
    "#         Dataframes of outcomes by confidence level over time.\n",
    "#     \"\"\"\n",
    "#     # Dictionaries for outcomes of distinct CHs at varied confidence levels\n",
    "#     area_dict = {cl:[] for cl in cl_list}\n",
    "#     lat_dict = {cl:[] for cl in cl_list}\n",
    "#     lon_dict = {cl:[] for cl in cl_list}\n",
    "#     unsigned_flux_dict = {cl:[] for cl in cl_list}\n",
    "#     signed_flux_dict = {cl:[] for cl in cl_list}\n",
    "#     mag_skew_dict = {cl:[] for cl in cl_list}\n",
    "#     unipolarity_dict = {cl:[] for cl in cl_list}\n",
    "\n",
    "#     for he_date_str in HE_DATE_LIST:\n",
    "        \n",
    "#         # Extract He I observation\n",
    "#         he_file = f'{ALL_HE_DIR}{he_date_str}.fts'\n",
    "#         he_map = prepare_data.get_nso_sunpy_map(he_file)\n",
    "#         if not he_map:\n",
    "#             print(f'{he_date_str} He I observation extraction failed.')\n",
    "#             continue\n",
    "        \n",
    "#         # Extract saved ensemble map\n",
    "#         ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "#         ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "#         ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "        \n",
    "#         # Extract saved processed magnetograms\n",
    "#         mag_date_str = prepare_data.get_nearest_date_str(\n",
    "#             MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "#         )\n",
    "#         reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "#                                  + f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "#         reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "        \n",
    "#         # Outcomes per CH detected at the given or greater\n",
    "#         # confidence levels\n",
    "#         outcome_tuple_by_cl = [\n",
    "#             detect.get_outcomes_by_ch(ensemble_map, reprojected_mag_map, cl)[:-1]\n",
    "#             for cl in cl_list\n",
    "#         ]\n",
    "#         area_single_date_dict = {\n",
    "#             cl:outcome_tuple[0] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         lat_single_date_dict = {\n",
    "#             cl:outcome_tuple[1] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         lon_single_date_dict = {\n",
    "#             cl:outcome_tuple[2] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         unsigned_flux_single_date_dict = {\n",
    "#             cl:outcome_tuple[3] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         signed_flux_single_date_dict = {\n",
    "#             cl:outcome_tuple[4] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         mag_skew_single_date_dict = {\n",
    "#             cl:outcome_tuple[5] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "#         unipolarity_single_date_dict = {\n",
    "#             cl:outcome_tuple[6] for cl, outcome_tuple\n",
    "#             in zip(cl_list, outcome_tuple_by_cl)\n",
    "#         }\n",
    "        \n",
    "#         # Extend outcomes per CH\n",
    "#         for cl in cl_list:\n",
    "#             area_dict[cl].extend(\n",
    "#                 area_single_date_dict[cl]\n",
    "#             )\n",
    "#             lat_dict[cl].extend(\n",
    "#                 lat_single_date_dict[cl]\n",
    "#             )\n",
    "#             lon_dict[cl].extend(\n",
    "#                 lon_single_date_dict[cl]\n",
    "#             )\n",
    "#             unsigned_flux_dict[cl].extend(\n",
    "#                 unsigned_flux_single_date_dict[cl]\n",
    "#             )\n",
    "#             signed_flux_dict[cl].extend(\n",
    "#                 signed_flux_single_date_dict[cl]\n",
    "#             )\n",
    "#             mag_skew_dict[cl].extend(\n",
    "#                 mag_skew_single_date_dict[cl]\n",
    "#             )\n",
    "#             unipolarity_dict[cl].extend(\n",
    "#                 unipolarity_single_date_dict[cl]\n",
    "#             )\n",
    "            \n",
    "#     return area_dict, lat_dict, lon_dict, \\\n",
    "#         unsigned_flux_dict, signed_flux_dict, \\\n",
    "#         mag_skew_dict, unipolarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes_by_all_ch_v0_5_1(cl_list):\n",
    "    \"\"\"Retrieve outcomes per CH in ensemble maps in all datetimes\n",
    "    at specified confidence levels from ensemble maps.\n",
    "    \n",
    "    See get_outcomes for retrieved outcomes.\n",
    "    \n",
    "    Args\n",
    "        cl_list: list of float confidence levels at which\n",
    "            to threshold ensemble maps for computing outcomes\n",
    "    Returns\n",
    "        Dataframes of outcomes by confidence level over time.\n",
    "    \"\"\"\n",
    "    # Dictionaries for outcomes of distinct CHs at varied confidence levels\n",
    "    outcomes_by_all_ch_dict = {}\n",
    "    for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "        outcomes_by_all_ch_dict[outcome_key] = {cl:[] for cl in cl_list}\n",
    "\n",
    "\n",
    "    for he_date_str in HE_DATE_LIST:\n",
    "        \n",
    "        # Extract saved ensemble map array and convert to Sunpy map\n",
    "        ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "        ensemble_map = sunpy.map.Map(ensemble_file)\n",
    "        \n",
    "        # Extract saved He I observation\n",
    "        he_fits_file = prepare_data.get_fits_path(\n",
    "            he_date_str, DATE_RANGE, ALL_HE_DIR, SELECT_HE_DIR\n",
    "        )\n",
    "        he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "        he_map_data = np.flipud(he_map.data)\n",
    "        \n",
    "        # Extract saved processed magnetogram\n",
    "        mag_date_str = prepare_data.get_nearest_date_str(\n",
    "            MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "        )\n",
    "        reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                                 + f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "        reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "        \n",
    "        # Extract single date outcomes by CH ---------------------------------\n",
    "        # Applied at varied confidence levels, then extending a main\n",
    "        # dictionary with outcomes by CH from all dates\n",
    "        \n",
    "        # List of varied confidence levels for outcomes per CH detected\n",
    "        # at the given or greater confidence levels\n",
    "        outcome_by_ch_dict_by_cl = [\n",
    "            detect.get_outcomes_by_ch(ensemble_map, he_map_data,\n",
    "                                      reprojected_mag_map, cl)\n",
    "            for cl in cl_list\n",
    "        ]\n",
    "        \n",
    "        # Dictionary of outcomes holding dictionaries of\n",
    "        # confidence levels for outcomes per CH\n",
    "        single_date_outcome_dict = {}\n",
    "        for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "            single_date_outcome_dict[outcome_key] = {\n",
    "                cl:outcome_by_ch_dict[outcome_key] for cl, outcome_by_ch_dict\n",
    "                in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "            }\n",
    "        \n",
    "        # Extend main outcomes per CH dictionary by confidence level\n",
    "        for cl in cl_list:\n",
    "            for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "                outcomes_by_all_ch_dict[outcome_key][cl].extend(\n",
    "                    single_date_outcome_dict[outcome_key][cl]\n",
    "                )\n",
    "            \n",
    "    return outcomes_by_all_ch_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_list = [0, 35, 65, 95]\n",
    "outcome_dicts = get_outcomes_by_all_ch(cl_list)\n",
    "area_dict, lat_dict, lon_dict = outcome_dicts[:3]\n",
    "unsigned_flux_dict, signed_flux_dict = outcome_dicts[3:5]\n",
    "mag_skew_dict, unipolarity_dict = outcome_dicts[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl_list = [50, 75, 90]\n",
    "cl_list = [0]\n",
    "unipolarity_confidence = True\n",
    "outcomes_by_all_ch_dict = get_outcomes_by_all_ch_v0_5_1(cl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = outcomes_by_all_ch_dict['cm_lon']\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Longitude Histogram')\n",
    "plt.ylabel('Number of CH Detections')\n",
    "plt.xlabel('Longitude (deg)')\n",
    "orientation = 'vertical'\n",
    "plt.xlim([-90,90])\n",
    "plt.ylim([0,50])\n",
    "\n",
    "# plt.ylim([0,120])\n",
    "\n",
    "# outcome_dict = outcomes_by_all_ch_dict['cm_lat']\n",
    "# plt.figure(figsize=(7,6))\n",
    "# plt.title('Latitude Histogram')\n",
    "# plt.ylabel('Latitude (deg)')\n",
    "# plt.xlabel('Number of CH Detections')\n",
    "# orientation = 'horizontal'\n",
    "# plt.ylim([-90,90])\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "\n",
    "bins = np.arange(-90,90.1,10)\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'>= {cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color,\n",
    "        orientation=orientation, label=label\n",
    "    )\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = outcomes_by_all_ch_dict['cm_lon']\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Longitude Histogram')\n",
    "plt.ylabel('Number of CH Detections')\n",
    "plt.xlabel('Sine Longitude')\n",
    "orientation = 'vertical'\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([0,40]) \n",
    "loc = 'upper left'\n",
    "\n",
    "# outcome_dict = outcomes_by_all_ch_dict['cm_lat']\n",
    "# plt.figure(figsize=(7,6))\n",
    "# plt.title('Latitude Histogram')\n",
    "# plt.ylabel('Sine Latitude')\n",
    "# plt.xlabel('Number of CH Detections')\n",
    "# orientation = 'horizontal'\n",
    "# plt.ylim([-1,1])\n",
    "# loc = 'lower right'\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "\n",
    "\n",
    "bins = np.arange(-1,1.01,0.125)\n",
    "# for cl, color in zip(cl_list, color_list):\n",
    "for cl, color, linestyle in zip(cl_list, color_list, ['-', '--', '-.']):\n",
    "    sine_angle = np.sin(np.deg2rad(outcome_dict[cl]))\n",
    "    \n",
    "    if unipolarity_confidence:\n",
    "        label = fr'$\\geq$ {cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    # plt.hist(\n",
    "    #     sine_angle, bins, color,\n",
    "    #     orientation=orientation, label=label\n",
    "    # )\n",
    "    plt.hist(\n",
    "        sine_angle, bins, histtype='step',\n",
    "        color='white', edgecolor=color,\n",
    "        linestyle=linestyle, linewidth=3,\n",
    "        orientation=orientation, label=label\n",
    "    )\n",
    "    \n",
    "plt.legend(loc=loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsigned Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = outcomes_by_all_ch_dict['unsigned_flux']\n",
    "\n",
    "bin_min = np.log10(np.min(outcome_dict[0]))\n",
    "bin_max = np.ceil(np.log10(np.max(outcome_dict[0])))\n",
    "bins = 10**(np.linspace(bin_min,bin_max,25))\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xscale('log')\n",
    "plt.title('Unsigned Flux Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unsigned Open Flux (Wb)')\n",
    "\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'{cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color,\n",
    "        orientation=orientation, label=label\n",
    "    )\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unipolarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipolarity_threshold = 0.5\n",
    "smooth_percentile_bounds = [0, 50, 80]\n",
    "\n",
    "unipolarity_list = outcomes_by_all_ch_dict['unipolarity'][0]\n",
    "grad_median_list = outcomes_by_all_ch_dict['grad_median'][0]\n",
    "\n",
    "# Smoothness percentile by candidate\n",
    "# Mapped in [0,100) and reversed order from gradient median quanitfying roughness\n",
    "smooth_percentiles = 100 - stats.rankdata(grad_median_list)/len(grad_median_list)*100\n",
    "\n",
    "# Stratify candidate CH unipolarity by percentiles of smoothness\n",
    "u_by_smooth_pct_dict = {}\n",
    "    \n",
    "for smooth_pct_bound in smooth_percentile_bounds:\n",
    "    \n",
    "    candidate_u_above_smooth_pct_list = [\n",
    "        unipolarity for unipolarity, smooth_percentile\n",
    "        in zip(unipolarity_list, smooth_percentiles)\n",
    "        if smooth_percentile >= smooth_pct_bound\n",
    "    ]\n",
    "    u_by_smooth_pct_dict[smooth_pct_bound] = candidate_u_above_smooth_pct_list\n",
    "\n",
    "bins = np.arange(0,1.01,0.05)\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(smooth_percentile_bounds)))\n",
    "\n",
    "line_styles = ['-', '--', '-.']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Unipolarity Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unipolarity')\n",
    "plt.xticks([0,0.25,0.5,0.75,1])\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,140])\n",
    "\n",
    "for smooth_pct, color, linestyle in zip(\n",
    "    smooth_percentile_bounds, color_list, line_styles):\n",
    "    \n",
    "    plt.hist(\n",
    "        u_by_smooth_pct_dict[smooth_pct], bins, histtype='step',\n",
    "        color='white', edgecolor=color,\n",
    "        linestyle=linestyle, linewidth=3,\n",
    "        label=f'{smooth_pct}th % Smoothness'\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.vlines([unipolarity_threshold, unipolarity_threshold], ymin=-10, ymax=150,\n",
    "           linestyles=['--'], color='k', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_candidates = len(u_by_smooth_pct_dict[0])\n",
    "\n",
    "u_among_all_candidates = np.array(u_by_smooth_pct_dict[0])\n",
    "num_bipolar_candidates = np.count_nonzero(u_among_all_candidates < 0.5)\n",
    "num_unipolar_candidates = np.count_nonzero(u_among_all_candidates >= 0.5)\n",
    "\n",
    "f'Bipolar candidate fraction: {num_bipolar_candidates/num_candidates*100:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_among_smooth_candidates = np.array(u_by_smooth_pct_dict[50])\n",
    "\n",
    "num_bipolar_smooth_candidates = np.count_nonzero(u_among_smooth_candidates < 0.5)\n",
    "num_unipolar_smooth_candidates = np.count_nonzero(u_among_smooth_candidates >= 0.5)\n",
    "\n",
    "('Bipolar, unsmooth candidate fraction: '\n",
    " f'{num_bipolar_smooth_candidates/num_bipolar_candidates*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Unpolar, smooth candidate fraction: '\n",
    " f'{num_unipolar_smooth_candidates/num_unipolar_candidates*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u_among_all_candidates) - len(u_among_unsmooth_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = unipolarity_dict\n",
    "\n",
    "# bins = np.linspace(0,1,25)\n",
    "bins = np.arange(0,1.01,0.05)\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Unipolarity Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unipolarity')\n",
    "\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'{cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color, label=label\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4s/map * 30 maps/step = 150s/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 10\n",
    "\n",
    "he_map = prepare_data.get_nso_sunpy_map(ALL_HE_DIR + HE_DATE_LIST[0] + '.fts')\n",
    "he = detect.pre_process_v0_4(he_map.data)\n",
    "ch_mask_data = detect.get_ch_mask(\n",
    "    he, percent_of_peak, morph_radius\n",
    ")\n",
    "plt.imshow(ch_mask_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bcddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_area_list(self, design_vars):\n",
    "        \"\"\"Retrieve detected area percentages for specied dates.\n",
    "        \"\"\"\n",
    "        percent_of_peak, morph_radius = design_vars\n",
    "        area_percent_list = []\n",
    "\n",
    "        for he_date_str in HE_DATE_LIST:\n",
    "            he_map = prepare_data.get_nso_sunpy_map(\n",
    "                ALL_HE_DIR + he_date_str + '.fts'\n",
    "            )\n",
    "            he = detect.pre_process_v0_4(he_map.data)\n",
    "            ch_mask_data = detect.get_ch_mask(\n",
    "                he, percent_of_peak, morph_radius\n",
    "            )\n",
    "            ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "            area_percent = detect.get_open_area(ch_mask_map, 0)[0]\n",
    "            area_percent_list.append(area_percent)\n",
    "            \n",
    "        return area_percent_list\n",
    "\n",
    "# Initiate optimizer object with cached area percent list\n",
    "OPTIMIZER = Optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 10\n",
    "MORPH_RADIUS = 10\n",
    "MAX_AREA_PERCENT = 4\n",
    "MIN_AREA_PERCENT = 1\n",
    "design_vars = (percent_of_peak, morph_radius)\n",
    "\n",
    "\n",
    "def obj_func():\n",
    "    \"\"\"Objective function for persistence optimization.\n",
    "    Penalizes normalized MAD of detected area\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def constraints(design_vars):\n",
    "    \"\"\"Constraints for persistence optimization.\n",
    "    \"\"\"\n",
    "    area_percent_list = [6, 5, 4, 9]\n",
    "    # OPTIMIZER.get_area_list(design_vars)\n",
    "    mean_area_percent = np.mean(area_percent_list)\n",
    "    \n",
    "    return (mean_area_percent - MIN_AREA_PERCENT,\n",
    "            MAX_AREA_PERCENT - mean_area_percent)\n",
    "    # return np.array([mean_area_percent - MIN_AREA_PERCENT],\n",
    "    #                  MAX_AREA_PERCENT - mean_area_percent])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.get_area_list(design_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_area_list(design_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func(90, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints(design_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLSQP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.show_options(solver='minimize', method='SLSQP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.rosen([0.5, 0, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_vars = (percent_of_peak, morph_radius)\n",
    "ineq_cons = ({\n",
    "    'type': 'ineq',\n",
    "    'fun' : constraints,\n",
    "    # 'args': (design_vars,)\n",
    "})\n",
    "# Unconstrained\n",
    "res = optimize.minimize(\n",
    "    obj_func, percent_of_peak, method='SLSQP',\n",
    "    options={'disp': True, 'finite_diff_rel_step': [0.1]},\n",
    ")\n",
    "# Constrained\n",
    "# res = optimize.minimize(\n",
    "#     obj_func, design_vars, args=(optimizer,), method='SLSQP', constraints=ineq_cons,\n",
    "#     options={'ftol': 1e-9, 'disp': True}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 100\n",
    "design_vars = (percent_of_peak, morph_radius)\n",
    "res = optimize.minimize(\n",
    "    obj_func, percent_of_peak, method='BFGS',\n",
    "    options={'disp': True, 'xrtol': 0.01, 'eps': 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res\n",
    "res1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSO CH Estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Level 2 Products: **Stage/Level2/**\n",
    "\n",
    "10830i equivalent width: **svsm_e3100_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "10830i intensity: **svsm_i3000_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "6302l magnetogram: **svsm_m1100_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "6302l intensity: **svsm_i1000_S2_yyyymmdd_hhmm.fts.gz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_he_eqw_fits_path = NSO_INPUT_DIR + 'svsm_e3100_S2_20140626_1419.fts'\n",
    "\n",
    "raw_he_intensity_fits_path = NSO_INPUT_DIR + 'svsm_i3000_S2_20140626_1419.fts'\n",
    "\n",
    "raw_magnetogram_fits_path = NSO_INPUT_DIR + 'svsm_m1100_S2_20140626_1444.fts'\n",
    "\n",
    "raw_mag_intensity_fits_path = NSO_INPUT_DIR + 'svsm_i1000_S2_20140626_1444.fts'\n",
    "\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    raw_he_eqw_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_he_intensity_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_magnetogram_fits_path, header_list=['IMTYPE'],\n",
    "    print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_mag_intensity_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Level 3 Products: **Stage/Level3/**\n",
    "He I EqW Maps List: **hDataList.txt**\n",
    "- 'Text list of available 10830 He EqW low-res sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic He I EqW maps\n",
    "\n",
    "Magnetogram Maps List: **mDataList.txt**\n",
    "- 'Text list of available 6301.5 low-res sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic magnetograms\n",
    "\n",
    "CH Maps List: **oDataList.txt**\n",
    "- 'Text list of available 10830 solar wind source sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic CH maps\n",
    "\n",
    "Carrington Rotation CH Images\n",
    "- '10830 solar wind sine-latlon daily synoptic map plots'\n",
    "- High-Res: **chsh.jpg**\n",
    "- Med-Res: **chsm.jpg**\n",
    "- Low-Res: **chsl.jpg**\n",
    "- Produced by **plot_lev3_map**\n",
    "  - Args: 1 carrington rotation CH map, CH maps list\n",
    "\n",
    "Unincluded He I Observation Image?: **svsm_o10mr_S3_{yyyymmdd}_{hhmm}.jpg**\n",
    "- Produced by **mk_obsimg**\n",
    "  - Args: 1 sky frame disk CH map, CH maps list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Maps: **../single/{yyyy}/**\n",
    "\n",
    "Sine-LatLon He I EQW: **svsm_e31hr_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 He EqW high-res sine-latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 He I EQW file, L2 He I continuum intensity file\n",
    "\n",
    "LatLon He I EQW: **svsm_e31lr_L3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 He EqW low-res latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 He I EQW file, L2 He I continuum intensity file\n",
    "\n",
    "Sine-LatLon Magnetogram: **svsm_m11hr_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '6301.5 high-res sine-latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 magnetogram file, L2 6302l magnetogram intensity file\n",
    "\n",
    "LatLon Magnetogram: **svsm_m11lr_L3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '6301.5 low-res latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 magnetogram file, L2 6302l magnetogram intensity file\n",
    "\n",
    "Sine-LatLon CH Map: **svsm_o1083_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 solar wind source sine-latlon heliographic map'\n",
    "- Produced by **mk_holeimg**\n",
    "  - Args: processed He I EqW, He I EqW maps list, processed magnetogram, magnetogram maps list\n",
    "\n",
    "Sky Frame Disk CH Map: **svsmgo1083_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 solar wind source sky frame heliocentric map'\n",
    "- Produced by **mk_dchimg**\n",
    "  - Args: 1 disk CH map, disk CH maps list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31hr_B3_20140606_1746.fts'\n",
    "he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31hr_B3_20140626_1419.fts'\n",
    "# he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31lr_L3_20140626_1419.fts'\n",
    "\n",
    "# mag_fits_path = NSO_SINGLE_DIR + 'svsm_m11hr_B3_20140606_1605.fts'\n",
    "mag_fits_path = NSO_SINGLE_DIR + 'svsm_m11hr_B3_20140626_1444.fts'\n",
    "\n",
    "# E: Empty file\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140601_1836.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140603_1704.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140606_1755.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140612_1438.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140620_1711.fts'\n",
    "ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140626_1428.fts'\n",
    "\n",
    "# E sky_ch_fits_path = NSO_SINGLE_DIR + 'svsmgo1083_B3_20140606_1755.fts'\n",
    "sky_ch_fits_path = NSO_SINGLE_DIR + 'svsmgo1083_B3_20140626_1428.fts'\n",
    "\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    he_eqw_fits_path, header_list=['IMTYPE', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    mag_fits_path, header_list=['IMTYPE', 'COMMENT1', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    ch_fits_map, header_list=['IMGTYP01', 'IMGTYP02', 'IMGTYP03'],\n",
    "    print_header=True\n",
    ")\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    sky_ch_fits_path, header_list=['COMMENT2'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NSO Processed EQW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = '~/Desktop/out_solarstrm_data/' + 'svsm_e31lr_L3_20140626_1419.fts'\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    fits, header_list=['IMTYPE', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray, plt.cm.gray],\n",
    "    print_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2014_06_26__00_00'\n",
    "\n",
    "raw_nso_eqw = NSO_EQW_DICT[date_str]\n",
    "nso_eqw_nan = detect.pre_process_eqw_v0_1(raw_nso_eqw)[2]\n",
    "\n",
    "titles = ['EQW', 'EQW NaN']\n",
    "plot_detection.plot_hists(\n",
    "    [raw_nso_eqw, nso_eqw_nan], titles, semilogy=True\n",
    ")\n",
    "\n",
    "lower_bounds = [-1600,  0,  0]\n",
    "upper_bounds = [0,      250, 500]\n",
    "plot_detection.plot_thresholds(\n",
    "    nso_eqw_nan, bounds=[lower_bounds, upper_bounds], \n",
    "    bounds_as_percent=False, threshold_type='band'\n",
    ")\n",
    "plot_detection.plot_thresholds(\n",
    "    nso_eqw_nan, bounds=[75, 85, 100], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Maps: **../merged/carr-daily/**\n",
    "Carrington Rotation CH Map: **svsm_o31hr_B3_cr{RRRR}_{DDD}.fts.gz**\n",
    "- 'Solar wind source high-res sine-latlon daily synoptic map'\n",
    "- Produced by **create_crmap**\n",
    "  - Args: 27 most recent disk CH maps, disk CH maps list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synoptic_ch_fits_path = NSO_MERGED_DIR + 'svsm_o31hr_B3_cr2152_275.fts'\n",
    "\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    synoptic_ch_fits_path,\n",
    "    # header_list=['DATE', 'CARR01', 'IMTYPE'],\n",
    "    header_list=['IMGTYP01', 'IMGTYP02', 'IMGTYP03', 'IMGTYP04'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm on Pre-Processed EQW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2014_06_26__00_00'\n",
    "\n",
    "raw_nso_eqw = NSO_EQW_DICT[date_str]\n",
    "nso_eqw_nan = detect.pre_process_eqw_v0_1(raw_nso_eqw)[2]\n",
    "\n",
    "percent_of_peak_list = [80,85,90]\n",
    "radius_list = [6]\n",
    "\n",
    "ensemble_map, holes_mask_list, confidence_list = detect.get_ensemble_v0_3(\n",
    "    nso_eqw_nan, percent_of_peak_list, radius_list)\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    nso_eqw_nan, ensemble_map, confidence_list, holes_mask_list\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSO Carrington Map Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work\n",
    "percent_of_peak_list = [80,90,100]\n",
    "radius_list = [11,13,15]\n",
    "\n",
    "for he_date_str in list(reversed(HE_DATE_LIST)):\n",
    "    raw_he = HE_FITS_DICT[date_str][0]\n",
    "    he = detect.pre_process_v0_1(raw_he)[0]\n",
    "    \n",
    "    ensemble_map = detect.get_ensemble(\n",
    "        he, percent_of_peak_list, radius_list\n",
    "    )[0]\n",
    "\n",
    "    euv = EUV_DICT[he_date_str]\n",
    "\n",
    "    plot_ensemble_comparison(he, he_date_str, ensemble_map, euv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ch_map(date_str_list, cr_str, ch_map_dict):\n",
    "    \"\"\"Plot NSO detected CH Carrington map.\n",
    "    \"\"\"\n",
    "    # Display selected column number corresponding to date list\n",
    "    selected_datetime_list = [\n",
    "        datetime.strptime(\n",
    "            date_str, DICT_DATE_STR_FORMAT)\n",
    "        for date_str in date_str_list\n",
    "    ]\n",
    "    selected_cr_list = [\n",
    "        carrington_rotation_number(selected_datetime)\n",
    "        for selected_datetime in selected_datetime_list\n",
    "    ]\n",
    "    \n",
    "    cr_str_list = cr_str.split('_')\n",
    "    cr_num_list = [float(cr_str) for cr_str in cr_str_list]\n",
    "    \n",
    "    cr_range = cr_num_list[-1] - cr_num_list[0]\n",
    "    cr_percent_list = [\n",
    "        (selected_cr - cr_num_list[0])/cr_range\n",
    "        for selected_cr in selected_cr_list\n",
    "    ]\n",
    "    \n",
    "    ch_map = ch_map_dict[cr_str]\n",
    "    rows, cols = ch_map.shape\n",
    "    \n",
    "    selected_col_list = [\n",
    "        cols - cr_percent*cols\n",
    "        for cr_percent in cr_percent_list\n",
    "    ]\n",
    "    \n",
    "    print('Selected Date Columns:')\n",
    "    for date_str, selected_col in zip(\n",
    "        date_str_list, selected_col_list):\n",
    "        print(f'{date_str}: {selected_col:.1f}px \\t', end='')\n",
    "\n",
    "    # Prepare the figure and axes with map projection\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_title(f'CR{cr_str}', fontsize=20)\n",
    "    \n",
    "    ax.imshow(ch_map, extent=[0,cols, rows, 0])\n",
    "    ax.vlines(x=selected_col_list, ymin=rows, ymax=0, linestyles='dashed',\n",
    "              colors='black')\n",
    "\n",
    "\n",
    "def rename_all_gong(gong_dir):\n",
    "    \"\"\"Rename all GONG magnetogram FITS files to include observation date in title\"\"\"\n",
    "    glob_pattern = gong_dir + '*.fits'\n",
    "    \n",
    "    fits_path_list = glob.glob(glob_pattern)\n",
    "    \n",
    "    for fits_path in fits_path_list:\n",
    "        gong_fits = fits.open(fits_path)\n",
    "        \n",
    "        gong_fits_header_keys = list(gong_fits[0].header.keys())\n",
    "                \n",
    "        # Pass to next FITS file if header information is missing\n",
    "        if 'CAR_ROT' not in gong_fits_header_keys:\n",
    "            continue\n",
    "        \n",
    "        # Carrington Rotation\n",
    "        CR_str = f'CR{gong_fits[0].header[\"CAR_ROT\"]}'\n",
    "        \n",
    "        gong_fits.close()\n",
    "            \n",
    "        os.rename(fits_path, gong_dir + CR_str + '.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_str = '2151.0342_2152.1035'\n",
    "\n",
    "plot_ch_map(list(reversed(HE_DATE_LIST)), cr_str, CH_MAP_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-debugger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "9702f0bff29bacff409d5ed2ffa7f0a67aa5aa939df8fc4f21a3e6487ad9172c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
