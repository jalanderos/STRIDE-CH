{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.io import fits\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib import cm, colormaps, ticker, colors\n",
    "from scipy import ndimage, optimize, stats, interpolate\n",
    "from skimage import morphology, filters, exposure, measure\n",
    "\n",
    "import sunpy.map\n",
    "from sunpy.coordinates import frames\n",
    "from astropy.coordinates import SkyCoord\n",
    "from sunpy.map.maputils import all_coordinates_from_map, coordinate_is_on_solar_disk\n",
    "\n",
    "\n",
    "import prepare_data\n",
    "import detect\n",
    "import plot_detection\n",
    "from settings import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Load lists of available datetimes of observations in the file system into memory.\n",
    "\n",
    "For the usage of ACWE maps, the `DATE_RANGE` in settings.py must encompass all available ACWE map files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract He I observation datetimes from FITS files\n",
    "HE_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, HE_DIR\n",
    ")\n",
    "\n",
    "# Extract magnetogram datetimes from 6302l FITS files\n",
    "MAG_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, MAG_DIR\n",
    ")\n",
    "\n",
    "# Extract EUV datetimes from FITS files\n",
    "EUV_DATE_LIST = prepare_data.get_fits_date_list(\n",
    "    DATE_RANGE, EUV_DIR\n",
    ")\n",
    "\n",
    "# Extract ACWE datetimes from FITS files\n",
    "ACWE_DATE_LIST = prepare_data.get_acwe_date_list(DATE_RANGE)\n",
    "\n",
    "date_strs = [HE_DATE_LIST[0], HE_DATE_LIST[-1]]\n",
    "file_date_str = f'{date_strs[0]}_{date_strs[-1]}'\n",
    "\n",
    "num_maps = len(HE_DATE_LIST)\n",
    "datetimes = [datetime.strptime(date_str, DICT_DATE_STR_FORMAT)\n",
    "             for date_str in date_strs]\n",
    "title_date_strs = [datetime.strftime(d, '%m/%d/%Y') for d in datetimes]\n",
    "DATE_RANGE_SUPTITLE = (f'{num_maps} Maps Evaluated from '\n",
    "                       + f'{title_date_strs[0]} to {title_date_strs[-1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for He I Observations:')\n",
    "prepare_data.display_dates(HE_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for Magnetograms:')\n",
    "prepare_data.display_dates(MAG_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for EUV Observations:')\n",
    "prepare_data.display_dates(EUV_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available Datetimes for ACWE Confidence Maps:')\n",
    "prepare_data.display_dates(ACWE_DATE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumental Darkening Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data into list\n",
    "he_data_list = []\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "    \n",
    "    he_data_list.append(np.flipud(he_map.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average of maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_he_data = np.mean(np.array(he_data_list), axis=0)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.title(DATE_RANGE_SUPTITLE)\n",
    "plt.imshow(avg_he_data,\n",
    "         #   vmin=-10, vmax=10,\n",
    "        #    vmin=-50, vmax=50,\n",
    "           vmin=-100, vmax=100,\n",
    "           cmap='RdBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrix to compute SVD\n",
    "all_he_data = np.array(he_data_list)\n",
    "num_maps = all_he_data.shape[0]\n",
    "image_shape = all_he_data.shape[1:]\n",
    "\n",
    "data_matrix = all_he_data.reshape(\n",
    "    (num_maps, image_shape[0]*image_shape[1])\n",
    ")\n",
    "data_matrix = data_matrix.T\n",
    "\n",
    "# # Retrieve and reshape an image from data matrix for verification\n",
    "# img_idx = 0\n",
    "# image = data_matrix[:,img_idx].reshape(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(data_matrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude last in range\n",
    "mode_range = [0,12]\n",
    "num_modes = mode_range[-1] - mode_range[0]\n",
    "\n",
    "num_cols = 2\n",
    "image_size = 7\n",
    "num_rows = int(np.ceil(num_modes/num_cols))\n",
    "fig = plt.figure(figsize=(image_size*num_cols, image_size*num_rows))\n",
    "axes = {}\n",
    "\n",
    "for i in range(0,num_modes):\n",
    "    axes[i] = fig.add_subplot(num_rows, num_cols, i + 1)\n",
    "    \n",
    "    mode = U[:,i].reshape(image_shape)\n",
    "    bound = np.min([np.abs(np.min(mode)), np.max(mode)])/10\n",
    "    \n",
    "    axes[i].imshow(-mode, interpolation='none',\n",
    "                   cmap=plt.cm.RdBu, vmin=-bound, vmax=bound)\n",
    "    axes[i].set_title(f'Eigen-Sun {i}', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.semilogy(np.arange(S.size), S)\n",
    "plt.ylabel('Singular Values')\n",
    "plt.xlabel('Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'output/Rotated_He/'\n",
    "\n",
    "for he_date_str in HE_DATE_LIST[:1]:\n",
    "    he_date_str = '2015_06_06__16_08'\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "    \n",
    "    # rotated_img_file = f'{out_dir}He{he_date_str}.jpg'\n",
    "    # if os.path.isfile(rotated_img_file):\n",
    "    #     print((f'He {he_date_str} rotation already exists.'))\n",
    "    #     continue\n",
    "    \n",
    "    datetime = datetime.strptime(he_date_str, DICT_DATE_STR_FORMAT)\n",
    "    P_angle = sunpy.coordinates.sun.P(time=datetime)\n",
    "\n",
    "    he_map_rotated = he_map.rotate(angle=P_angle)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = plot_detection.plot_he_map(fig, (1,1,1), he_map_rotated, he_date_str)\n",
    "# ax = plot_detection.plot_he_map(fig, (1, 1, 1), he_map, he_date_str)\n",
    "#     fig = plt.figure(figsize=(4,4))\n",
    "    \n",
    "    # fig = plt.figure(dpi=300)\n",
    "    # ax = fig.add_subplot()\n",
    "    # im = ax.imshow(np.flipud(he_map_rotated.data), vmin=-50, vmax=50, cmap='RdBu',\n",
    "    #                extent=[0,2700,2700,0])\n",
    "    # ax.set_title(he_date_str)\n",
    "    # fig.colorbar(im)\n",
    "\n",
    "    # plt.savefig(rotated_img_file)\n",
    "    # plt.close()\n",
    "    # print(f'{he_date_str} rotated map saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect.write_video(out_dir, fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPVT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nispdata.nso.edu/ftp/kpvt/daily/medres/ \n",
    "fits_path = TEST_HE_DIR + '011219mag.fits'\n",
    "kpvt_mag_disk_med_res_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/daily/raw/\n",
    "kpvt_he_fits_path = TEST_HE_DIR + '01dec19h.fits'\n",
    "kpvt_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    kpvt_he_fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/daily/raw/\n",
    "kpvt_mag_fits_path = TEST_HE_DIR + '01dec19m.fits'\n",
    "kpvt_mag_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    kpvt_mag_fits_path, header_list=['UTDATE'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# https://nispdata.nso.edu/ftp/kpvt/synoptic/hel.hires/\n",
    "fits_path = TEST_HE_DIR + 'hB1984.fits'\n",
    "kpvt_he_synoptic_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE9'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "# /ftp/kpvt/synoptic/helium\n",
    "fits_path = TEST_HE_DIR + 'h1984.fits'\n",
    "kpvt_he_synoptic_low_res_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['2001_12_19', 'CR1984']\n",
    "plot_detection.plot_hists(\n",
    "    [kpvt_he_disk_img, kpvt_he_synoptic_img], titles, semilogy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_he_disk_img, vmin=-200, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_mag_disk_img, vmin=-50, vmax=50, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpvt_he_synoptic_low_res_img, vmin=-300, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_path = HE_DIR + '2009_10_21__17_50.fts'\n",
    "rockwell_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = HE_DIR + '2011_03_28__17_35.fts'\n",
    "sarnoff_2011_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = HE_DIR + '2015_03_31__18_13.fts'\n",
    "sarnoff_2015_he_disk_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray, plt.cm.afmhot],\n",
    "    # print_header=True\n",
    ")[0]\n",
    "fits_path = TEST_HE_DIR + 'kbv2g150410t1408c2162_000_int-mas_dim-900.fits'\n",
    "sarnoff_2015_he_synoptic_img = plot_detection.plot_raw_fits_content(\n",
    "    fits_path, header_list=['DATE-OBS'], cmaps=[plt.cm.gray],\n",
    "    # print_header=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['2015_03_31__18_13', 'CR2162']\n",
    "plot_detection.plot_hists(\n",
    "    [sarnoff_2015_he_disk_img, sarnoff_2015_he_synoptic_img],\n",
    "    titles, semilogy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detection.plot_images(\n",
    "    image_list=[rockwell_he_disk_img, sarnoff_2011_he_disk_img],\n",
    "    title_list=['2009_10_21__17_50', '2011_03_28__17_35']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rockwell_he_disk_img, vmin=-100, vmax=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Map Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data\n",
    "\n",
    "Observations and pre-computed segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper Cases ---------------------\n",
    "# KPVT: H&H\n",
    "# he_date_str = '2003_07_14__18_07'\n",
    "\n",
    "# Rockwell: CH boundary difference\n",
    "# he_date_str = '2004_11_13__16_31'\n",
    "\n",
    "# Rockwell: Solar min case\n",
    "# he_date_str = '2009_10_22__18_38'\n",
    "\n",
    "# Sarnoff: Poster case\n",
    "# he_date_str = '2012_06_11__18_01'\n",
    "\n",
    "# Mini-Paper COSPAR Cases ---------\n",
    "\n",
    "# Best case\n",
    "# he_date_str = '2015_03_31__18_13'\n",
    "\n",
    "# Not so bad case\n",
    "he_date_str = '2015_06_06__16_08'\n",
    "\n",
    "# Other COSPAR Cases --------------\n",
    "\n",
    "# East axe\n",
    "# he_date_str = '2015_01_04__20_30'\n",
    "\n",
    "# N-S polar\n",
    "# he_date_str = '2015_01_20__20_25'\n",
    "\n",
    "# Center Hook\n",
    "# he_date_str = '2015_02_10__18_45'\n",
    "\n",
    "# QS blotch\n",
    "# he_date_str = '2015_04_18__17_22'\n",
    "\n",
    "# Other Cases ---------------------\n",
    "# # Rockwell: Null detection\n",
    "# he_date_str = '2004_11_20__17_07'\n",
    "\n",
    "# Rockwell: Smiley\n",
    "# he_date_str = '2004_12_03__16_36'\n",
    "\n",
    "# Sarnoff: Pre-updated FITS\n",
    "# he_date_str = '2012_04_01__17_03'\n",
    "\n",
    "# Sarnoff: Start of Updated FITS in May\n",
    "# he_date_str = '2012_05_01__18_08'\n",
    "\n",
    "# Sarnoff: East limb hammer\n",
    "# he_date_str = '2012_06_28__16_44'\n",
    "\n",
    "# Failed limb detection\n",
    "# he_date_str = '2012_07_08__19_37'\n",
    "\n",
    "# Sarnoff: Greatest area in 06/2012\n",
    "# he_date_str = '2012_06_09__19_20'\n",
    "\n",
    "he_fits_file = DATA_FITS_FORMAT.format(\n",
    "    data_dir=HE_DIR, date_str=he_date_str\n",
    ")\n",
    "he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "he_map_data = np.flipud(he_map.data)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plot_detection.plot_he_map(fig, (1, 1, 1), he_map, he_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_date_str = prepare_data.get_nearest_date_str(\n",
    "    MAG_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Extract magnetogram\n",
    "mag_fits_file = DATA_FITS_FORMAT.format(\n",
    "    data_dir=MAG_DIR, date_str=mag_date_str\n",
    ")\n",
    "mag_map = prepare_data.get_nso_sunpy_map(mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=mag_map)\n",
    "mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euv_date_str = prepare_data.get_nearest_date_str(\n",
    "    EUV_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Extract euv map\n",
    "euv_fits_file = DATA_FITS_FORMAT.format(\n",
    "    data_dir=EUV_DIR, date_str=euv_date_str\n",
    ")\n",
    "euv_map = sunpy.map.Map(euv_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plot_detection.plot_euv_map(fig, (1, 1, 1), euv_map, euv_date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processed Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract FITS file pre-processed map\n",
    "pre_process_fits_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "pre_processed_map = sunpy.map.Map(pre_process_fits_file)\n",
    "pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=pre_processed_map)\n",
    "pre_processed_map.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract differentially rotated magnetogram map\n",
    "reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                         f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved ensemble map\n",
    "ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "ensemble_map = sunpy.map.Map(ensemble_file)\n",
    "ensemble_map_data = np.flipud(ensemble_map.data)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "im = ensemble_map.plot(\n",
    "    axes=ax, title=ensemble_map.observer_coordinate.obstime.value,\n",
    "    cmap='magma', vmin=0, vmax=1)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete\n",
    "# PAPER_PLOT_DIR = 'paper/paper_plots/2024_08_plots/'\n",
    "PAPER_PLOT_DIR = 'paper/mini_paper_plots/2024_08_plots/'\n",
    "\n",
    "# DETECTION_VERSION_DIR = DETECT_DIR + 'v1_0/'\n",
    "# DETECTION_VERSION_DIR = DETECT_DIR + 'v1_0_No_Thresh/'\n",
    "DETECTION_VERSION_DIR = DETECT_DIR + 'v1_1/'\n",
    "# DETECTION_VERSION_DIR = DETECT_DIR + 'v1_1_No_Thresh/'\n",
    "DETECTION_MAP_SAVE_DIR = DETECTION_VERSION_DIR + 'Saved_fits_Files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACWE & Fused Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acwe_date_str = prepare_data.get_nearest_date_str(\n",
    "    ACWE_DATE_LIST, he_date_str\n",
    ")\n",
    "acwe_map = prepare_data.get_acwe_sunpy_map(\n",
    "    acwe_date_str, ACWE_DATE_LIST\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111, projection=acwe_map)\n",
    "im = acwe_map.plot(\n",
    "    axes=ax, title=acwe_map.observer_coordinate.obstime.value,\n",
    "    cmap='viridis', vmin=0, vmax=1\n",
    ")\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_file = f'{FUSED_MAP_SAVE_DIR}{he_date_str}_fused_map.fits'\n",
    "fused_map = sunpy.map.Map(fused_file)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111, projection=fused_map)\n",
    "im = fused_map.plot(\n",
    "    axes=ax, title=fused_map.observer_coordinate.obstime.value,\n",
    "    cmap='copper', vmin=0, vmax=1)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past STRIDE-CH Data Products\n",
    "\n",
    "v0.1-v0.5 Pre-Proccessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract saved pre-processed image array\n",
    "pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                    + '_pre_processed_map.npy')\n",
    "pre_processed_map_data = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "pre_processed_map = sunpy.map.Map(np.flipud(pre_processed_map_data), he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=pre_processed_map)\n",
    "pre_processed_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Heliographic coordinate reprojected magnetogram map\n",
    "hg_mag_fits_file = (f'{HELIOGRAPH_MAG_SAVE_DIR}'\n",
    "                    f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "hg_mag_map = sunpy.map.Map(hg_mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=hg_mag_map)\n",
    "hg_mag_map.plot(axes=ax, title='', vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved ensemble map\n",
    "ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "ensemble_map = sunpy.map.Map(ensemble_file)\n",
    "ensemble_map_data = np.flipud(ensemble_map.data)\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "im = ensemble_map.plot(axes=ax, title='', cmap='magma', vmin=0, vmax=100)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved ensemble map array and convert to Sunpy map\n",
    "ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing ensemble map from a single segmentation\n",
    "# percent_of_peak = 70\n",
    "# morph_radius_dist = 15\n",
    "\n",
    "percent_of_peak = 80\n",
    "morph_radius_dist = 13\n",
    "\n",
    "ch_mask_data = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, [percent_of_peak], [morph_radius_dist]\n",
    ")[0]\n",
    "ensemble_map_data = np.where(~np.isnan(np.flipud(pre_processed_map.data)), 0, np.nan)\n",
    "ensemble_map_data = np.where(ch_mask_data, 100, ensemble_map_data)\n",
    "\n",
    "ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), pre_processed_map.meta)\n",
    "ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract saved single mask array and convert to Sunpy map\n",
    "mask_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "mask_data = np.load(mask_file, allow_pickle=True)[-1]\n",
    "mask_map = sunpy.map.Map(np.flipud(mask_data), he_map.meta)\n",
    "mask_map.plot_settings['cmap'] = colormaps['gray']\n",
    "\n",
    "he_base_data = np.where(he_map.data == he_map.data[0,0], np.nan, he_map.data)\n",
    "he_base_map = sunpy.map.Map(he_base_data, he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5))\n",
    "\n",
    "plot_detection.plot_he_map(fig, (1, 4, 1), he_map, he_date_str)\n",
    "\n",
    "# Plot He I observation with overlayed detection contours\n",
    "ax = fig.add_subplot(142, projection=he_map)\n",
    "he_base_map.plot(axes=ax, vmin=-100, vmax=100, title=he_date_str,\n",
    "                    cmap='afmhot')\n",
    "for contour in mask_map.contour(0):\n",
    "    ax.plot_coord(contour, color='black', linewidth=1)\n",
    "\n",
    "plot_detection.plot_euv_map(fig, (1, 4, 3), euv_map, euv_date_str)\n",
    "\n",
    "ax = fig.add_subplot(144, projection=he_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50, title=mag_date_str)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop EUV map to similar zoom level to other observations \n",
    "he_submap = he_map.submap(\n",
    "    bottom_left=SkyCoord(\n",
    "        # Tx=-750*u.arcsec, Ty=-600*u.arcsec,\n",
    "        Tx=-150*u.arcsec, Ty=-600*u.arcsec,\n",
    "        frame=he_map.coordinate_frame\n",
    "    ),\n",
    "    top_right=SkyCoord(\n",
    "        # Tx=0*u.arcsec, Ty=0*u.arcsec,\n",
    "        Tx=600*u.arcsec, Ty=0*u.arcsec,\n",
    "        frame=he_map.coordinate_frame\n",
    "    )\n",
    ")\n",
    "he_submap.plot(vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = np.where(he_submap.data.flatten() == 0, np.nan, he_submap.data.flatten())\n",
    "edges = np.arange(-100, 101, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5), dpi=150)\n",
    "ax = fig.add_axes(111)\n",
    "\n",
    "data = ax.hist(\n",
    "    hist_data, edges, histtype='step',\n",
    "    color='white', edgecolor='black', linewidth=3,\n",
    ")\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_xlabel('Equivalent Width (m\\u00C5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication Plots\n",
    "\n",
    "Requires data loaded from the Single Map Data section.\n",
    "\n",
    "See Multi-Date Outcomes > Confidence Histograms for lat,lon histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAPER_PLOT_DATE_NAME = 'kpvt-'\n",
    "# PAPER_PLOT_DATE_NAME = 'rockwell-decline-'\n",
    "# PAPER_PLOT_DATE_NAME = 'rockwell-min-'\n",
    "# PAPER_PLOT_DATE_NAME = 'sarnoff-'\n",
    "\n",
    "# PAPER_PLOT_DATE_NAME = '2012-06-'\n",
    "# PAPER_PLOT_DATE_NAME = '2015-03-'\n",
    "# PAPER_PLOT_DATE_NAME = '2015-03-v1_0'\n",
    "# PAPER_PLOT_DATE_NAME = '2015-04-05-'\n",
    "# PAPER_PLOT_DATE_NAME = '2015-06-06-'\n",
    "# PAPER_PLOT_DATE_NAME = '2015-06-06-v1_0-'\n",
    "PAPER_PLOT_DATE_NAME = '2015-06-11-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He I Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_mode = False\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "\n",
    "if pretty_mode:\n",
    "    # Remove off disk pixels\n",
    "    all_hp_coords = sunpy.map.maputils.all_coordinates_from_map(he_map)\n",
    "    on_disk_mask = sunpy.map.maputils.coordinate_is_on_solar_disk(all_hp_coords)\n",
    "    he_plot_map = sunpy.map.Map(\n",
    "        np.where(on_disk_mask, he_map.data, -100), he_map.meta\n",
    "    )\n",
    "    \n",
    "    darkened_cmap_array = colormaps['afmhot'](\n",
    "        np.linspace(0, 0.9, 256)\n",
    "    )\n",
    "    plot_cmap = colors.ListedColormap(darkened_cmap_array)\n",
    "    \n",
    "    # Saturate post-2010 Sarnoff imagery at +/- 100mA\n",
    "    ax = fig.add_subplot(111, projection=he_map)\n",
    "    he_plot_map.plot(axes=ax, vmin=-100, vmax=100, cmap=plot_cmap)\n",
    "    \n",
    "    pretty_str = '-pretty'\n",
    "else:\n",
    "    he_plot_map = he_map\n",
    "    pretty_str = ''\n",
    "    \n",
    "    ax = plot_detection.plot_he_map(\n",
    "        fig, (1, 1, 1), he_plot_map, he_date_str\n",
    "    )\n",
    "\n",
    "ax.set_title('')\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + f'he{pretty_str}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EUV Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "ax = plot_detection.plot_euv_map(fig, (1, 1, 1), euv_map, euv_date_str)\n",
    "\n",
    "ax.set_title('')\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + 'euv.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnetogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to access Sunpy colormaps via matplotlib.colormaps\n",
    "import sunpy.visualization.colormaps as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_mode = False\n",
    "pretty_cmap_thresh = 0.2\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 10), dpi=300)\n",
    "fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "ax = fig.add_subplot(111, projection=mag_map)\n",
    "\n",
    "if pretty_mode:\n",
    "    # Remove off disk pixels\n",
    "    all_hp_coords = sunpy.map.maputils.all_coordinates_from_map(mag_map)\n",
    "    on_disk_mask = sunpy.map.maputils.coordinate_is_on_solar_disk(all_hp_coords)\n",
    "    mag_plot_map = sunpy.map.Map(\n",
    "        np.where(on_disk_mask, mag_map.data, -2000), mag_map.meta\n",
    "    )\n",
    "    pretty_str = '-pretty'\n",
    "    \n",
    "    black_background = [0,0,0,1]\n",
    "    \n",
    "    # # Blue to black to red attempts\n",
    "    # blue_black_cmap_array = colormaps['seismic_r'](\n",
    "    #     np.linspace(0.5 + pretty_cmap_thresh, 1, 256)\n",
    "    # )\n",
    "    # black_red_cmap_array = colormaps['seismic_r'](\n",
    "    #     np.linspace(0, 0.5 - pretty_cmap_thresh, 256)\n",
    "    # )\n",
    "    # plot_cmap = colors.ListedColormap(\n",
    "    #     np.vstack((\n",
    "    #         black_background, blue_black_cmap_array,\n",
    "    #         # black_background, black_background,\n",
    "    #         black_red_cmap_array\n",
    "    #     ))\n",
    "    # )\n",
    "    \n",
    "    brightened_cmap_array = colormaps['RdBu_r'](\n",
    "        np.linspace(0, 1, 256)\n",
    "    )\n",
    "    plot_cmap = colors.ListedColormap(\n",
    "        np.vstack((black_background, brightened_cmap_array))\n",
    "    )\n",
    "    mag_plot_map.plot(axes=ax, vmin=-50, vmax=50, cmap=plot_cmap)\n",
    "    \n",
    "    # # HMI colormap\n",
    "    # hmi_cmap_array = colormaps['hmimag'](\n",
    "    #     np.linspace(0, 1, 256)\n",
    "    # )\n",
    "    # plot_cmap = colors.ListedColormap(\n",
    "    #     np.vstack((black_background, hmi_cmap_array))\n",
    "    # )\n",
    "    # mag_plot_map.plot(axes=ax, vmin=-1500, vmax=1500, cmap=plot_cmap)\n",
    "else:\n",
    "    mag_plot_map = sunpy.map.Map(\n",
    "        np.where(mag_map.data == 0, -50, mag_map.data), mag_map.meta\n",
    "    )\n",
    "    mag_plot_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "    \n",
    "    pretty_str = ''\n",
    "\n",
    "ax.set_title('')\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + f'mag{pretty_str}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Plot with Confidence, Posterior Probability, or Unipolarity-Labeled Colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_type = 'confidence'\n",
    "# cb_type = 'v1.1_no_thresh'\n",
    "# cb_type = 'v1.0_no_thresh'\n",
    "background_offset = -0.4\n",
    "confidence_range = [0,1]\n",
    "cmap = 'magma'\n",
    "\n",
    "probability_threshold = 0.4\n",
    "gray_below_thresh = ('no_thresh' in cb_type)\n",
    "\n",
    "# Ensemble map to plot with the background solar disk set to an offset value\n",
    "offset_bg_ensemble_map_data = np.where(\n",
    "    ensemble_map.data == 0, background_offset, ensemble_map.data\n",
    ")\n",
    "ensemble_plot_map = sunpy.map.Map(offset_bg_ensemble_map_data, he_map.meta)\n",
    "\n",
    "# Create colormap objects with brightened colors for nonzero-valued pixels.\n",
    "# The plot colormap has a black background as well, but not the colorbar.\n",
    "# Arrays are first sampled from colormap objects, then modified colormap\n",
    "# objects are created. -------------------------------------------------------\n",
    "black_background = [0,0,0,1]\n",
    "zero_confidence_color_val = np.interp(\n",
    "    0, [background_offset, 1], confidence_range\n",
    ")\n",
    "\n",
    "if gray_below_thresh:\n",
    "    cmap_sample_num = int((1 - probability_threshold)*256)\n",
    "    gray_cmap_array = colormaps['gray'](\n",
    "        np.linspace(0.4, 0.6, int(probability_threshold*256))\n",
    "    )\n",
    "    \n",
    "    cmap_array = colormaps[cmap](np.linspace(0, 1, cmap_sample_num))\n",
    "    plot_cmap = colors.ListedColormap(\n",
    "        np.vstack((black_background, gray_cmap_array, cmap_array))\n",
    "    )\n",
    "    \n",
    "    brightened_cmap_array = colormaps[cmap](\n",
    "        np.linspace(zero_confidence_color_val, 1, cmap_sample_num)\n",
    "    )\n",
    "    cb_cmap = colors.ListedColormap(\n",
    "        np.vstack((gray_cmap_array, brightened_cmap_array))\n",
    "    )\n",
    "    gray_below_thresh_str = '-gray'\n",
    "else:\n",
    "    cmap_array = colormaps[cmap](np.linspace(0, 1, 256))\n",
    "    plot_cmap = colors.ListedColormap(\n",
    "        np.vstack((black_background, cmap_array))\n",
    "    )\n",
    "\n",
    "    brightened_cmap_array = colormaps[cmap](\n",
    "    np.linspace(zero_confidence_color_val, 1, 256)\n",
    "    )\n",
    "    cb_cmap = colors.ListedColormap(brightened_cmap_array)\n",
    "    gray_below_thresh_str = ''\n",
    "\n",
    "# Plot figure -----------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(4, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=ensemble_plot_map)\n",
    "ensemble_plot_map.plot(\n",
    "    axes=ax, cmap=plot_cmap, annotate=False,\n",
    "    vmin=background_offset, vmax=confidence_range[1]\n",
    ")\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "ensemble_plot_map.draw_grid(axes=ax)\n",
    "\n",
    "# Remove xy grid and ticks\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cb_cmap), ax=ax)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "if cb_type == 'confidence':\n",
    "    cb.set_label('Confidence', labelpad=11, rotation=-90)\n",
    "    plt.savefig(\n",
    "        PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + 'stride.jpeg',\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "else:\n",
    "    if cb_type == 'v1.1_no_thresh':\n",
    "        cb.set_label(r'Posterior Probability', labelpad=13, rotation=-90)\n",
    "    elif cb_type == 'v1.0_no_thresh':\n",
    "        cb.set_label(r'Unipolarity $U$', labelpad=13, rotation=-90)\n",
    "    \n",
    "    plt.savefig(\n",
    "        (PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME\n",
    "         + f'no-thresh{gray_below_thresh_str}.jpeg'),\n",
    "        bbox_inches='tight'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old v1.0 with optional dark or light mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark = True\n",
    "background_offset = -0.4\n",
    "confidence_range = [0,1]\n",
    "\n",
    "\n",
    "# Ensemble map to plot with the background solar disk set to an offset value\n",
    "if dark:\n",
    "    cmap = 'magma'\n",
    "    ensemble_plot_map = sunpy.map.Map(\n",
    "        np.where(ensemble_map.data == 0, background_offset, ensemble_map.data), he_map.meta\n",
    "    )\n",
    "else:\n",
    "    cmap = 'viridis'\n",
    "    ensemble_plot_map = sunpy.map.Map(\n",
    "        np.where(ensemble_map.data == 0, np.nan, ensemble_map.data), he_map.meta\n",
    "    )\n",
    "\n",
    "# Create colormap objects with brightened colors for nonzero-valued pixels,\n",
    "# as well as a black background ------------------------------------------------\n",
    "zero_confidence_color = np.interp(0, [background_offset, 1], confidence_range)\n",
    "color_range = np.linspace(zero_confidence_color, 1, 256)\n",
    "black_background = [0,0,0,1]\n",
    "brightened_cmap_array = colormaps[cmap](color_range)\n",
    "\n",
    "black_background_cmap = colors.ListedColormap(\n",
    "    np.vstack((black_background, brightened_cmap_array))\n",
    ")\n",
    "cb_cmap = colors.ListedColormap(brightened_cmap_array)\n",
    "\n",
    "# Plot figure -----------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(4, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=ensemble_plot_map)\n",
    "ensemble_plot_map.plot(\n",
    "    axes=ax, cmap=black_background_cmap, annotate=False,\n",
    "    vmin=confidence_range[0], vmax=confidence_range[1]\n",
    ")\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "\n",
    "if dark:\n",
    "    ensemble_plot_map.draw_grid(axes=ax)\n",
    "else:\n",
    "    ensemble_plot_map.draw_grid(axes=ax, color='black')\n",
    "    ensemble_plot_map.draw_limb(axes=ax, color='black')\n",
    "\n",
    "# Remove xy grid and ticks\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cb_cmap), ax=ax)\n",
    "# custom_font = {'fontname':'Times New Roman'}\n",
    "cb.set_label(r'Unipolarity $U$', labelpad=13, rotation=-90)#, **custom_font)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME +'no-thresh.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACWE Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background_offset = -0.2\n",
    "background_offset = 0\n",
    "confidence_range = [0,1]\n",
    "cmap = 'inferno'\n",
    "\n",
    "\n",
    "# ACWE map to plot with the background solar disk set to an offset value\n",
    "offset_bg_acwe_map_data = np.where(\n",
    "    acwe_map.data == 0, background_offset, acwe_map.data\n",
    ")\n",
    "acwe_plot_map = sunpy.map.Map(offset_bg_acwe_map_data, acwe_map.meta)\n",
    "\n",
    "# Create colormap objects with brightened colors for nonzero-valued pixels.\n",
    "# The plot colormap has a black background as well, but not the colorbar.\n",
    "# Arrays are first sampled from colormap objects, then modified colormap\n",
    "# objects are created. -------------------------------------------------------\n",
    "black_background = [0,0,0,1]\n",
    "zero_confidence_color_val = np.interp(\n",
    "    0, [background_offset, 1], confidence_range\n",
    ")\n",
    "\n",
    "cmap_array = colormaps[cmap](np.linspace(0, 1, 256))\n",
    "plot_cmap = colors.ListedColormap(\n",
    "    np.vstack((black_background, cmap_array))\n",
    ")\n",
    "\n",
    "brightened_cmap_array = colormaps[cmap](\n",
    "np.linspace(zero_confidence_color_val, 1, 256)\n",
    ")\n",
    "cb_cmap = colors.ListedColormap(brightened_cmap_array)\n",
    "\n",
    "# Plot figure -----------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(4, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=acwe_plot_map)\n",
    "acwe_plot_map.plot(\n",
    "    axes=ax, cmap=plot_cmap, annotate=False,\n",
    "    vmin=background_offset, vmax=confidence_range[1]\n",
    ")\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "acwe_plot_map.draw_grid(axes=ax)\n",
    "\n",
    "# Remove xy grid and ticks\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cb_cmap), ax=ax)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "cb.set_label('Confidence', labelpad=11, rotation=-90)\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + f'acwe-{cmap}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fused Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_offset = -0.6\n",
    "confidence_range = [0,1]\n",
    "cmap = 'copper'\n",
    "\n",
    "# fused map to plot with the background solar disk set to an offset value\n",
    "offset_bg_fused_map_data = np.where(\n",
    "    fused_map.data == 0, background_offset, fused_map.data\n",
    ")\n",
    "fused_plot_map = sunpy.map.Map(offset_bg_fused_map_data, he_map.meta)\n",
    "\n",
    "# Create colormap objects with brightened colors for nonzero-valued pixels.\n",
    "# The plot colormap has a black background as well, but not the colorbar.\n",
    "# Arrays are first sampled from colormap objects, then modified colormap\n",
    "# objects are created. -------------------------------------------------------\n",
    "black_background = [0,0,0,1]\n",
    "zero_confidence_color_val = np.interp(\n",
    "    0, [background_offset, 1], confidence_range\n",
    ")\n",
    "\n",
    "cmap_array = colormaps[cmap](np.linspace(0, 1, 256))\n",
    "plot_cmap = colors.ListedColormap(\n",
    "    np.vstack((black_background, cmap_array))\n",
    ")\n",
    "\n",
    "brightened_cmap_array = colormaps[cmap](\n",
    "np.linspace(zero_confidence_color_val, 1, 256)\n",
    ")\n",
    "cb_cmap = colors.ListedColormap(brightened_cmap_array)\n",
    "\n",
    "# Plot figure -----------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(4, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=fused_plot_map)\n",
    "fused_plot_map.plot(\n",
    "    axes=ax, cmap=plot_cmap, annotate=False,\n",
    "    vmin=background_offset, vmax=confidence_range[1]\n",
    ")\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "fused_plot_map.draw_grid(axes=ax)\n",
    "\n",
    "# Remove xy grid and ticks\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "cb = plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cb_cmap), ax=ax)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "cb.set_label('Confidence', labelpad=11, rotation=-90)\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + 'fused.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He I Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = np.where(he_map_data.flatten() == 0, np.nan, he_map_data.flatten())\n",
    "edges = np.arange(-100, 101, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(7*0.6,5*0.6), dpi=300)\n",
    "ax = fig.add_axes(111)\n",
    "\n",
    "data = ax.hist(\n",
    "    hist_data, edges, histtype='step',\n",
    "    color='white', edgecolor='black', linewidth=2,\n",
    ")\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "ax.set_xticks([-100, -50, 0, 50, 100])\n",
    "\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_xlabel('He I Pixel Intensity')\n",
    "\n",
    "fig.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + 'hist-he.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processed He I Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = pre_processed_map_data.flatten()\n",
    "edges = np.arange(-1, 1.1, 0.1)\n",
    "\n",
    "fig = plt.figure(figsize=(7*0.6,5*0.6), dpi=300)\n",
    "ax = fig.add_axes(111)\n",
    "\n",
    "data = ax.hist(\n",
    "    hist_data, edges, histtype='step',\n",
    "    color='white', edgecolor='black', linewidth=2,\n",
    ")\n",
    "ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "ax.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_xlabel('Rescaled He I Pixel Intensity')\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + 'hist-preproc.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "# percent_of_peak = 70\n",
    "\n",
    "thresh_bound = detect.get_thresh_bound(pre_processed_map_data, percent_of_peak)\n",
    "\n",
    "thresh_mask = np.where(pre_processed_map_data > thresh_bound, 1, 0)\n",
    "empty_disk = np.where(~np.isnan(pre_processed_map_data), 0, np.nan)\n",
    "thresh_disk = np.where(thresh_mask, 1, empty_disk)\n",
    "thresh_map = sunpy.map.Map(np.flipud(thresh_disk), he_map.meta)\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.imshow(thresh_disk, cmap='magma')\n",
    "\n",
    "# ax.tick_params(left=False, right=False, labelleft=False,\n",
    "#                labelbottom=False, bottom=False)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=thresh_map)\n",
    "thresh_map.plot(axes=ax, cmap='magma', annotate=False)\n",
    "thresh_map.draw_grid(axes=ax)\n",
    "\n",
    "\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + f't{percent_of_peak}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Segmentation Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_of_peak = 90\n",
    "# morph_radius = 13\n",
    "\n",
    "percent_of_peak = 70\n",
    "morph_radius = 15\n",
    "\n",
    "ch_mask = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, [percent_of_peak], [morph_radius]\n",
    ")[0]\n",
    "\n",
    "empty_disk = np.where(~np.isnan(pre_processed_map_data), 0, np.nan)\n",
    "ch_disk = np.where(ch_mask, 1, empty_disk)\n",
    "ch_map = sunpy.map.Map(np.flipud(ch_disk), he_map.meta)\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.imshow(ch_disk, cmap='magma')\n",
    "\n",
    "# ax.tick_params(left=False, right=False, labelleft=False,\n",
    "#                labelbottom=False, bottom=False)\n",
    "\n",
    "fig = plt.figure(figsize=(3, 3), dpi=300)\n",
    "ax = fig.add_subplot(111, projection=ch_map)\n",
    "ch_map.plot(axes=ax, cmap='magma', annotate=False)\n",
    "ch_map.draw_grid(axes=ax)\n",
    "\n",
    "\n",
    "ax.coords.grid(False)\n",
    "for coord in ax.coords:\n",
    "    coord.set_ticks_visible(False)\n",
    "    coord.set_ticklabel_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + PAPER_PLOT_DATE_NAME + f't{percent_of_peak}-r{morph_radius}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC\n",
    "\n",
    "Plots the following:\n",
    "- Community CH detection method scatter from COSPAR benchmark dataset\n",
    "- STRIDE v1.1 ROC curves for training vs testing data\n",
    "\n",
    "Pre-run CH Labels functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.479 # 2012 only\n",
    "outcomes_for_features = ['unipolarity', 'grad_median', 'cm_foreshort']\n",
    "labeled_cand_df_file = CH_LABEL_DIR + '2024_06_06__20_29_outcomes.csv'\n",
    "\n",
    "# Load LDA ---------------------------------------------------------------------\n",
    "with open(LDA_FILE_NAME, 'rb') as lda_file:\n",
    "    lda = pickle.load(lda_file)\n",
    "\n",
    "# Load classified features and labels ------------------------------------------\n",
    "labeled_cand_df = pd.read_csv(labeled_cand_df_file)\n",
    "classify_cand_df = labeled_cand_df[labeled_cand_df['label_id'] != 0.5]\n",
    "num_cands = classify_cand_df.shape[0]\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])\n",
    "\n",
    "num_train_cands = int(train_fraction*num_cands)\n",
    "X_train = X[:num_train_cands, :]\n",
    "y_train = y[:num_train_cands]\n",
    "X_test = X[num_train_cands:, :]\n",
    "y_test = y[num_train_cands:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_probabilities = [0.1, 0.2, 0.3, 0.4, 0.8, 0.9]\n",
    "probability_thresholds = np.linspace(0,1,500)\n",
    "\n",
    "train_probabilities = lda.predict_proba(X_train)[:,1]\n",
    "train_roc_rates = np.array(\n",
    "    [get_roc_rates(train_probabilities, y_train, probability_thresh)\n",
    "     for probability_thresh in probability_thresholds]\n",
    ")\n",
    "\n",
    "test_probabilities = lda.predict_proba(X_test)[:,1]\n",
    "test_roc_rates = np.array(\n",
    "    [get_roc_rates(test_probabilities, y_test, probability_thresh)\n",
    "     for probability_thresh in probability_thresholds]\n",
    ")\n",
    "scatter_roc_rates = np.array(\n",
    "    [get_roc_rates(test_probabilities, y_test, probability_thresh)\n",
    "     for probability_thresh in scatter_probabilities]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_thresh_idx = scatter_probabilities.index(0.4)\n",
    "scatter_roc_rates[p_thresh_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "colors = sns.color_palette('muted')\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_mode = True\n",
    "plt.figure(figsize=(3.5,3.5), dpi=300)\n",
    "\n",
    "# Marker size\n",
    "s = 20\n",
    "\n",
    "# STRIDE ROC ----------------------------------------------------------------------\n",
    "plt.plot(\n",
    "    test_roc_rates[:,1], test_roc_rates[:,0], \n",
    "    label=r'STRIDE-CH', color='peru', \n",
    "    linewidth=2\n",
    ")\n",
    "# plt.plot(\n",
    "#     train_roc_rates[:,1], train_roc_rates[:,0], \n",
    "#     label=r'STRIDE-CH Train', color='peru', \n",
    "#     linewidth=1\n",
    "# )\n",
    "\n",
    "# Corner COSPAR ------------------------------------------------------------------\n",
    "plt.scatter(0.21,0.77, s,label='Baseline', color='k', marker='s')\n",
    "plt.scatter(0,0.78, s,label='CHIMERA', color=colors[0])\n",
    "# plt.scatter(0.04,0.84,label='SPOCA (o:Base | ^:HEK)')\n",
    "plt.scatter(0.04,0.84, s,label='SPoCA-CH/-HEK', color=colors[1])\n",
    "# plt.scatter(0.25,0.87,marker='^',color='tab:orange')  # SPOCA-HEK\n",
    "plt.scatter(0.25,0.87, s,marker='^', color=colors[1])      # SPOCA-HEK\n",
    "# plt.scatter(0.07,0.83,label='ACWE  (o:03     | ^:  04)')\n",
    "plt.scatter(0.07,0.83, s,label='ACWE03/04', color=colors[2])\n",
    "# plt.scatter(0.29,0.86,marker='^',color='tab:green')   # ACWE-04\n",
    "plt.scatter(0.29,0.86, s,marker='^', color=colors[2])      # ACWE-04\n",
    "plt.scatter(0.13,0.85, s,label='CRONNOS', color=colors[3])\n",
    "plt.scatter(0.21,0.79, s,label='CHARM', color=colors[4])\n",
    "plt.scatter(0.35,0.82, s,label='CNN193', color=colors[5])\n",
    "plt.scatter(0.38,0.98, s,label='WWWBCS', color=colors[6])\n",
    "\n",
    "if zoom_mode:\n",
    "    zoom_str = '-zoom'\n",
    "    plt.xlim([-0.015,0.45])\n",
    "    plt.ylim([0.55,1.022])\n",
    "\n",
    "    # STRIDE Scatter -------------------------------------------------\n",
    "    plt.scatter(\n",
    "        scatter_roc_rates[:,1], scatter_roc_rates[:,0],\n",
    "        color='peru', marker='s'\n",
    "    )\n",
    "    for prob, xi, yi in zip(\n",
    "            scatter_probabilities, scatter_roc_rates[:,1],\n",
    "            scatter_roc_rates[:,0]\n",
    "        ):\n",
    "        \n",
    "        if prob == 0.4:\n",
    "            xytext = (-6, 6)\n",
    "        else:\n",
    "            xytext = (2, -10)\n",
    "        \n",
    "        plt.annotate(\n",
    "            f'{prob:.1f}', xy=(xi, yi), xycoords='data', \n",
    "            xytext=xytext,    # Corner zoom\n",
    "            # xytext=(-20, 8),  # Unzoomed\n",
    "            textcoords='offset points', fontsize=7.5\n",
    "        )\n",
    "else:\n",
    "    zoom_str = ''\n",
    "    plt.xlim([-0.05,1.05])\n",
    "    plt.ylim([-0.05,1.05])\n",
    "\n",
    "    # All COSPAR ----------------------------------------------------\n",
    "    plt.scatter(0.49,0.99, s,label='CHMAP', color=colors[7])\n",
    "    plt.scatter(0.54,0.86, s,label='CHIPS', color=colors[8])\n",
    "    plt.scatter(0.62,0.87, s,label='SYNCH', color=colors[9])\n",
    "    plt.scatter(0.85,0.92, s,label='CHORTLE', marker='d', color=colors[0])\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right', reverse=True, fontsize=7.5)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + f'ROC{zoom_str}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Segmentation\n",
    "\n",
    "Requires single map extraction from single map data section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "## Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_map_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "arrays = [he_map_data, np.flipud(pre_processed_map_v0_5_1_map.data)]\n",
    "titles = ['L2 Observation', 'Pre-Processed Observation']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, pre_process_v0_4_he]\n",
    "titles = ['L2 Observation', 'Pre-Processed Observation']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    pre_process_v0_4_he = detect.pre_process_v0_4(raw_he)\n",
    "    arrays = [raw_he, pre_process_v0_4_he]\n",
    "    titles = [he_date_str, 'Pre-Processed']\n",
    "    plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_1_he, he_high_cut, he_nan = detect.pre_process_v0_1(\n",
    "    he_map_data, peak_count_cutoff_percent=0.1\n",
    ")\n",
    "\n",
    "arrays = [he_map_data, he_nan, he_high_cut, pre_process_v0_1_he]\n",
    "titles = ['he', 'he NaN', 'he High Cut', 'he Band Cut']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    pre_process_v0_1_he = detect.pre_process_v0_1(raw_he)[0]\n",
    "    arrays = [raw_he, pre_process_v0_1_he]\n",
    "    titles = [he_date_str, 'Pre-Processed']\n",
    "    plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY: Pre-Processed Map Reprojected to Heliographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "fig = plt.figure(figsize=(11, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1, projection=he_map)\n",
    "he_map.plot(axes=ax, title=he_map.date)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2, projection=he_map)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100, title='+/-100 mAngstrom Saturation')\n",
    "\n",
    "ax = fig.add_subplot(2, 2, (3,4), projection=pre_processed_vY_map)\n",
    "pre_processed_vY_map.plot(axes=ax, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1-v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "if not os.path.isdir(PREPROCESS_NPY_SAVE_DIR):\n",
    "    os.makedirs(PREPROCESS_NPY_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    if os.path.isfile(pre_process_file) and not overwrite:\n",
    "        print((f'He {he_date_str} pre-processed map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map_data = prepare_data.get_image_from_fits(he_fits_file)\n",
    "    \n",
    "    # pre_processed_map_data = detect.pre_process_v0_1(he_map_data)[0]\n",
    "    pre_processed_map_data = detect.pre_process_v0_4(he_map_data)\n",
    "    \n",
    "    save_list = [he_date_str, pre_processed_map_data]\n",
    "    np.save(pre_process_file, np.array(save_list, dtype=object), \n",
    "            allow_pickle=True)\n",
    "    print(f'{he_date_str} Pre-Processed Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY: Pre-Processed Map Reprojected to Heliographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "if not os.path.isdir(PREPROCESS_MAP_SAVE_DIR):\n",
    "    os.makedirs(PREPROCESS_MAP_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    if os.path.isfile(pre_process_file) and not overwrite:\n",
    "        print((f'He {he_date_str} pre-processed map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "    \n",
    "    pre_processed_map = detect.pre_process_vY(he_map)\n",
    "    \n",
    "    pre_processed_map.save(pre_process_file, overwrite=overwrite)\n",
    "    print(f'{he_date_str} Pre-Processed Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HE I/EUV Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "\n",
    "if not os.path.isdir(RATIO_SAVE_DIR):\n",
    "    os.makedirs(RATIO_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "\n",
    "    euv_date_str = prepare_data.get_nearest_date_str(\n",
    "        EUV_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "    if os.path.isfile(ratio_fits_file):\n",
    "        if overwrite:\n",
    "            os.remove(ratio_fits_file)\n",
    "        else:\n",
    "            print((f'{he_date_str} to {euv_date_str} ratio already exists.'))\n",
    "            continue\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    # Remove error causing keywords which have invalid ascii content\n",
    "    he_map.meta.pop('history')\n",
    "    he_map.meta.pop('comment')\n",
    "    \n",
    "    # Extract and reproject EUV observation\n",
    "    euv_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=EUV_DIR, date_str=euv_date_str\n",
    "    )\n",
    "    euv_map = sunpy.map.Map(euv_fits_file)\n",
    "    reprojected_euv_map = prepare_data.diff_rotate(\n",
    "        input_map=euv_map, target_map=he_map\n",
    "    )\n",
    "    \n",
    "    # Pre-process He I data via background removal and upper cutoff\n",
    "    # Satisfactory only for Sarnoff camera observations\n",
    "    he_map_data = np.where(he_map.data == he_map.data[0,0],\n",
    "                           np.nan, he_map.data)\n",
    "    he_map_data = np.where(he_map_data >= np.percentile(he_map_data, 99.9),\n",
    "                           np.nan, he_map_data)\n",
    "    \n",
    "    ratio_data = np.divide(he_map_data, (reprojected_euv_map.data)**0.5)\n",
    "    ratio_map = sunpy.map.Map(ratio_data, he_map.meta)\n",
    "    \n",
    "    # Save to FITS files\n",
    "    ratio_map.save(ratio_fits_file)\n",
    "    print(f'He{he_date_str} to EUV{euv_date_str} ratio saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He I/EUV Ratio Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "if not os.path.isdir(PREPROCESS_NPY_SAVE_DIR):\n",
    "    os.makedirs(PREPROCESS_NPY_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    euv_date_str = prepare_data.get_nearest_date_str(\n",
    "        EUV_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    if os.path.isfile(pre_process_file) and not overwrite:\n",
    "        print((f'He {he_date_str} pre-processed map already exists.'))\n",
    "        continue\n",
    "\n",
    "    ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "\n",
    "    ratio_map_data = prepare_data.get_image_from_fits(ratio_fits_file)\n",
    "    \n",
    "    # pre_processed_map_data = detect.pre_process_v0_1(ratio_map_data)[0]\n",
    "    pre_processed_map_data = detect.pre_process_v0_4(ratio_map_data)\n",
    "    \n",
    "    save_list = [he_date_str, pre_processed_map_data]\n",
    "    np.save(pre_process_file, np.array(save_list, dtype=object), \n",
    "            allow_pickle=True)\n",
    "    print(f'{he_date_str} Pre-Processed Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio Comparison\n",
    "He I, EUV, & He I/EUV Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "out_dir = OUTPUT_DIR + 'Ratio_Comparison/' + DATE_DIR\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}He{he_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'He {he_date_str} ratio comparison already exists.'))\n",
    "        continue\n",
    "\n",
    "    euv_date_str = prepare_data.get_nearest_date_str(\n",
    "        EUV_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    # Extract observations and ratio map\n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    euv_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=EUV_DIR, date_str=euv_date_str\n",
    "    )\n",
    "    euv_map = sunpy.map.Map(euv_fits_file)\n",
    "    \n",
    "    ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "    ratio_map = sunpy.map.Map(ratio_fits_file)\n",
    "    \n",
    "    # Create panel plot\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "    plot_detection.plot_he_map(fig, (1, 3, 1), he_map, he_date_str)\n",
    "\n",
    "    plot_detection.plot_euv_map(fig, (1, 3, 2), euv_map, euv_date_str)\n",
    "\n",
    "    ax = fig.add_subplot(133, projection=he_map)\n",
    "    ratio_map.plot(axes=ax, cmap='jet', vmin=-1, vmax=6)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(comparison_img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'He {he_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_peak_counts(array):\n",
    "    \"\"\"Retrieve an array with the value of peak counts replaced with NaN.\n",
    "    \"\"\"\n",
    "    peak_counts_val = detect.get_peak_counts_loc(array, bins_as_percent=False)\n",
    "    zero_vals = (array > peak_counts_val - 1e-2) & (array < peak_counts_val + 1e-2)\n",
    "    \n",
    "    return np.where(zero_vals, np.NaN, array)\n",
    "\n",
    "def band_pass(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by setting background to NaN\n",
    "    and a simple brightness band pass.\n",
    "    \"\"\"\n",
    "    he_nan = np.where(raw_he == 0, np.NaN, raw_he)\n",
    "    \n",
    "    he_high_cut = np.where(he_nan > 100, np.NaN, he_nan)\n",
    "    # he_band_cut = np.where(he_high_cut < -100, np.NaN, he_high_cut)\n",
    "    he_band_cut = np.clip(he_high_cut, -100, 100)\n",
    "    \n",
    "    return he_band_cut, he_high_cut, he_nan\n",
    "\n",
    "\n",
    "def equalize(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by setting background to NaN\n",
    "    and a simple brightness band pass.\n",
    "    \"\"\"\n",
    "    # Histogram equalization\n",
    "    he1 = exposure.equalize_hist(raw_he)\n",
    "    he1 = detect.remove_background(he1)\n",
    "    \n",
    "    # Shift nonzero values into positive range and equalize histogram\n",
    "    he2 = np.where(raw_he == 0, 0, raw_he + np.abs(np.min(raw_he)))\n",
    "    he3 = exposure.equalize_hist(he2)\n",
    "    \n",
    "    he3 = np.where(he3 == np.min(he3), np.NaN, he3)\n",
    "    \n",
    "    return he3, he2, he1\n",
    "\n",
    "\n",
    "def rescale(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by applying linear rescaling\n",
    "    to normalize the contrast and setting background to NaN. Linear\n",
    "    rescaling between 2-98 percentiles produces a less harsh contrast\n",
    "    enhancement than histogram equalization.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(raw_he[~np.isnan(raw_he)], (2, 98))\n",
    "    \n",
    "    # Shift nonzero values into positive range and normalize\n",
    "    he1 = np.where(raw_he == 0, 0, raw_he + np.abs(np.min(raw_he)))\n",
    "    he2 = exposure.rescale_intensity(he1, in_range=(p2, p98))\n",
    "    \n",
    "    # Normalize directly\n",
    "    he3 = exposure.rescale_intensity(raw_he, in_range=(p2, p98))\n",
    "    he3 = detect.remove_background(he3)\n",
    "        \n",
    "    return he3, he2, he1\n",
    "\n",
    "\n",
    "def rescale_center(raw_he):\n",
    "    \"\"\"Pre-process equivalent width array by applying linear rescaling\n",
    "    to normalize the contrast, set background to NaN, and centering mode\n",
    "    to zero.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(raw_he, (2, 98))\n",
    "    \n",
    "    # Linearly rescale\n",
    "    he1 = exposure.rescale_intensity(raw_he, in_range=(p2, p98))    \n",
    "    he2 = detect.remove_background(he1)\n",
    "    \n",
    "    # Center mode to zero\n",
    "    peak_counts_val = detect.get_peak_counts_loc(he2, bins_as_percent=False)\n",
    "    he3 = he2 - peak_counts_val + 1\n",
    "\n",
    "    return he3, he2, he1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Band Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_band_cut, he_high_cut, he_nan = band_pass(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he_nan, he_high_cut, he_band_cut]\n",
    "titles = ['he', 'he NaN', 'he High Cut', 'he Band Cut']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = equalize(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he1, he2, he3]\n",
    "titles = ['he', 'Equalized', 'Shifted', 'Shifted & Equalized']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = detect.get_hist(he3, bins_as_percent=True, n=1000)\n",
    "plt.semilogy(edges[0:-1], hist)\n",
    "detect.get_peak_counts_loc(he3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "Rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = rescale(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he3, he1, he2]\n",
    "titles = ['he', 'Stretched', 'Shifted', 'Shifted & Stretched']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "he3, he2, he1 = rescale_center(he_map_data)\n",
    "\n",
    "arrays = [he_map_data, he1, he2, he3]\n",
    "titles = ['he', 'Stretched', 'Shifted', 'Removed Background']\n",
    "\n",
    "plot_detection.plot_hists(arrays[0:2], titles[0:2], semilogy=True)\n",
    "plot_detection.plot_hists(arrays[2:4], titles[2:4], semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "raw_ratio = prepare_data.get_image_from_fits(ratio_fits_file)\n",
    "ratio = detect.pre_process_v0_4(raw_ratio)\n",
    "\n",
    "arrays = [raw_ratio, ratio]\n",
    "titles = ['Raw Ratio', 'Pre-Processed Ratio']\n",
    "\n",
    "plot_detection.plot_hists(arrays, titles, semilogy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-Limb Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hp_coords = all_coordinates_from_map(he_map)\n",
    "mask = coordinate_is_on_solar_disk(all_hp_coords)\n",
    "limb_removed_he_map = sunpy.map.Map(\n",
    "    np.where(mask, he_map.data, np.nan), he_map.meta\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=limb_removed_he_map)\n",
    "limb_removed_he_map.plot(axes=ax, title='', vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hp_coords = all_coordinates_from_map(mag_map)\n",
    "mask = coordinate_is_on_solar_disk(all_hp_coords)\n",
    "limb_removed_mag_map = sunpy.map.Map(mag_map.data, mag_map.meta, mask=~mask)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=limb_removed_mag_map)\n",
    "limb_removed_mag_map.plot(axes=ax, title='', vmin=-50, vmax=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Heliographic Reprojection Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "smooth_size_percent = 10\n",
    "\n",
    "if not os.path.isdir(HELIOGRAPH_MAG_SAVE_DIR):\n",
    "    os.makedirs(HELIOGRAPH_MAG_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    fits_file_name = f'{HELIOGRAPH_MAG_SAVE_DIR}Mag{mag_date_str}_He{he_date_str}'\n",
    "    reprojected_fits_file = f'{fits_file_name}.fits'\n",
    "    reprojected_smooth_fits_file = f'{fits_file_name}_smooth.fits'\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    if (os.path.isfile(reprojected_fits_file) or \\\n",
    "        os.path.isfile(reprojected_smooth_fits_file)) and not overwrite:\n",
    "        print((f'{mag_date_str} magnetogram reprojected '\n",
    "                + f'to {he_date_str} already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract observations\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    pre_processed_map = sunpy.map.Map(pre_process_file)\n",
    "    \n",
    "    mag_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=MAG_DIR, date_str=mag_date_str\n",
    "    )\n",
    "    mag_map = prepare_data.get_nso_sunpy_map(mag_fits_file)\n",
    "\n",
    "    # Process magnetogram\n",
    "    hg_mag_map = detect.reproject_to_cea(mag_map)\n",
    "    reprojected_mag_map = hg_mag_map.reproject_to(\n",
    "        pre_processed_map.wcs, algorithm='adaptive'\n",
    "    )\n",
    "    \n",
    "    smoothed_map = prepare_data.get_smoothed_map(mag_map, smooth_size_percent)\n",
    "    hg_smoothed_map = detect.reproject_to_cea(smoothed_map)\n",
    "    reprojected_smooth_map = hg_smoothed_map.reproject_to(\n",
    "        pre_processed_map.wcs, algorithm='adaptive'\n",
    "    )\n",
    "    \n",
    "    # Save to FITS files\n",
    "    reprojected_mag_map.save(reprojected_fits_file, overwrite=overwrite)\n",
    "    reprojected_smooth_map.save(reprojected_smooth_fits_file, overwrite=overwrite)\n",
    "    print(f'{mag_date_str} magnetogram reprojected to {he_date_str} map saved.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_date_str = HE_DATE_LIST[3]\n",
    "# mag_date_str = MAG_DATE_LIST[4]\n",
    "\n",
    "he_fits_file = DATA_FITS_FORMAT.format(\n",
    "   data_dir=HE_DIR, date_str=he_date_str\n",
    ")\n",
    "mag_fits_file = DATA_FITS_FORMAT.format(\n",
    "    data_dir=MAG_DIR, date_str=mag_date_str\n",
    ")\n",
    "he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "mag_map = prepare_data.get_nso_sunpy_map(mag_fits_file)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=mag_map)\n",
    "mag_map.plot(axes=ax1, vmin=-50, vmax=50,\n",
    "               title=f'Original: {mag_map.date}')\n",
    "\n",
    "smoothed_map = prepare_data.get_smoothed_map(mag_map, smooth_size_percent=10)\n",
    "plot_detection.plot_map_contours(ax1, smoothed_map)\n",
    "\n",
    "reprojected_map = prepare_data.diff_rotate(\n",
    "   input_map=mag_map, target_map=he_map\n",
    ")\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection=reprojected_map)\n",
    "reprojected_map.plot(axes=ax2, vmin=-50, vmax=50,\n",
    "                     title=f'Reprojection: {reprojected_map.date}')\n",
    "\n",
    "reprojected_smooth_map = prepare_data.diff_rotate(\n",
    "   input_map=smoothed_map, target_map=he_map\n",
    ")\n",
    "plot_detection.plot_map_contours(ax2, reprojected_smooth_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helioprojective Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec)/pix > (HP Tx, Ty arcsec)\n",
    "Tx_scale = he_map.scale.axis1.to(u.arcsec/u.pix) * u.pix\n",
    "Ty_scale = he_map.scale.axis2.to(u.arcsec/u.pix) * u.pix\n",
    "\n",
    "f'Tx: {Tx_scale.value:.5f} Ty: {Ty_scale.value:.5f} arcsec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    he_map.scale.axis1*u.pix,\n",
    "    he_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=he_map.date)\n",
    ")\n",
    "f'x: {hc_delta_coords.x.to(u.Mm).value:.5f} y: {hc_delta_coords.x.to(u.Mm).value:.5f} Mm'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrington Non-CEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection map shape scaling factors\n",
    "# Increase to increase map resolution and reduce distance scale / pixel\n",
    "# Aim to match Helioprojective scale to preserve resolution\n",
    "NON_CEA_LON_FACTOR = 1.55\n",
    "NON_CEA_LAT_FACTOR = 1.55\n",
    "\n",
    "# Obtain dimension  in image pixel number of the solar radius\n",
    "Rs_hp_coord = SkyCoord(\n",
    "    he_map.rsun_obs, 0*u.arcsec, frame='helioprojective',\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "Rs_pixel_pair = he_map.world_to_pixel(Rs_hp_coord)\n",
    "ref_pixel_pair = he_map.world_to_pixel(he_map.reference_coordinate)\n",
    "Rs_dim = int((Rs_pixel_pair.x - ref_pixel_pair.x).value)\n",
    "\n",
    "new_rows = int(2*Rs_dim*NON_CEA_LAT_FACTOR)\n",
    "new_cols = int(4*Rs_dim*NON_CEA_LON_FACTOR)\n",
    "\n",
    "hg_header = sunpy.map.header_helper.make_heliographic_header(\n",
    "    he_map.date, he_map.observer_coordinate,\n",
    "    shape=(new_rows, new_cols), frame='stonyhurst'\n",
    ")\n",
    "\n",
    "# Convert to 180 deg center longitude for Carrington map visualization\n",
    "hg_header['crval1'] = 180\n",
    "non_cea_map = he_map.reproject_to(\n",
    "    hg_header#, algorithm='adaptive'\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1, projection=he_map)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100, title=he_map.date)\n",
    "\n",
    "ax = fig.add_subplot(1, 3, (2,3), projection=non_cea_map)\n",
    "non_cea_map.plot(axes=ax, vmin=-100, vmax=100, title='')\n",
    "\n",
    "(new_rows, new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-CEA Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg)/pix > (HG lon, lat deg)\n",
    "lon_scale = non_cea_map.scale.axis1.to(u.deg/u.pix) * u.pix\n",
    "lat_scale = non_cea_map.scale.axis2.to(u.deg/u.pix) * u.pix\n",
    "\n",
    "f'lon: {lon_scale.value:.5f} lat: {lat_scale.value:.5f} deg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg) > (HC Mm)\n",
    "x_scale = (non_cea_map.rsun_meters * lon_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "y_scale = (non_cea_map.rsun_meters * lat_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG\n",
    "# (HG lon, lat deg) > (HC Mm)\n",
    "hg_delta_coords = frames.HeliographicStonyhurst(lon_scale, lat_scale)\n",
    "hc_delta_coords = hg_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=non_cea_map.date)\n",
    ")\n",
    "\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'\n",
    "\n",
    "# WRONG\n",
    "# (HP??? lon, lat deg) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    lon_scale, lat_scale,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=non_cea_map.date)\n",
    ")\n",
    "\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stonyhurst CEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection map shape scaling factors\n",
    "# Increase to increase map resolution and reduce distance scale / pixel\n",
    "# Aim to match Helioprojective scale within 0.01 tolerance to preserve resolution\n",
    "CEA_X_SCALE_FACTOR = np.pi/2\n",
    "CEA_Y_SCALE_FACTOR = 1\n",
    "\n",
    "# Obtain dimension  in image pixel number of the solar radius\n",
    "Rs_hp_coord = SkyCoord(\n",
    "    he_map.rsun_obs, 0*u.arcsec, frame='helioprojective',\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "Rs_pixel_pair = he_map.world_to_pixel(Rs_hp_coord)\n",
    "ref_pixel_pair = he_map.world_to_pixel(he_map.reference_coordinate)\n",
    "Rs_dim = int((Rs_pixel_pair.x - ref_pixel_pair.x).value)\n",
    "\n",
    "new_row_num = int(2*Rs_dim*CEA_Y_SCALE_FACTOR)\n",
    "new_col_num = int(4*Rs_dim*CEA_X_SCALE_FACTOR)\n",
    "\n",
    "hg_header = sunpy.map.header_helper.make_heliographic_header(\n",
    "    he_map.date,\n",
    "    he_map.observer_coordinate,\n",
    "    # observer,\n",
    "    shape=(new_row_num, new_col_num), frame='stonyhurst',\n",
    "    projection_code='CEA'\n",
    ")\n",
    "\n",
    "# Specify Earth-based observer for solar radius, distance to Sun,\n",
    "# and Heliographic coordinates to avoid warning messages due to\n",
    "# missing keywords\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    Tx=0*u.arcsec, Ty=0*u.arcsec,\n",
    "    observer='earth', obstime=he_map.date,\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header((1,1), earth_hp_coords)\n",
    "for earth_coord_key in ['RSUN_REF', 'DSUN_OBS', 'HGLN_OBS', 'HGLT_OBS']:\n",
    "    hg_header[earth_coord_key] = earth_header[earth_coord_key]\n",
    "\n",
    "cea_he_map = he_map.reproject_to(\n",
    "    hg_header, #algorithm='adaptive'\n",
    ")\n",
    "\n",
    "# Crop map to within 90 degrees of the central meridian\n",
    "top_right = SkyCoord(\n",
    "    lon=90*u.deg, lat=90*u.deg, frame=cea_he_map.coordinate_frame\n",
    ")\n",
    "bottom_left = SkyCoord(\n",
    "    lon=-90*u.deg, lat=-90*u.deg, frame=cea_he_map.coordinate_frame\n",
    ")\n",
    "cea_he_map = cea_he_map.submap(bottom_left, top_right=top_right)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111, projection=cea_he_map)\n",
    "cea_he_map.plot(axes=ax, vmin=-100, vmax=100)\n",
    "\n",
    "(new_row_num, new_col_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec)/pix > (HP Tx, Ty arcsec)\n",
    "Tx_scale = he_map.scale.axis1.to(u.arcsec/u.pix) * u.pix\n",
    "Ty_scale = he_map.scale.axis2.to(u.arcsec/u.pix) * u.pix\n",
    "\n",
    "f'Tx: {Tx_scale.value:.5f} Ty: {Ty_scale.value:.5f} arcsec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HP Tx, Ty arcsec) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    he_map.scale.axis1*u.pix,\n",
    "    he_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=he_map.date)\n",
    ")\n",
    "f'x: {hc_delta_coords.x.to(u.Mm).value:.5f} y: {hc_delta_coords.x.to(u.Mm).value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_mag_map = detect.reproject_to_cea(mag_map)\n",
    "\n",
    "reprojected_mag_map = cea_mag_map.reproject_to(\n",
    "    cea_he_map.wcs, #algorithm='adaptive'\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEA Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Available: (HG lon, lat deg)/pix > (HG lon, lat deg)\n",
    "lon_scale = cea_he_map.scale.axis1.to(u.deg/u.pix) * u.pix\n",
    "lat_scale = cea_he_map.scale.axis2.to(u.deg/u.pix) * u.pix\n",
    "\n",
    "f'lon: {lon_scale.value:.5f} lat: {lat_scale.value:.5f} deg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HG lon, lat deg) > (HC Mm)\n",
    "x_scale = (cea_he_map.rsun_meters * lon_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "y_scale = (cea_he_map.rsun_meters * lat_scale.to(u.rad)/u.rad).to(u.Mm)\n",
    "\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG\n",
    "# (HG lon, lat deg) > (HC Mm)\n",
    "hg_delta_coords = frames.HeliographicStonyhurst(\n",
    "    pre_processed_map.scale.axis1*u.pix,\n",
    "    pre_processed_map.scale.axis2*u.pix,\n",
    ")\n",
    "hc_delta_coords = hg_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'\n",
    "\n",
    "# WRONG\n",
    "# (HP??? lon, lat deg) > (HC Mm)\n",
    "hp_delta_coords = frames.Helioprojective(\n",
    "    pre_processed_map.scale.axis1*u.pix,\n",
    "    pre_processed_map.scale.axis2*u.pix,\n",
    "    observer='earth', obstime=he_map.date\n",
    ")\n",
    "hc_delta_coords = hp_delta_coords.transform_to(\n",
    "    frames.Heliocentric(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "x_scale = hc_delta_coords.x.to(u.Mm)\n",
    "y_scale = hc_delta_coords.y.to(u.Mm)\n",
    "f'x: {x_scale.value:.5f} y: {y_scale.value:.5f} Mm'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "## Preliminary Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '#6ece58'\n",
    "BLUE = '#3e4989'\n",
    "ORANGE = '#fd9668'\n",
    "PURPLE = '#721f81'\n",
    "\n",
    "\n",
    "def get_thresh_px_percent_list(array, percent_of_peak_list):\n",
    "    \"\"\"Retrieve the area percentage of pixels accepted by varied thresholds.\n",
    "    \"\"\"\n",
    "    thresh_bound_list = [\n",
    "        detect.get_thresh_bound(array, percent_of_peak)\n",
    "        for percent_of_peak in percent_of_peak_list\n",
    "    ]\n",
    "    px_percent_list = [\n",
    "        np.count_nonzero(array > thresh_bound)*100/array.size\n",
    "        for thresh_bound in thresh_bound_list\n",
    "    ]\n",
    "    return px_percent_list\n",
    "\n",
    "\n",
    "def get_parameter_stats(outcome_list):\n",
    "    \"\"\"Retrieve maximum difference between segmentations in area percentage\n",
    "    detected, the average area percentage at the max difference for a cutoff,\n",
    "    the number selected below this cutoff, and differences in area percentage.\n",
    "    \"\"\"    \n",
    "    outcome_diffs = np.abs(np.diff(outcome_list))\n",
    "\n",
    "    max_diff_i = np.argmax(outcome_diffs)\n",
    "    max_diff = np.max(outcome_diffs)*100/outcome_list[max_diff_i]\n",
    "    \n",
    "    cutoff = np.mean([outcome_list[max_diff_i], \n",
    "                      outcome_list[max_diff_i + 1]])\n",
    "\n",
    "    selected_parameter_num = np.count_nonzero(outcome_list > cutoff)\n",
    "    \n",
    "    return max_diff, cutoff, selected_parameter_num, outcome_diffs\n",
    "\n",
    "\n",
    "def plot_pixel_percent_bars(ax, parameter_list, pixel_percent_list,\n",
    "                            max_diff, cutoff, selected_parameter_num,\n",
    "                            step, title, unit, xlabel, thresh=True):\n",
    "    bar_width = 0.8*step\n",
    "    selected_parameters = parameter_list[selected_parameter_num:]\n",
    "    \n",
    "    ax.set_title(f'{title}\\n Cutoff: {selected_parameters[0]}{unit} | ' +\n",
    "                 f'Max Difference: {max_diff:.1f}%' , fontsize=28)\n",
    "    ax.set_xlabel(xlabel, fontsize=24)\n",
    "    \n",
    "    ax.set_ylabel('Pixel Percentage (%)', fontsize=24)\n",
    "    \n",
    "    ax.plot([parameter_list[0] - step/2, parameter_list[-1] + step/2], [cutoff, cutoff], \n",
    "               linestyle='--', color='k', linewidth=3)\n",
    "    \n",
    "    if thresh:\n",
    "        ax.bar(parameter_list, pixel_percent_list, \n",
    "               width=bar_width, color=BLUE)\n",
    "        ax.bar(selected_parameters, \n",
    "               pixel_percent_list[selected_parameter_num:], \n",
    "               width=bar_width, color=GREEN)\n",
    "    else:\n",
    "        ax.bar(parameter_list, pixel_percent_list,\n",
    "               width=bar_width, color=PURPLE)\n",
    "        ax.bar(selected_parameters,\n",
    "               pixel_percent_list[selected_parameter_num:], \n",
    "               width=bar_width, color=ORANGE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    np.flipud(pre_processed_v0_5_1_map.data), bounds=[75, 90, 105],\n",
    "    bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    pre_process_v0_4_he, bounds=[75, 90, 105], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    pre_process_v0_1_he, bounds=[75, 90, 105], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dates\n",
    "date_idx = 0\n",
    "for he_date_str in HE_DATE_LIST[date_idx:date_idx + 3]:\n",
    "    compare_he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(compare_he_fits_file)\n",
    "    he = detect.pre_process_v0_1(raw_he)[0]\n",
    "\n",
    "    plot_detection.plot_thresholds(he, bounds=[75, 85, 100], bounds_as_percent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweep\n",
    "step = 5\n",
    "percent_of_peak_lists = [\n",
    "    list(np.arange(0,200,step)), list(np.arange(80,130,step))\n",
    "]\n",
    "\n",
    "for percent_of_peak_list in percent_of_peak_lists:\n",
    "    px_percent_list = get_thresh_px_percent_list(pre_process_v0_1_he, percent_of_peak_list)\n",
    "    \n",
    "    parameter_stats = get_parameter_stats(px_percent_list)\n",
    "    max_diff, cutoff, selected_parameter_num, pixel_percent_diffs = parameter_stats\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    plot_pixel_percent_bars(\n",
    "        ax, percent_of_peak_list, px_percent_list, max_diff, cutoff, selected_parameter_num,\n",
    "        step, title='Threshold', unit='%', xlabel='Percent of Peak Pixel Count (%)', thresh=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY Pre-Process: Pre-Processed Map Reprojected to Heliographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "plot_detection.plot_thresholds(\n",
    "    np.flipud(pre_processed_vY_map.data), bounds=[75, 90, 105],\n",
    "    bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "### Structuring Element Radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_varied_morph_radius(pre_processed_map_data, percent_of_peak_list,\n",
    "                             morph_radius_list, ch_mask_list, px=False):\n",
    "    plot_detection.plot_images([pre_processed_map_data], image_size=4)\n",
    "\n",
    "    image_list = [pre_processed_map_data for _ in range(len(ch_mask_list))]\n",
    "    axes = plot_detection.plot_image_grid(\n",
    "        image_list, num_cols=3, cmap='afmhot', image_size=7\n",
    "    )\n",
    "    zipped_items = zip(axes.values(), percent_of_peak_list,\n",
    "                       morph_radius_list, ch_mask_list)\n",
    "\n",
    "    for ax, percent_of_peak, radius, ch_mask in zipped_items:\n",
    "        if px:\n",
    "            ax.set_title(f'{percent_of_peak:d}% of Peak | {radius:d}px Radius')\n",
    "        else:\n",
    "            ax.set_title((f'{percent_of_peak:d}% of Mode Threshold | '\n",
    "                          f'{radius:d}Mm SE Disk Radius'))\n",
    "        \n",
    "        ax.tick_params(left=False, right=False, labelleft=False,\n",
    "                       labelbottom=False, bottom=False)\n",
    "            \n",
    "        ax.contour(ch_mask, cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [8, 12, 16]\n",
    "percent_of_peak_list = [80 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "\n",
    "pre_processed_v0_5_1_map = detect.pre_process_v0_5_1(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_v0_5_1_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_v0_5_1_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.4 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [12,16,20]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "pre_process_v0_4_he = detect.pre_process_v0_4(he_map_data)\n",
    "\n",
    "ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_process_v0_4_he, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "plot_varied_morph_radius(pre_process_v0_4_he, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, px=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1 Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_radius_list = [12,16,20]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "plot_varied_morph_radius(pre_process_v0_1_he, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, px=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweep\n",
    "step = 2\n",
    "morph_radius_lists = [\n",
    "    list(np.arange(1,21,step)), list(np.arange(8,15,step))\n",
    "]\n",
    "\n",
    "for morph_radius_list in morph_radius_lists:\n",
    "    percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "    ch_mask_list = [\n",
    "        detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    px_percent_list = detect.get_px_percent_list(ch_mask_list)\n",
    "    \n",
    "    parameter_stats = get_parameter_stats(px_percent_list)\n",
    "    max_diff, cutoff, selected_parameter_num, pixel_percent_diffs = parameter_stats\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    plot_pixel_percent_bars(\n",
    "        ax, morph_radius_list, px_percent_list, max_diff, cutoff, selected_parameter_num,\n",
    "        step, title='SE Disk Radius', unit='px', xlabel='SE Disk Radius (px)', thresh=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY Pre-Process: Pre-Processed Map Reprojected to Heliographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_date_str = HE_DATE_LIST[1]\n",
    "morph_radius_list = [8, 12, 16]\n",
    "percent_of_peak_list = [90 for _ in range(len(morph_radius_list))]\n",
    "\n",
    "\n",
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_vY(\n",
    "    pre_processed_vY_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_vY_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial distance definition of SE disk radius ideas\n",
    "\n",
    "# Area square: (HP Tx, Ty arcsec) > (HC Mm)\n",
    "# Get HG coords: (pixel) > (HP Tx, Ty arcsec) > (HG lon, lat deg)\n",
    "# Reproject CEA: (HP Tx, Ty arcsec) > (pixel)\n",
    "\n",
    "# SE Disk Radius: (HC Mm) > (pixel)\n",
    "# HC to HP/HG to pixel\n",
    "# To pixel step fails\n",
    "empty_dim_list = [0*u.km for _ in morph_radius_list]\n",
    "morph_radius_hc_coords = SkyCoord(\n",
    "    x=[morph_radius_dist*u.Mm for morph_radius_dist in morph_radius_list],\n",
    "    y=empty_dim_list, z=empty_dim_list, frame='heliocentric',\n",
    "    observer='earth', obstime=pre_processed_map.date\n",
    ")\n",
    "morph_radius_hp_coords = morph_radius_hc_coords.transform_to(\n",
    "    frames.Helioprojective(observer='earth', obstime=pre_processed_map.date)\n",
    ")\n",
    "morph_radius_pixel_coord = pre_processed_map.world_to_pixel(\n",
    "    morph_radius_hc_coords\n",
    ").x\n",
    "ref_pixel_coord = pre_processed_map.world_to_pixel(\n",
    "    pre_processed_map.reference_coordinate\n",
    ").x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Single Mask Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "percent_of_peak = 80\n",
    "morph_radius = 18\n",
    "\n",
    "\n",
    "if not os.path.isdir(DETECTION_NPY_SAVE_DIR):\n",
    "    os.makedirs(DETECTION_NPY_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    mask_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    if os.path.isfile(mask_file) and not overwrite:\n",
    "        print((f'He {he_date_str} single mask already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "\n",
    "    ch_mask = detect.get_ch_mask(\n",
    "        pre_processed_map, percent_of_peak, morph_radius\n",
    "    )\n",
    "    \n",
    "    save_list = [he_date_str, ch_mask]\n",
    "    np.save(mask_file, np.array(save_list, dtype=object), allow_pickle=True)\n",
    "    print(f'{he_date_str} Single Mask Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Variable Grid\n",
    "\n",
    "Includes fill and remove steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_design_var_grid(pre_processed_map_data, percent_of_peak_list,\n",
    "                         morph_radius_list, ch_mask_list, num_cols):\n",
    "    image_list = [pre_processed_map_data for _ in range(len(ch_mask_list))]\n",
    "    axes = plot_detection.plot_image_grid(\n",
    "        image_list, num_cols, cmap='afmhot', image_size=7\n",
    "    )\n",
    "    zipped_items = zip(axes.values(), percent_of_peak_list,\n",
    "                    morph_radius_list, ch_mask_list)\n",
    "\n",
    "    for ax, percent_of_peak, radius, ch_mask in zipped_items:\n",
    "        ax.set_title(f'{percent_of_peak:d}% of Peak | {radius:d}Mm Radius')\n",
    "        \n",
    "        ax.tick_params(left=False, right=False, labelleft=False,\n",
    "                        labelbottom=False, bottom=False)\n",
    "            \n",
    "        ax.contour(ch_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_varied_masks(pre_processed_map_data, percent_of_peak_list,\n",
    "                      morph_radius_list, ch_mask_list, title=True):\n",
    "    \"\"\"UNUSED: Plot each mask as an individual image with not axis ticks\"\"\"\n",
    "    plot_detection.plot_images([pre_processed_map_data], image_size=4)\n",
    "    \n",
    "    zipped_items = zip(percent_of_peak_list,\n",
    "                       morph_radius_list, ch_mask_list)\n",
    "    \n",
    "    for percent_of_peak, radius, ch_mask in zipped_items:\n",
    "        \n",
    "        empty_disk = np.where(~np.isnan(pre_processed_map_data), 0, np.nan)\n",
    "        ch_disk = np.where(ch_mask, 1, empty_disk)\n",
    "        \n",
    "        axes = plot_detection.plot_image_grid(\n",
    "            [ch_disk], num_cols=3, cmap='magma', image_size=7\n",
    "        )\n",
    "        \n",
    "        if title:\n",
    "            axes[0].set_title((f'{percent_of_peak:d}% of Mode Threshold | '\n",
    "                               f'{radius:d}Mm SE Disk Radius'))\n",
    "        \n",
    "        axes[0].tick_params(left=False, right=False, labelleft=False,\n",
    "                       labelbottom=False, bottom=False)\n",
    "\n",
    "percent_of_peak_list = [90, 70]\n",
    "morph_radius_list = [13, 15]\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_masks(\n",
    "    np.flipud(pre_processed_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list, title=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v0.5.1 Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.5.1 SOLIS Design\n",
    "percent_of_peak_list = [70, 70, 80, 90]\n",
    "morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # v0.5.1 KPVT Design\n",
    "# percent_of_peak_list = [85, 105, 85, 95]\n",
    "# morph_radius_list = [   17, 13, 15, 13] # Mm\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "plot_design_var_grid(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list,\n",
    "    ch_mask_list, num_cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Breaks up QS merger for COSPAR triangular CH\n",
    "# percent_of_peak_list = [80, 70, 80, 90]\n",
    "# morph_radius_list = [   12, 17, 13, 13] # Mm\n",
    "\n",
    "percent_of_peaks = [80, 90, 100]\n",
    "morph_radii = [      9, 13, 17] # Mm\n",
    "percent_of_peak_list = [percent_of_peak \n",
    "                        for _ in morph_radii\n",
    "                        for percent_of_peak in percent_of_peaks]\n",
    "morph_radius_list = [morph_radius \n",
    "                     for morph_radius in reversed(morph_radii)\n",
    "                     for _ in percent_of_peaks]\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "plot_design_var_grid(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list,\n",
    "    ch_mask_list, num_cols=len(percent_of_peaks)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vY Pre-Process\n",
    "Pre-Processed Map Reprojected to Heliographic Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [90, 90]\n",
    "morph_radius_list = [10, 15]\n",
    "\n",
    "pre_processed_vY_map = detect.pre_process_vY(he_map)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_vY(\n",
    "    pre_processed_vY_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "plot_varied_morph_radius(\n",
    "    np.flipud(pre_processed_vY_map.data), percent_of_peak_list,\n",
    "    morph_radius_list, ch_mask_list\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions\n",
    "\n",
    "Appropriate pre-processed products must be extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1.1: Pre-run CH Labels functions and CH Labels > Load Saved Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDA_FILE_NAME, 'rb') as lda_file:\n",
    "    lda = pickle.load(lda_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.0 SOLIS Design\n",
    "percent_of_peak_list = [70, 70, 80, 90]\n",
    "morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # v1.0 KPVT Design\n",
    "# percent_of_peak_list = [85, 105, 85, 95]\n",
    "# morph_radius_list = [   17, 13, 15, 13] # Mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_1_CLASSIFY_FEATURES = ['unipolarity', 'grad_median', 'cm_foreshort']\n",
    "from detect import *\n",
    "\n",
    "morph_radius_dist_list = morph_radius_list\n",
    "# TODO: load saved lda\n",
    "# # def get_ensemble_v1_1(he_map_data, pre_processed_map, reprojected_mag_map,\n",
    "# #                       percent_of_peak_list, morph_radius_dist_list,\n",
    "# #                       probability_threshold):\n",
    "# #     \"\"\"Retrieve an ensemble of segmentations sorted by CH unipolarity.\n",
    "    \n",
    "# #     Args\n",
    "#         # he_map_data: Numpy array of He I observation\n",
    "#         # pre_processed_map: Sunpy map object to segment\n",
    "#         # reprojected_mag_map: Sunpy map object of magnetogram reprojected\n",
    "# #             to align with the ensemble map\n",
    "# #         percent_of_peak_list: list of float percentage measured from the zero\n",
    "# #             value up to or beyond the histogram value\n",
    "# #         morph_radius_dist_list: list of float distances in Mm for radius of\n",
    "# #             disk structuring element in morphological operations\n",
    "# #         probability_threshold: float probability in [0,1) at which to\n",
    "# #             threshold candidate CHs\n",
    "# #     Returns\n",
    "# #         Ensemble greyscale coronal holes mask sorted by unipolarity.\n",
    "# #         List of coronal holes masks.\n",
    "# #         List of confidence levels in mask layers.\n",
    "# #         Confidence assignment metric list of unipolarity.\n",
    "# #     \"\"\"\n",
    "\n",
    "# Create segmentation masks across the full solar disk of candidate\n",
    "# regions for varied design variable combinations\n",
    "full_disk_cand_mask_list = get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_dist_list\n",
    ")\n",
    "\n",
    "# List to be extended by masks for distinct CHs from all segmentations\n",
    "cand_masks = []\n",
    "ones_array = np.ones_like(he_map_data)\n",
    "\n",
    "for full_disk_cand_mask in full_disk_cand_mask_list:\n",
    "    cand_masks_in_full_disk_mask = get_map_data_by_ch(\n",
    "        ones_array, full_disk_cand_mask\n",
    "    )\n",
    "    cand_masks.extend(cand_masks_in_full_disk_mask)\n",
    "\n",
    "num_cand = len(cand_masks)\n",
    "\n",
    "# Compute constant area per square pixel once for all CHs\n",
    "A_per_square_px = get_A_per_square_px(pre_processed_map)\n",
    "\n",
    "# Array to hold candidate feature values\n",
    "cand_feature_array = np.zeros((num_cand, len(V1_1_CLASSIFY_FEATURES)))\n",
    "\n",
    "# TODO: Takes 10s, speed-up?\n",
    "for cand_idx in range(num_cand):\n",
    "    distinct_cand_mask = cand_masks[cand_idx]\n",
    "    distinct_cand_map = sunpy.map.Map(\n",
    "        distinct_cand_mask, # Not flipping works right\n",
    "        pre_processed_map.meta\n",
    "    )\n",
    "    outcome_dict = get_outcomes(\n",
    "        distinct_cand_map, reprojected_mag_map, A_per_square_px,\n",
    "        he_map_data=he_map_data\n",
    "    )\n",
    "    cand_feature_values = [\n",
    "        outcome_dict[feature] for feature in V1_1_CLASSIFY_FEATURES\n",
    "    ]\n",
    "    cand_feature_array[cand_idx, :] = cand_feature_values\n",
    "\n",
    "cand_probabilities = lda.predict_proba(cand_feature_array)[:,1]\n",
    "\n",
    "# Sort unipolarities from greatest to least\n",
    "sorted_idxs = np.argsort(cand_probabilities)\n",
    "\n",
    "# Sort candidate regions from greatest to least predicted probability\n",
    "cand_masks = [cand_masks[i] for i in sorted_idxs]\n",
    "cand_probabilities = [\n",
    "    cand_probabilities[i] for i in sorted_idxs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0 # All false alarms\n",
    "# probability_threshold = 0.04 # No misses\n",
    "# probability_threshold = 0.4 # Balance hits vs misses\n",
    "\n",
    "\n",
    "# Assign confidence by probability above a threshold\n",
    "confidence_levels = np.array(\n",
    "    [(probability - probability_threshold)/(1 - probability_threshold)\n",
    "    for probability in cand_probabilities]\n",
    ")\n",
    "confidence_levels = np.where(\n",
    "    confidence_levels > 0, confidence_levels, 0\n",
    ")\n",
    "\n",
    "# Construct ensemble map by adding distinct CHs with assigned\n",
    "# confidence level values to an empty base disk\n",
    "ensemble_map_data = np.where(\n",
    "    ~np.isnan(np.flipud(pre_processed_map.data)), 0, np.nan\n",
    ")\n",
    "for distinct_cand, confidence in zip(cand_masks, confidence_levels):\n",
    "    ensemble_map_data = np.where(\n",
    "        ~np.isnan(distinct_cand), confidence, ensemble_map_data\n",
    "    )\n",
    "# return ensemble_map_data, cand_masks, confidence_levels, cand_feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUV comparison\n",
    "nrows = 1\n",
    "ensemble_map = sunpy.map.Map(\n",
    "    np.flipud(ensemble_map_data), pre_processed_map.meta\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(21, 5))\n",
    "\n",
    "# Plot He observation\n",
    "ax1_gridspec = (nrows, 3, 1)\n",
    "ax2_gridspec = (nrows, 3, 2)\n",
    "ax3_gridspec = (nrows, 3, 3)\n",
    "\n",
    "plot_detection.plot_he_map(fig, ax1_gridspec, he_map, he_date_str)\n",
    "\n",
    "# Plot ensemble map with overlayed neutral lines\n",
    "ax = plot_detection.plot_ensemble_map_v1_0(fig, ax2_gridspec, ensemble_map)\n",
    "plot_detection.plot_map_contours(ax, reprojected_smooth_map)\n",
    "ax.set_title(f'Probability Threshold {probability_threshold:.2f}')\n",
    "\n",
    "plot_detection.plot_euv_map(fig, ax3_gridspec, euv_map, euv_date_str)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_list = list(cand_feature_array[:,0])\n",
    "metric_list = cand_probabilities\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, cand_masks,\n",
    "    confidence_levels, metric_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative Design\n",
    "# percent_of_peak_list = [80, 80, 90, 100]\n",
    "# morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# Aggressive Design\n",
    "percent_of_peak_list = [70, 70, 80, 90]\n",
    "morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_5_1(\n",
    "    pre_processed_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, masks_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5: Directly Assigned Unipolarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [   18,20, 16, 16, 20] # px\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_5(\n",
    "    pre_processed_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list, unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, map_data_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, map_data_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.3: Evenly Assigned Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [   13,17, 15, 13, 17] # px\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_3(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2: Detected Pixel Percentage Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80,90,100,100]\n",
    "morph_radius_list = [13,17,15,13,17] # px\n",
    "\n",
    "\n",
    "out = detect.get_ensemble_v0_2(\n",
    "    pre_processed_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map_data, ch_mask_list, confidence_list, px_percent_list = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, ch_mask_list,\n",
    "    confidence_list, px_percent_list, mask_contour=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [85, 73, 95, 85]\n",
    "morph_radius_list = [   10, 14, 10, 14]\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "out = detect.get_ensemble_vY(\n",
    "    pre_processed_map, hg_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    unipolarity_threshold\n",
    ")\n",
    "ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_processed_map_data, ensemble_map_data, masks_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "# v0.5.1 SOLIS Design\n",
    "percent_of_peak_list = [70, 70, 80, 90]\n",
    "morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "# unipolarity_threshold = 0.5\n",
    "unipolarity_threshold = 0\n",
    "\n",
    "# # v0.5.1 KPVT Design\n",
    "# percent_of_peak_list = [85, 105, 85, 95]\n",
    "# morph_radius_list = [   17, 13, 15, 13] # Mm\n",
    "# unipolarity_threshold = 0\n",
    "\n",
    "\n",
    "if not os.path.isdir(DETECTION_MAP_SAVE_DIR):\n",
    "    os.makedirs(DETECTION_MAP_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "    if os.path.isfile(ensemble_file) and not overwrite:\n",
    "        print((f'He {he_date_str} ensemble map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract pre-processed map\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    pre_processed_map = sunpy.map.Map(pre_process_file)\n",
    "\n",
    "    # Extract saved processed magnetograms\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                             f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "    ensemble_map_data = detect.get_ensemble_v0_5_1(\n",
    "        pre_processed_map, reprojected_mag_map,\n",
    "        percent_of_peak_list, morph_radius_list,\n",
    "        unipolarity_threshold\n",
    "    )[0]\n",
    "    ensemble_map = sunpy.map.Map(\n",
    "        np.flipud(ensemble_map_data), pre_processed_map.meta\n",
    "    )\n",
    "    \n",
    "    ensemble_map.save(ensemble_file, overwrite=overwrite)\n",
    "    print(f'{he_date_str} Ensemble Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "if not os.path.isdir(DETECTION_NPY_SAVE_DIR):\n",
    "    os.makedirs(DETECTION_NPY_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    if os.path.isfile(ensemble_file) and not overwrite:\n",
    "        print((f'He {he_date_str} ensemble map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "\n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map_data = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    pre_processed_map = sunpy.map.Map(\n",
    "        np.flipud(pre_processed_map_data), he_map.meta\n",
    "    )\n",
    "\n",
    "    # Extract saved processed magnetograms\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                            f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "    ensemble_map_data = detect.get_ensemble_v0_5(\n",
    "        pre_processed_map, reprojected_mag_map,\n",
    "        percent_of_peak_list, morph_radius_list,\n",
    "        unipolarity_threshold\n",
    "    )[0]\n",
    "    \n",
    "    save_list = [he_date_str, ensemble_map_data]\n",
    "    np.save(ensemble_file, np.array(save_list, dtype=object), allow_pickle=True)\n",
    "    print(f'{he_date_str} Ensemble Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "\n",
    "# percent_of_peak_list = [80,80, 90, 100,100]\n",
    "# morph_radius_list = [18,20, 16, 16,20]\n",
    "percent_of_peak_list = [100,100, 110, 120,120]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "\n",
    "\n",
    "if not os.path.isdir(DETECTION_NPY_SAVE_DIR):\n",
    "    os.makedirs(DETECTION_NPY_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    if os.path.isfile(ensemble_file) and not overwrite:\n",
    "        print((f'He {he_date_str} ensemble map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "\n",
    "    # ensemble_map_data = detect.get_ensemble_v0_2(\n",
    "    #     pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    "    # )[0]\n",
    "    ensemble_map_data = detect.get_ensemble_v0_3(\n",
    "        pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    "    )[0]\n",
    "    \n",
    "    save_list = [he_date_str, ensemble_map_data]\n",
    "    np.save(ensemble_file, np.array(save_list, dtype=object), allow_pickle=True)\n",
    "    print(f'{he_date_str} Ensemble Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "# percent_of_peak_list = [  62, 68, 73, 80]\n",
    "# morph_radius_list = [11, 13,  8, 10]\n",
    "# unipolarity_threshold = 0.01\n",
    "percent_of_peak_list = [85, 73, 95, 85]\n",
    "morph_radius_list = [   10, 14, 10, 14]\n",
    "unipolarity_threshold = 0.5\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.isdir(DETECTION_MAP_SAVE_DIR):\n",
    "    os.makedirs(DETECTION_MAP_SAVE_DIR)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "    if os.path.isfile(ensemble_file) and not overwrite:\n",
    "        print((f'He {he_date_str} ensemble map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract pre-processed map\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    pre_processed_map = sunpy.map.Map(pre_process_file)\n",
    "\n",
    "    # Extract saved processed magnetograms\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    reprojected_fits_file = (f'{HELIOGRAPH_MAG_SAVE_DIR}'\n",
    "                            f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "    ensemble_map_data = detect.get_ensemble_vY(\n",
    "        pre_processed_map, reprojected_mag_map,\n",
    "        percent_of_peak_list, morph_radius_list,\n",
    "        unipolarity_threshold\n",
    "    )[0]\n",
    "    ensemble_map = sunpy.map.Map(\n",
    "        np.flipud(ensemble_map_data), pre_processed_map.meta\n",
    "    )\n",
    "    \n",
    "    ensemble_map.save(ensemble_file, overwrite=overwrite)\n",
    "    print(f'{he_date_str} Ensemble Map Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5b: Evenly Assigned Unipolarity, No threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unipol_ensemble(pre_processed_map, reprojected_mag_map,\n",
    "                        percent_of_peak_list, morph_radius_list,\n",
    "                        even_confidence=True):\n",
    "    \"\"\"Retrieve an ensemble of segmentations sorted by CH unipolarity.\n",
    "    \n",
    "    Args\n",
    "        pre_processed_map: Sunpy map object to segment\n",
    "        reprojected_mag_map: Sunpy map object of magnetogram reprojected\n",
    "            to align with the ensemble map\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "        even_confidence: boolean to specify confidence assignment\n",
    "            True to assign confidence by even ranking in (0,100]%\n",
    "            False to assign confidence as 100% for unipolarity of 0\n",
    "                and 0% for unipolarity of 1\n",
    "    Returns\n",
    "        Ensemble greyscale coronal holes mask sorted by unipolarity.\n",
    "        List of coronal holes masks.\n",
    "        List of confidence levels in mask layers.\n",
    "    \"\"\"\n",
    "    pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "    \n",
    "    # Create global segmentations for varied design variable combinations\n",
    "    ch_masks = [\n",
    "        detect.get_ch_mask(pre_processed_map_data, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    # List to be extended by masks for distinct CHs from all segmentations\n",
    "    masks_by_ch = []\n",
    "    \n",
    "    ones_array = np.ones_like(pre_processed_map_data)\n",
    "    \n",
    "    for ch_mask in ch_masks:\n",
    "        masks_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(ones_array, ch_mask)\n",
    "        )\n",
    "    \n",
    "    num_ch = len(masks_by_ch)\n",
    "    \n",
    "    # Compute constant area per square pixel once for all CHs\n",
    "    A_per_square_px = detect.get_A_per_square_px(pre_processed_map)\n",
    "    \n",
    "    # List to hold unipolarity for distinct CHs from all segmentations\n",
    "    unipolarity_by_ch = []\n",
    "    \n",
    "    for ch_label in range(num_ch):\n",
    "        distinct_ch_mask = masks_by_ch[ch_label]\n",
    "        \n",
    "        # Not flipping works right\n",
    "        distinct_ch_map = sunpy.map.Map(\n",
    "            distinct_ch_mask, pre_processed_map.meta\n",
    "        )\n",
    "        # ax = fig.add_subplot(num_rows, num_cols, ch_label + 1, projection=pre_processed_map)\n",
    "        # distinct_ch_map.plot(cmap='magma')\n",
    "        \n",
    "        # fake_mag_map_data = np.where(~np.isnan(np.flipud(distinct_ch_mask)), 25,\n",
    "        #                              reprojected_mag_map.data)\n",
    "        # fake_mag_map = sunpy.map.Map(fake_mag_map_data, reprojected_mag_map.meta)\n",
    "        # outcomes = get_outcomes(\n",
    "        #     distinct_ch_map, fake_mag_map, A_per_square_px\n",
    "        # )\n",
    "        \n",
    "        outcome_dict = detect.get_outcomes(\n",
    "            distinct_ch_map, pre_processed_map_data, reprojected_mag_map,\n",
    "            A_per_square_px\n",
    "        )\n",
    "        unipolarity_by_ch.append(outcome_dict['unipolarity'])\n",
    "    \n",
    "    # Sort unipolarities from greatest to least\n",
    "    sorted_idxs = np.argsort(unipolarity_by_ch)\n",
    "    \n",
    "    # Sort candidate CHs from greatest to least gradient median\n",
    "    masks_by_ch = [masks_by_ch[i] for i in sorted_idxs]\n",
    "    unipolarity_by_ch = [unipolarity_by_ch[i] for i in sorted_idxs]\n",
    "    \n",
    "    # Assign confidence by direct ranking or by unipolarity value\n",
    "    if even_confidence:\n",
    "        confidence_list = [(c + 1)*100/num_ch\n",
    "                           for c in range(num_ch)]\n",
    "    else:\n",
    "        confidence_list = [100 - unipolarity*100\n",
    "                           for unipolarity in unipolarity_by_ch]\n",
    "\n",
    "    # Construct ensemble map by adding distinct CHs with assigned\n",
    "    # confidence level values to an empty base disk    \n",
    "    ensemble_map_data = np.where(\n",
    "        ~np.isnan(pre_processed_map_data), 0, np.nan\n",
    "    )\n",
    "    for distinct_ch, confidence in zip(masks_by_ch, confidence_list):\n",
    "        ensemble_map_data = np.where(\n",
    "            ~np.isnan(distinct_ch), confidence, ensemble_map_data\n",
    "        )\n",
    "    return ensemble_map_data, masks_by_ch, confidence_list, unipolarity_by_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "\n",
    "\n",
    "pre_process_v0_4_he_map_data = detect.pre_process_v0_4(he_map_data)\n",
    "pre_process_v0_4_he_map = sunpy.map.Map(\n",
    "    np.flipud(pre_process_v0_4_he), he_map.meta\n",
    ")\n",
    "\n",
    "out = get_unipol_ensemble(\n",
    "    pre_process_v0_4_he_map, reprojected_mag_map,\n",
    "    percent_of_peak_list, morph_radius_list,\n",
    "    even_confidence=True\n",
    ")\n",
    "ensemble_map_data, map_data_by_ch, confidence_list, unipolarity_by_ch = out\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_4_he_map_data, ensemble_map_data, map_data_by_ch,\n",
    "    confidence_list, unipolarity_by_ch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5c: EUV Ratio, v0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80, 90, 100,100]\n",
    "morph_radius_list = [18,20, 16, 16,20]\n",
    "\n",
    "\n",
    "ratio_fits_file = f'{RATIO_SAVE_DIR}He{he_date_str}_EUV{euv_date_str}.fits'\n",
    "raw_ratio = prepare_data.get_image_from_fits(ratio_fits_file)\n",
    "\n",
    "pre_process_v0_4_ratio_map_data = detect.pre_process_v0_4(raw_ratio)\n",
    "\n",
    "out = detect.get_ensemble_v0_3(\n",
    "    pre_process_v0_4_ratio_map_data, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_4_ratio_map_data, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "240",
   "metadata": {},
   "source": [
    "v0.3b: Percentile Assigned Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_ensemble(pre_processed_map, percent_of_peak_list,\n",
    "                        morph_radius_list, even_confidence=True):\n",
    "    \"\"\"Retrieve an ensemble of segmentations sorted by CH smoothness.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "        even_confidence: boolean to specify confidence assignment\n",
    "            True to assign confidence by even ranking in (0,100]%\n",
    "            False to assign confidence by percentile of gradient\n",
    "                median among values from other candidate CHs in [0,100]%\n",
    "    Returns\n",
    "        Ensemble greyscale coronal holes mask sorted by mean gradient.\n",
    "        List of binary coronal holes masks.\n",
    "        List of confidence levels in mask layers.\n",
    "    \"\"\"\n",
    "    # Create global segmentations for varied design variable combinations\n",
    "    ch_masks = [\n",
    "        detect.get_ch_mask(pre_processed_map, percent_of_peak, morph_radius)\n",
    "        for percent_of_peak, morph_radius\n",
    "        in zip(percent_of_peak_list, morph_radius_list)\n",
    "    ]\n",
    "    \n",
    "    # Lists to hold pre processed map and gradient data respectively\n",
    "    # for distinct CHs from all segmentations\n",
    "    map_data_by_ch = []\n",
    "    grad_data_by_ch = []\n",
    "    \n",
    "    for ch_mask in ch_masks:\n",
    "        # Masked array of candidate CHs\n",
    "        masked_candidates = detect.get_masked_candidates(pre_processed_map, ch_mask)\n",
    "        \n",
    "        # Compute spatial gradient\n",
    "        gradient_candidates = filters.sobel(masked_candidates)\n",
    "        \n",
    "        map_data_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(pre_processed_map, ch_mask)\n",
    "        )\n",
    "        grad_data_by_ch.extend(\n",
    "            detect.get_map_data_by_ch(gradient_candidates, ch_mask)\n",
    "        )\n",
    "    \n",
    "     # Obtain sorting indixes from greatest to least gradient median\n",
    "    gradient_medians = [np.median(grad_data[~np.isnan(grad_data)])\n",
    "                        for grad_data in grad_data_by_ch]\n",
    "    sorted_idxs = np.flip(np.argsort(gradient_medians))\n",
    "    \n",
    "    # Sort candidate CHs from greatest to least gradient median\n",
    "    map_data_by_ch = [map_data_by_ch[i] for i in sorted_idxs]\n",
    "    gradient_medians = [gradient_medians[i] for i in sorted_idxs]\n",
    "    \n",
    "    # Assign confidence by percentile or direct ranking\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    if even_confidence:\n",
    "        confidence_list = [(c + 1)*100/num_ch\n",
    "                           for c in range(num_ch)]\n",
    "    else:\n",
    "        percent_conversion = 100 / (np.max(gradient_medians)\n",
    "                                    - np.min(gradient_medians))\n",
    "        confidence_list = [\n",
    "            100 - (grad_median - np.min(gradient_medians)) *percent_conversion\n",
    "            for grad_median in gradient_medians\n",
    "        ]\n",
    "\n",
    "    # Construct ensemble map by adding distinct CHs with assigned\n",
    "    # confidence level values to an empty base disk\n",
    "    ensemble_map = np.where(~np.isnan(pre_processed_map), 0, np.nan)\n",
    "    \n",
    "    for distinct_ch, confidence in zip(map_data_by_ch, confidence_list):\n",
    "        ensemble_map = np.where(\n",
    "            ~np.isnan(distinct_ch), confidence, ensemble_map\n",
    "        )\n",
    "    return ensemble_map, map_data_by_ch, confidence_list, gradient_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80,80,90,100,100]\n",
    "morph_radius_list = [13,17,15,13,17]\n",
    "\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "out = get_smooth_ensemble(\n",
    "    pre_process_v0_1_he, percent_of_peak_list, morph_radius_list,\n",
    "    even_confidence=False\n",
    ")\n",
    "ensemble_map, map_data_by_ch, confidence_list, gradient_medians = out\n",
    "plot_detection.plot_ensemble(\n",
    "    pre_process_v0_1_he, ensemble_map, map_data_by_ch,\n",
    "    confidence_list, gradient_medians\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACWE Fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size for down sampling from 2048x2048 to 512x512\n",
    "ENSEMBLE_MAP_DOWNSAMPLE_STEP = 4\n",
    "\n",
    "# stride_map_data = ensemble_map_data[\n",
    "#     ::ENSEMBLE_MAP_DOWNSAMPLE_STEP, ::ENSEMBLE_MAP_DOWNSAMPLE_STEP\n",
    "# ]\n",
    "stride_map_data = np.where(np.isnan(ensemble_map_data), 0, ensemble_map_data)\n",
    "\n",
    "stride_map = sunpy.map.Map(np.flipud(stride_map_data), ensemble_map.meta)\n",
    "# resized_stride_map = sunpy.map.Map(np.flipud(stride_map_data), ensemble_map.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map alignment via differential rotation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRIDE to ACWE (4096, 4096) takes 45 sec\n",
    "rotated_stride_map = diff_rotate(ensemble_map, acwe_map)\n",
    "plt.imshow(np.flipud(rotated_stride_map.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACWE to STRIDE (2048, 2048) takes 10 sec\n",
    "rotated_acwe_map = diff_rotate(acwe_map, ensemble_map)\n",
    "plt.imshow(np.flipud(rotated_acwe_map.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align ACWE map to STRIDE map datetime via differential rotation\n",
    "rotated_acwe_map = diff_rotate(acwe_map, ensemble_map)\n",
    "rotated_acwe_map_data = np.flipud(rotated_acwe_map.data)\n",
    "\n",
    "# Fuse by taking pixel-wise maximum confidence\n",
    "fused_map_data = np.max(\n",
    "    np.stack((ensemble_map_data, rotated_acwe_map_data)), axis=0\n",
    ")\n",
    "fused_map = sunpy.map.Map(np.flipud(fused_map_data), ensemble_map.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "im = ax.imshow(fused_map_data, cmap='copper')\n",
    "# ax.set_title(f' He I: {he_date_str} \\nEUV: {acwe_date_str}')\n",
    "ax.set_title(f' He I: {he_date_str}')\n",
    "\n",
    "ax.tick_params(left=False, right=False, labelleft=False,\n",
    "               labelbottom=False, bottom=False)\n",
    "\n",
    "cb = fig.colorbar(im)\n",
    "cb.set_label('Confidence', labelpad=11, rotation=-90)\n",
    "cb.ax.tick_params(labelsize=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "253",
   "metadata": {},
   "source": [
    "# Single Date Outcomes\n",
    "\n",
    "Requires single map extraction from single map data section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties Per CH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate on CHs in Confidence Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_range = [0,1]\n",
    "# confidence_range = [0.5,0.9]\n",
    "thresh_ensemble_map_data = np.where(\n",
    "    np.all([ensemble_map.data >= confidence_range[0],\n",
    "            ensemble_map.data <= confidence_range[1]], axis=0),\n",
    "    ensemble_map.data, 0\n",
    ")\n",
    "\n",
    "# Remove leftover edges\n",
    "large_obj_mask = morphology.remove_small_objects(\n",
    "    np.where(thresh_ensemble_map_data > 0, True, False), min_size=5000\n",
    ")\n",
    "thresh_ensemble_map_data[~large_obj_mask] = 0\n",
    "\n",
    "# Remove off-disk for plottting, though not necessary for outcomes\n",
    "thresh_ensemble_map_data[np.isnan(ensemble_map.data)] = np.nan\n",
    "\n",
    "thresh_ensemble_map = sunpy.map.Map(\n",
    "    thresh_ensemble_map_data, ensemble_map.meta\n",
    ")\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=thresh_ensemble_map)\n",
    "thresh_ensemble_map.plot(axes=ax, title='', cmap='magma',\n",
    "                         vmin=confidence_range[0], vmax=confidence_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "# Compute outcomes by CH and sort from greatest to least\n",
    "outcome_by_ch_dict = detect.get_outcomes_by_ch(\n",
    "    thresh_ensemble_map, he_map_data, reprojected_mag_map, confidence_level\n",
    ")\n",
    "sorted_idxs = np.flip(np.argsort(outcome_by_ch_dict['unipolarity']))\n",
    "\n",
    "sorted_outcome_by_ch_dict = {}\n",
    "for key, outcome_by_ch in zip(outcome_by_ch_dict, outcome_by_ch_dict.values()):\n",
    "    sorted_outcome_by_ch_dict[key] = [outcome_by_ch[i] for i in sorted_idxs]\n",
    "\n",
    "    \n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "\n",
    "# Mask of detected CHs at the given confidence level\n",
    "confidence_ch_mask = np.where(\n",
    "    thresh_ensemble_map.data >= confidence_level, 1, 0\n",
    ")\n",
    "\n",
    "# List of ensemble map data for distinct CHs\n",
    "ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    thresh_ensemble_map.data, confidence_ch_mask\n",
    ")\n",
    "ensemble_map_data_by_ch = [ensemble_map_data_by_ch[i] for i in sorted_idxs]\n",
    "\n",
    "\n",
    "[print(f'{area:.1e} Mm^2', end='\\t')\n",
    " for area in sorted_outcome_by_ch_dict['area']]\n",
    "print()\n",
    "[print(f'Lat: {cm_lat:.1f} deg', end='\\t')\n",
    " for cm_lat in sorted_outcome_by_ch_dict['cm_lat']]\n",
    "print()\n",
    "[print(f'Lon: {cm_lon:.1f} deg', end='\\t')\n",
    " for cm_lon in sorted_outcome_by_ch_dict['cm_lon']]\n",
    "print()\n",
    "[print(f'{signed_flux:.4e} Mx', end='\\t')\n",
    " for signed_flux in sorted_outcome_by_ch_dict['signed_flux']]\n",
    "print()\n",
    "[print(f'Skew: {mag_skew:.4f}', end='\\t')\n",
    " for mag_skew in sorted_outcome_by_ch_dict['mag_skew']]\n",
    "print()\n",
    "[print(f'U: {unipolarity:.4f}', end='\\t')\n",
    " for unipolarity in sorted_outcome_by_ch_dict['unipolarity']]\n",
    "print()\n",
    "[print(f'Grad: {grad_median:.4f}', end='\\t')\n",
    " for grad_median in sorted_outcome_by_ch_dict['grad_median']]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch_idx = np.argmin(sorted_outcome_by_ch_dict['unipolarity'])\n",
    "# ch_idx = np.argmax(sorted_outcome_by_ch_dict['area'])\n",
    "# ch_idx = np.argmax(sorted_outcome_by_ch_dict['grad_median'])\n",
    "# ch_idx = np.argmax(np.abs(sorted_outcome_by_ch_dict['mag_skew']))\n",
    "# ch_idx = np.argmin(np.abs(sorted_outcome_by_ch_dict['cm_lon']))\n",
    "ch_idx = 2\n",
    "\n",
    "# signed_flux = sorted_outcome_by_ch_dict['signed_flux'][ch_idx]\n",
    "# mag_skew = sorted_outcome_by_ch_dict['mag_skew'][ch_idx]\n",
    "unipolarity = sorted_outcome_by_ch_dict['unipolarity'][ch_idx]\n",
    "area = sorted_outcome_by_ch_dict['area'][ch_idx]\n",
    "cm_lon = sorted_outcome_by_ch_dict['cm_lon'][ch_idx]\n",
    "cm_lat = sorted_outcome_by_ch_dict['cm_lat'][ch_idx]\n",
    "# grad_median = sorted_outcome_by_ch_dict['grad_median'][ch_idx]\n",
    "\n",
    "# Calculate foreshortening factor. 0: 90deg inc angle, 1: 0deg inc angle\n",
    "B0 = ensemble_map.observer_coordinate.lat.value\n",
    "foreshort_factor = np.cos(np.deg2rad(cm_lon))*np.cos(np.deg2rad(cm_lat - B0))\n",
    "\n",
    "# title = f'{area:.2e} Mm^2 | {signed_flux:.2e} Mx | {mag_skew:.2f} Skew'\n",
    "# title = f'{signed_flux:.2e} Mx | {unipolarity:.2f} Unipolarity | {mag_skew:.2f} Skew'\n",
    "title = rf'{unipolarity:.2f} Unipolarity | {area:.1e} $Mm^2$ | {foreshort_factor:.2f} Foreshort Factor'\n",
    "# title = f'{unipolarity:.2f} Unipolarity | {grad_median:.2f} Gradient Median'\n",
    "\n",
    "A_per_square_px = detect.get_A_per_square_px(ensemble_map)        \n",
    "\n",
    "\n",
    "selected_ch_map_data = ensemble_map_data_by_ch[ch_idx]\n",
    "selected_ch_map_data = np.where(np.isnan(selected_ch_map_data), -100,\n",
    "                                selected_ch_map_data)\n",
    "selected_ch_map = sunpy.map.Map(selected_ch_map_data, he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.suptitle(he_date_str)\n",
    "\n",
    "ax = fig.add_subplot(projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title=title)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in selected_ch_map.contour(0):\n",
    "    ax.plot_coord(contour, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank each CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_mag_map(ensemble_map, confidence_level, sorted_idxs):\n",
    "    \"\"\"Retrieve a map of CHs from a single segmentation as ranked by a\n",
    "    histogram statistic.\n",
    "    \n",
    "    Args\n",
    "        ensemble_map_data: map data which was segmented\n",
    "        ch_mask: binary coronal holes mask\n",
    "    Returns\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Mask of detected CHs at the given confidence level\n",
    "    confidence_ch_mask = np.where(\n",
    "        ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    "    )\n",
    "\n",
    "    # List of ensemble map data for distinct CHs\n",
    "    ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        ensemble_map.data, confidence_ch_mask\n",
    "    )\n",
    "    num_ch = len(ensemble_map_data_by_ch)\n",
    "    ensemble_map_data_by_ch = [ensemble_map_data_by_ch[i]\n",
    "                               for i in np.flip(sorted_idxs)]\n",
    "    \n",
    "    ranked_map_data = np.where(\n",
    "        ~np.isnan(ensemble_map.data), 0, np.nan\n",
    "    )\n",
    "    for map_data, ch_num in zip(ensemble_map_data_by_ch, range(num_ch)):\n",
    "        ranked_map_data = np.where(\n",
    "            ~np.isnan(map_data), (ch_num + 1)*100/num_ch, ranked_map_data\n",
    "        )\n",
    "    \n",
    "    ranked_map = sunpy.map.Map(ranked_map_data, he_map.meta)\n",
    "    return ranked_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch = len(ensemble_map_data_by_ch)\n",
    "\n",
    "# image_list = [pre_processed_map_data for _ in range(num_ch)]\n",
    "\n",
    "mag_data = np.flipud(reprojected_mag_map.data)\n",
    "smooth_size = 0.05 *mag_data.shape[0]\n",
    "smoothed_mag_data = np.where(np.isnan(mag_data), 0, mag_data)\n",
    "smoothed_mag_data = ndimage.uniform_filter(\n",
    "    smoothed_mag_data, smooth_size\n",
    ")\n",
    "mag_data = np.clip(\n",
    "    np.where(~np.isnan(mag_data), smoothed_mag_data, np.nan), -2, 2\n",
    ")\n",
    "image_list = [mag_data for _ in range(num_ch)]\n",
    "\n",
    "axes = plot_detection.plot_image_grid(image_list, num_cols=3, cmap='gray')\n",
    "\n",
    "for ax, i, ch_data in zip(axes.values(), range(num_ch), ensemble_map_data_by_ch):\n",
    "    mask = np.where(np.isnan(ch_data), 0, 1)\n",
    "    \n",
    "    ax.set_title((f'{sorted_outcome_by_ch_dict[\"unipolarity\"][i]:.2f} Unipolarity | '\n",
    "                  f'{sorted_outcome_by_ch_dict[\"mag_skew\"][i]:.2f} Skew'))\n",
    "    ax.contour(np.flipud(mask), cmap=plt.cm.plasma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save maps thresholded above a confidence level and then sorted by that outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEUTRAL LINE COMPARISON NEEDS UPDATE\n",
    "overwrite = False\n",
    "confidence_level = 0\n",
    "\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Unipolarity_Rank/'\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}He{he_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'EUV {euv_date_str} comparison already exists.'))\n",
    "        continue\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    # Extract saved ensemble map array and convert to Sunpy map\n",
    "    ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "    ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "    # Extract saved processed magnetograms\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    mag_fits_name = f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}_He{he_date_str}'\n",
    "    reprojected_fits_file = f'{mag_fits_name}.fits'\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "    # Compute outcomes by CH and sort from greatest to least\n",
    "    # TODO: Update get_outcomes_by_ch call to dict format\n",
    "    outcomes_by_ch = detect.get_outcomes_by_ch(\n",
    "        ensemble_map, pre_processed_map, reprojected_mag_map, confidence_level\n",
    "    )\n",
    "    unipolarity_by_ch = outcomes_by_ch[4]\n",
    "    sorted_idxs = np.argsort(unipolarity_by_ch)\n",
    "\n",
    "    # Obtain ranked map\n",
    "    ranked_map = get_ranked_mag_map(\n",
    "        ensemble_map, confidence_level, sorted_idxs\n",
    "    )\n",
    "    ranked_map.plot_settings['cmap'] = colormaps['magma']\n",
    "\n",
    "    euv_date_str = prepare_data.get_nearest_date_str(\n",
    "        EUV_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    plot_detection.plot_he_neutral_lines_euv_comparison(\n",
    "        fig, he_date_str, mag_date_str, euv_date_str, ranked_map\n",
    "    )\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(comparison_img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{euv_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness Histogram in Single CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sym_log_hist(data_list, num_bins):\n",
    "    # Symmetric log bins\n",
    "    neg_outcomes = np.where(data_list < 0, np.abs(data_list), 0)\n",
    "    bin_min = np.ceil(np.log10(np.max(neg_outcomes)))\n",
    "    bin_max = np.ceil(np.log10(np.max(np.abs(data_list))))\n",
    "    bins = np.hstack((-np.logspace(bin_min, 0, num_bins//2),\n",
    "                    np.logspace(0, bin_max, num_bins//2)))\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot()\n",
    "    ax.set_xscale('symlog')\n",
    "    ax.hist(data_list, bins)\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = sorted_outcome_by_ch_dict['pixel_signed_fluxes'][ch_idx]\n",
    "fig, ax = plot_sym_log_hist(data_list, num_bins=500)\n",
    "fig.suptitle(f'CH Index: {ch_idx}')\n",
    "ax.set_title('Pixel Magnetic Flux Histogram')\n",
    "ax.set_xlabel('Magnetic Flux (Wb)')\n",
    "ax.set_ylim([0,3500])\n",
    "f'Summed: {np.sum(data_list):.3e} | Pre-Computed {sorted_outcome_by_ch_dict[\"signed_flux\"][ch_idx]:.3e}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line of Sight B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "    \n",
    "percent_unipolar_by_ch = []\n",
    "signed_B_by_ch = []\n",
    "unsigned_B_by_ch = []\n",
    "unipolarity_by_ch = []\n",
    "\n",
    "# Thresholded array at the given confidence level\n",
    "confidence_map_data = np.where(\n",
    "    ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    ")\n",
    "\n",
    "# Array with number labels per distinct CH and number of labels\n",
    "labeled_map_data, num_labels = ndimage.label(confidence_map_data)\n",
    "num_ch = num_labels - 1\n",
    "\n",
    "# List of magnetic field data images for distinct CHs.\n",
    "# Equivalent to obtain Helioprojective coordinates of pixels per CH\n",
    "# and then obtaining magnetic data at HP coordinates\n",
    "mag_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    reprojected_mag_map.data, confidence_map_data\n",
    ")\n",
    "\n",
    "# # Obtain masked magnetic data per CH > Compute on magnetic data per CH\n",
    "# mask_idxs_by_ch = [np.where(labeled_map_data == label)\n",
    "#                    for label in range(1, num_labels)]\n",
    "# mag_data_by_ch = [reprojected_mag_map.data[mask_idxs]\n",
    "#                   for mask_idxs in mask_idxs_by_ch]\n",
    "\n",
    "# List of masks for distinct CHs\n",
    "masks_by_ch = [np.where(labeled_map_data == label, 1, 0)\n",
    "               for label in range(1, num_labels)]\n",
    "\n",
    "for ch_label in range(num_ch):\n",
    "    mag_map_data = mag_map_data_by_ch[ch_label]\n",
    "    mag_data = mag_map_data[~np.isnan(mag_map_data)]\n",
    "    \n",
    "    # Pixel percent unipolarity\n",
    "    num_positive = np.count_nonzero(mag_data > 0)\n",
    "    num_negative = np.count_nonzero(mag_data < 0)\n",
    "    num_px = np.count_nonzero(mag_data)\n",
    "    percent_unipolarity = max(num_positive, num_negative)*100/num_px\n",
    "    percent_unipolar_by_ch.append(percent_unipolarity)\n",
    "    \n",
    "    # Signed average magnetic field\n",
    "    signed_B = np.abs(np.mean(mag_data))\n",
    "    signed_B_by_ch.append(signed_B)\n",
    "\n",
    "    # Unsigned average magnetic field\n",
    "    unsigned_B = np.mean(np.abs(mag_data))\n",
    "    unsigned_B_by_ch.append(unsigned_B)\n",
    "    \n",
    "    # Unipolarity\n",
    "    unipolarity = (unsigned_B - signed_B)/unsigned_B\n",
    "    unipolarity_by_ch.append(unipolarity)\n",
    "\n",
    "\n",
    "# Sort outcomes by CH from greatest to least\n",
    "sorted_idxs = np.flip(np.argsort(percent_unipolar_by_ch))\n",
    "percent_unipolar_by_ch.sort(reverse=True)\n",
    "\n",
    "# Sort candidate CHs from greatest to least outcome median\n",
    "masks_by_ch = [masks_by_ch[i] for i in sorted_idxs]\n",
    "signed_B_by_ch = [signed_B_by_ch[i] for i in sorted_idxs]\n",
    "unsigned_B_by_ch = [unsigned_B_by_ch[i] for i in sorted_idxs]\n",
    "unipolarity_by_ch = [unipolarity_by_ch[i] for i in sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = [pre_processed_map for _ in range(num_ch)]\n",
    "axes = plot_detection.plot_image_grid(image_list, num_cols=3, cmap='gray')\n",
    "\n",
    "for ax, i, mask in zip(axes.values(), range(num_ch), masks_by_ch):\n",
    "    ax.set_title((f'{percent_unipolar_by_ch[i]:.1f}% Unipolar '\n",
    "                  + f'| {unipolarity_by_ch[i]:.2f} Unipolarity'))\n",
    "    ax.contour(np.flipud(mask), cmap=plt.cm.plasma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Magnetic\n",
    "\n",
    "Verify magnetic properties by setting a uniform value to detected regions to verify perfect unipolarity observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_mag_map_data = np.where(ensemble_map.data > 1, 25, reprojected_mag_map.data)\n",
    "fake_mag_map = sunpy.map.Map(fake_mag_map_data, reprojected_mag_map.meta)\n",
    "fake_mag_map.plot(vmin=-50, vmax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "outcome_by_ch_dict = detect.get_outcomes_by_ch(\n",
    "    ensemble_map, he_map_data, fake_mag_map, confidence_level\n",
    ")\n",
    "\n",
    "# Mask of detected CHs at the given confidence level\n",
    "confidence_ch_mask = np.where(\n",
    "    ensemble_map.data >= confidence_level, ensemble_map.data, 0\n",
    ")\n",
    "\n",
    "# List of ensemble map data for distinct CHs\n",
    "ensemble_map_data_by_ch = detect.get_map_data_by_ch(\n",
    "    ensemble_map.data, confidence_ch_mask\n",
    ")\n",
    "\n",
    "[print(f'{unipolarity:.4f}', end='\\t')\n",
    " for unipolarity in outcome_by_ch_dict['unipolarity']]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify vs Global Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLAR_AREA = 4*np.pi*(1*u.solRad).to(u.Mm)**2\n",
    "\n",
    "summed_area = np.sum(outcome_by_ch_dict['area'])*u.Mm**2 /SOLAR_AREA*100\n",
    "global_area = detect.get_open_area(ensemble_map, confidence_level=0)[0]\n",
    "\n",
    "f'Summed: {summed_area:.6f} % | Global: {global_area:.6f} %'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_flux = np.sum(outcome_by_ch_dict['unsigned_flux'])\n",
    "global_flux = detect.get_unsigned_open_flux(\n",
    "    ensemble_map, reprojected_mag_map, confidence_level=0\n",
    ")\n",
    "f'{summed_flux:.6e} Wb | Global: {global_flux:.6e} Wb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### He I Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ch_band_widths(map_data_by_ch):\n",
    "    \"\"\"Retrieve a list of 5th to 95th percentile band widths for each\n",
    "    detected CH.\n",
    "    \n",
    "    Args\n",
    "        map_data_by_ch: list of isolated CH images from a segmentation\n",
    "    \"\"\"\n",
    "    percentiles = [5, 95]\n",
    "    bound_list = [np.percentile(map_data[~np.isnan(map_data)], percentiles)\n",
    "                  for map_data in map_data_by_ch]\n",
    "    \n",
    "    hole_band_widths = [bounds[1] - bounds[0]\n",
    "                        for bounds in bound_list]\n",
    "    return hole_band_widths\n",
    "\n",
    "\n",
    "def get_ch_lower_tail_widths(map_data_by_ch):\n",
    "    \"\"\"Retrieve a list of lower tail widths for each detected CH.\n",
    "    \n",
    "    Args\n",
    "        map_data_by_ch: list of isolated CH images from a segmentation\n",
    "    \"\"\"\n",
    "    filt_map_data_by_ch = [map_data[~np.isnan(map_data)]\n",
    "                         for map_data in map_data_by_ch]\n",
    "    \n",
    "    # List of the 1st percentile brightness value of each CH\n",
    "    first_percentile_list = [np.percentile(map_data, 1)\n",
    "                             for map_data in filt_map_data_by_ch]\n",
    "        \n",
    "    # List of peak count of each CH\n",
    "    peak_counts_value_list = [\n",
    "        detect.get_peak_counts_loc(map_data, bins_as_percent=False)\n",
    "        for map_data in filt_map_data_by_ch\n",
    "    ]\n",
    "\n",
    "    # List of lower tail widths of each CH\n",
    "    ch_lower_tail_width_list = [\n",
    "        peak_count - first_percentile\n",
    "        for peak_count, first_percentile \n",
    "        in zip(peak_counts_value_list, first_percentile_list)]\n",
    "    \n",
    "    return ch_lower_tail_width_list\n",
    "\n",
    "\n",
    "def plot_sorted_ch_hists(array, ch_mask, apply_gradient, hist_stat,\n",
    "                         descend_sort=False):\n",
    "    \"\"\"Plot segmented CH histograms sorted by histogram statistics.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask: binary coronal holes mask\n",
    "        apply_gradient: boolean to specify taking spatial gradient of image\n",
    "        hist_stat: str to specify histogram sorting statistic\n",
    "            'median', 'width', 'tail_width'\n",
    "        descend_sort: boolean to specify sorting CHs from greatest to least\n",
    "            statistic\n",
    "    \"\"\"\n",
    "    # Masked array of candidate CHs\n",
    "    masked_candidates = detect.get_masked_candidates(array, ch_mask)\n",
    "    if apply_gradient:\n",
    "        masked_candidates = filters.sobel(masked_candidates)\n",
    "    \n",
    "    # Isolated images of detected CHs\n",
    "    map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        masked_candidates, ch_mask\n",
    "    )\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    \n",
    "    # Compute statistics for each CH\n",
    "    medians = [np.nanmedian(map_data) for map_data in map_data_by_ch]\n",
    "    ch_band_widths = get_ch_band_widths(map_data_by_ch)\n",
    "    \n",
    "    # Histogram x limit bounds\n",
    "    hist_xlim_min = np.mean(medians)\n",
    "    if not apply_gradient:\n",
    "        hist_xlim_min = hist_xlim_min - 2*np.max(ch_band_widths)\n",
    "    hist_xlim_max = np.mean(medians) + 2*np.max(ch_band_widths)\n",
    "    \n",
    "    # Obtain indices of candidates sorted by specifed mode\n",
    "    if hist_stat == 'median':\n",
    "        sorted_candidate_idxs = np.argsort(medians)\n",
    "        titles = [f'Median: {median:.2f}'\n",
    "                  for median in medians]\n",
    "    elif hist_stat == 'width':\n",
    "        sorted_candidate_idxs = np.argsort(ch_band_widths)\n",
    "        titles = [f'90% Band Width: {ch_band_width:.1f}'\n",
    "                  for ch_band_width in ch_band_widths]\n",
    "    elif hist_stat == 'tail_width':\n",
    "        ch_lower_tail_width_list = get_ch_lower_tail_widths(\n",
    "            map_data_by_ch\n",
    "        )\n",
    "        sorted_candidate_idxs = np.argsort(ch_lower_tail_width_list)\n",
    "        titles = [f'1% to Peak Width: {ch_lower_tail_width:.1f}'\n",
    "                  for ch_lower_tail_width in ch_lower_tail_width_list]\n",
    "    \n",
    "    if descend_sort:\n",
    "        sorted_candidate_idxs = np.flip(sorted_candidate_idxs)\n",
    "\n",
    "    for r in range(int(np.ceil(num_ch/2))):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(60, 10))\n",
    "        ax = axes.ravel()\n",
    "        \n",
    "        for c in range(2):\n",
    "            i = 2*r + c\n",
    "            ax_i = 3*c\n",
    "            if i + 1 > num_ch:\n",
    "                return\n",
    "            \n",
    "            # Retrieve isolated CH image and contour\n",
    "            ch_num = sorted_candidate_idxs[i]\n",
    "            ch_im = map_data_by_ch[ch_num]\n",
    "            ch_contour = np.where(~np.isnan(ch_im), 1, 0)\n",
    "\n",
    "            # Zoom in on an isolated CH\n",
    "            y, x = np.where(~np.isnan(ch_im))\n",
    "            ch_zoom = ch_im[np.min(y) - 10:np.max(y) + 10,\n",
    "                             np.min(x) - 10:np.max(x) + 10]\n",
    "                \n",
    "            hist, edges = detect.get_hist(\n",
    "                ch_zoom[~np.isnan(ch_zoom)], bins_as_percent=False, n=200\n",
    "            )\n",
    "            \n",
    "            ax[ax_i].set_title(f'Hole {ch_num + 1}', fontsize=32)\n",
    "            ax[ax_i].imshow(array, cmap='gray', vmin=-100, vmax=100)\n",
    "            ax[ax_i].contour(ch_contour, cmap='plasma')\n",
    "            \n",
    "            if apply_gradient:\n",
    "                cmap = plt.cm.viridis\n",
    "            else:\n",
    "                cmap = plt.cm.magma\n",
    "\n",
    "            ax[ax_i + 1].imshow(ch_zoom, cmap)\n",
    "\n",
    "            ax[ax_i + 2].set_title(titles[ch_num], fontsize=32)\n",
    "            ax[ax_i + 2].bar(edges[0:-1], hist)\n",
    "            ax[ax_i + 2].set_xlim([hist_xlim_min, hist_xlim_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires single mask ensemble map\n",
    "ch_mask = np.where(ensemble_map_data > 0, 1, 0)\n",
    "plot_sorted_ch_hists(\n",
    "    he_map_data, ch_mask, apply_gradient=True,\n",
    "    hist_stat='median'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all CHs and histograms for a single date.\n",
    "# Display ranked maps for all dates.\n",
    "def get_ranked_map(array, ch_mask, apply_gradient, hist_stat,\n",
    "                   ascend_sort=True):\n",
    "    \"\"\"Retrieve a map of CHs from a single segmentation as ranked by a\n",
    "    histogram statistic.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask: binary coronal holes mask\n",
    "        apply_gradient: boolean to specify taking spatial gradient of image\n",
    "        hist_stat: str to specify histogram sorting statistic\n",
    "            'median', 'width', 'tail_width'\n",
    "        ascend_sort: boolean to specify sorting CHs from least to greatest\n",
    "            statistic\n",
    "    Returns\n",
    "        List of isolated CH images from a segmentation.\n",
    "    \"\"\"\n",
    "    # Masked array of candidate CHs\n",
    "    masked_candidates = detect.get_masked_candidates(array, ch_mask)\n",
    "    if apply_gradient:\n",
    "        masked_candidates = filters.sobel(masked_candidates)\n",
    "    \n",
    "    # Isolated images of detected CHs\n",
    "    map_data_by_ch = detect.get_map_data_by_ch(\n",
    "        masked_candidates, ch_mask\n",
    "    )\n",
    "    num_ch = len(map_data_by_ch)\n",
    "    \n",
    "    # Rank candidates by histogram statistic\n",
    "    if hist_stat == 'median':\n",
    "        medians = [np.nanmedian(map_data) for map_data in map_data_by_ch]\n",
    "        sorted_candidate_idxs = np.argsort(medians)\n",
    "    elif hist_stat == 'width':\n",
    "        ch_band_widths = get_ch_band_widths(map_data_by_ch)\n",
    "        sorted_candidate_idxs = np.argsort(ch_band_widths)\n",
    "    elif hist_stat == 'tail_width':\n",
    "        ch_lower_tail_width_list = get_ch_lower_tail_widths(\n",
    "            map_data_by_ch\n",
    "        )\n",
    "        sorted_candidate_idxs = np.argsort(ch_lower_tail_width_list)\n",
    "    \n",
    "    if ascend_sort:\n",
    "        sorted_candidate_idxs = np.flip(sorted_candidate_idxs)\n",
    "    \n",
    "    map_data_by_ch = np.array(map_data_by_ch)\n",
    "    ranked_ch_ims = map_data_by_ch[sorted_candidate_idxs]\n",
    "    \n",
    "    ranked_map = np.where(\n",
    "        ~np.isnan(array), 0, np.nan\n",
    "    )\n",
    "    for isolated_ch_im, ch_num in zip(ranked_ch_ims, range(num_ch)):\n",
    "        ranked_map = np.where(\n",
    "            ~np.isnan(isolated_ch_im), (ch_num + 1)*100/num_ch, ranked_map\n",
    "        )\n",
    "    return ranked_map\n",
    "\n",
    "\n",
    "def plot_ensemble_comparison(he_map, ensemble_map, euv_map):\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Plot He observation\n",
    "    ax = fig.add_subplot(1, 3, 1, projection=he_map)\n",
    "    he_map.plot(axes=ax, vmin=-100, vmax=100)\n",
    "    \n",
    "    # Plot ensemble map with overlayed neutral lines\n",
    "    ax = fig.add_subplot(1, 3, 2, projection=he_map)\n",
    "    ensemble_map.plot(axes=ax, title='', cmap='magma')\n",
    "    \n",
    "    # Plot EUV observation\n",
    "    ax = fig.add_subplot(1, 3, 3, projection=euv_map)\n",
    "    euv_map.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brightness Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=False, hist_stat='width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brightness Tail Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=False, hist_stat='tail_width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=True, hist_stat='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "\n",
    "plot_sorted_ch_hists(pre_process_v0_1_he, ch_mask,\n",
    "                     apply_gradient=True, hist_stat='width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ranking by different smoothness statistics for a single date\n",
    "percent_of_peak = 90\n",
    "morph_radius = 15\n",
    "apply_gradient_list = [False, False, True, True]\n",
    "hist_stat_list = ['width', 'tail_width', 'median', 'width']\n",
    "\n",
    "\n",
    "pre_process_v0_1_he = detect.pre_process_v0_1(he_map_data)[0]\n",
    "\n",
    "ch_mask = detect.get_ch_mask(pre_process_v0_1_he, percent_of_peak, morph_radius)\n",
    "    \n",
    "for apply_gradient, hist_stat in zip(apply_gradient_list, hist_stat_list):\n",
    "    ranked_map_data = get_ranked_map(\n",
    "        pre_process_v0_1_he, ch_mask, apply_gradient, hist_stat\n",
    "    )\n",
    "    \n",
    "    ranked_map = sunpy.map.Map(np.flipud(ranked_map_data), he_map.meta)\n",
    "    plot_ensemble_comparison(he_map, ranked_map, euv_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates: Missing Helioprojective Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without modifying when reading FITS file\n",
    "with fits.open(he_fits_file) as hdu_list:\n",
    "    header = hdu_list[-1].header\n",
    "    num_data_arrays = header.get('NAXIS3')\n",
    "    \n",
    "    if not num_data_arrays:\n",
    "        data = hdu_list[-1].data\n",
    "    else:\n",
    "        data = hdu_list[-1].data[0]\n",
    "\n",
    "he_map = sunpy.map.Map(data, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup Header Heliocentric delt/pix To Helioprojective delt/pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian distance change per pixel\n",
    "hc_delta_coords = frames.Heliocentric(\n",
    "    header['CDELT1A']*u.Unit(header['CUNIT1A']),\n",
    "    header['CDELT2A']*u.Unit(header['CUNIT2A']),\n",
    "    z=0*u.m, observer='earth', obstime=header['DATE-OBS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunpy coordinate transform\n",
    "hp_delta_coords = hc_delta_coords.transform_to(\n",
    "    frames.Helioprojective(\n",
    "        header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    ")\n",
    "print(f'HP Scale: {hp_delta_coords.Tx.value:.11f}, {hp_delta_coords.Ty.value:.11f} arcsec/pix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thompson 2005 method section 4.1\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "\n",
    "# Sun-Earth distance\n",
    "Ds = earth_header['dsun_obs']*u.m\n",
    "\n",
    "hp_delta_Tx = (hc_delta_coords.x/Ds).to(u.dimensionless_unscaled)\n",
    "hp_delta_Tx = (hp_delta_Tx*u.rad).to(u.arcsec)\n",
    "hp_delta_Ty = (hc_delta_coords.y/Ds).to(u.dimensionless_unscaled)\n",
    "hp_delta_Ty = (hp_delta_Ty*u.rad).to(u.arcsec)\n",
    "\n",
    "print(f'HP Scale: {hp_delta_Tx.value:.11f}, {hp_delta_Ty.value:.11f} arcsec/pix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreshortening Factor\n",
    "\n",
    "Testing with ~June maps nearly removes B0 influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plot_detection.plot_he_map(fig, (1, 1, 1), he_map, he_date_str)\n",
    "\n",
    "he_map.draw_limb(axes=ax, color=\"k\")\n",
    "he_map.draw_grid(axes=ax, color=\"k\")\n",
    "\n",
    "B0 = he_map.observer_coordinate.lat\n",
    "# lon = np.linspace(0,90,5) * u.deg\n",
    "lon = [0,19,42,90] * u.deg\n",
    "# lon = [-0.13] * u.deg\n",
    "# lon = np.arccos(np.linspace(1,0,5)) * u.rad\n",
    "lat = [0 for _ in range(len(lon))] * u.deg\n",
    "\n",
    "coords = SkyCoord(\n",
    "    lon, lat, frame=frames.HeliographicStonyhurst, obstime=he_map.date\n",
    ")\n",
    "ax.plot_coord(coords, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_lons = lon.to(u.rad).value\n",
    "pixel_lats = lat.to(u.rad).value - B0.to(u.rad).value\n",
    "foreshort_factors = np.cos(pixel_lons)*np.cos(pixel_lats)\n",
    "foreshort_factors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Map Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(35):\n",
    "    ax.plot_coord(contour, color='white')\n",
    "    \n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(65):\n",
    "    ax.plot_coord(contour, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out detections below a CL threshold\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ensemble_map.mask = (ensemble_map.data > 50)\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(np.flipud(ensemble_map.mask), cmap='magma')\n",
    "\n",
    "ensemble_map.mask = (ensemble_map.data < 50)\n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "\n",
    "ensemble_map.mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold Map by Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 20\n",
    "\n",
    "\n",
    "confidence_map = np.where(ensemble_map_data >= confidence_level, ensemble_map_data, 0)\n",
    "labeled_map, num_ch = ndimage.label(confidence_map)\n",
    "\n",
    "confidence_map = np.where(np.isnan(ensemble_map_data), np.nan, confidence_map)\n",
    "labeled_map = np.where(np.isnan(ensemble_map_data), np.nan, labeled_map)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 10))\n",
    "\n",
    "ax[0].set_title(he_date_str)\n",
    "ax[0].imshow(he, cmap=plt.cm.gray)\n",
    "\n",
    "ax[1].imshow(ensemble_map_data, cmap=plt.cm.magma)\n",
    "ax[2].imshow(labeled_map, cmap=plt.cm.bone)\n",
    "print(num_ch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ensemble_data = np.where(~np.isnan(np.flipud(ensemble_map.data)), 1, np.nan)\n",
    "ensemble_map = sunpy.map.Map(fake_ensemble_data, pre_processed_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = fig.add_subplot(111, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax, title='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Area: Compare official function with decomposed to investigate failed retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLAR_AREA = 4*np.pi *(1*u.solRad).to(u.Mm)**2\n",
    "\n",
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    hp_delta_coord = frames.Helioprojective(\n",
    "        ensemble_map.scale.axis1*u.pix,\n",
    "        ensemble_map.scale.axis2*u.pix,\n",
    "        observer='earth', obstime=ensemble_map.date\n",
    "    )\n",
    "    hc_delta_coord = hp_delta_coord.transform_to(\n",
    "        frames.Heliocentric(observer='earth', obstime=ensemble_map.date)\n",
    "    )\n",
    "    A_per_square_px = np.abs(\n",
    "        hc_delta_coord.x.to(u.Mm)*hc_delta_coord.y.to(u.Mm)\n",
    "    )\n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "\n",
    "    x_scale, y_scale = detect.get_hg_map_dist_scales(pre_processed_map)\n",
    "    A_per_square_px = x_scale*y_scale\n",
    "else:\n",
    "    raise Exception(('Coordinate frame not recognized for obtaining '\n",
    "                     'area per square pixel.'))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Detected pixels at a confidence level\n",
    "# Flip upside down to align Sunpy coordinates and Numpy indices\n",
    "detected_px_coords = np.where(\n",
    "    np.flipud(ensemble_map.data) >= confidence_level\n",
    ")\n",
    "\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    \n",
    "    # Convert detected pixels to Helioprojective Tx, Ty\n",
    "    detected_hp_coords = ensemble_map.pixel_to_world(\n",
    "        detected_px_coords[1]*u.pix, detected_px_coords[0]*u.pix\n",
    "    )\n",
    "\n",
    "    # Convert detected Helioprojective Tx, Ty to Heliographic lon, lat\n",
    "    raw_detected_hg_coords = detected_hp_coords.transform_to(\n",
    "        frames.HeliographicStonyhurst(obstime=ensemble_map.date)\n",
    "    )\n",
    "\n",
    "    # Remove pixels with failed conversion and longitudes outside (-90,90)\n",
    "    failed_coord_idxs = np.where(\n",
    "        np.isnan(raw_detected_hg_coords.lon) \n",
    "        | (np.abs(raw_detected_hg_coords.lon.to(u.deg).value) >= 90)\n",
    "    )\n",
    "    detected_hg_coords = np.delete(\n",
    "        raw_detected_hg_coords, failed_coord_idxs\n",
    "    )\n",
    "    \n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "    \n",
    "    # Convert detected pixels to Heliographic lon, lat\n",
    "    detected_hg_coords = ensemble_map.pixel_to_world(\n",
    "        detected_px_coords[1]*u.pix, detected_px_coords[0]*u.pix\n",
    "    )\n",
    "    failed_coord_idxs = np.array([], dtype=np.int8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "if ensemble_map.coordinate_frame.name == 'helioprojective':\n",
    "    \n",
    "    # B-angle to subtract from latitude\n",
    "    B0 = ensemble_map.observer_coordinate.lat\n",
    "\n",
    "    pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "    pixel_lats = detected_hg_coords.lat.to(u.rad).value - B0.to(u.rad).value\n",
    "    pixel_areas = A_per_square_px/(np.cos(pixel_lons)*np.cos(pixel_lats))\n",
    "\n",
    "elif ensemble_map.coordinate_frame.name == 'heliographic_stonyhurst':\n",
    "    pixel_areas = np.ones(detected_hg_coords.shape)*A_per_square_px\n",
    "\n",
    "# Sum area detected in all pixels\n",
    "area = np.sum(pixel_areas)\n",
    "area_percent = area/SOLAR_AREA*100\n",
    "area_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect.get_open_area(ensemble_map, confidence_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(detected_hg_coords.lat > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import CylindricalRepresentation\n",
    "\n",
    "# WRONG BC RHO IS CYLINDRICAL AS OPPOSED TO SPHERICAL\n",
    "# # Convert detected Helioprojective Sky Coords to Heliocentric radial rho, psi\n",
    "# raw_pixel_hc_coords = pixel_hp_coords.transform_to(\n",
    "#     frames.Heliocentric(observer='earth', obstime=obstime)\n",
    "# )\n",
    "# # raw_pixel_hc_coords = raw_pixel_hc_coords.represent_as(CylindricalRepresentation)\n",
    "# pixel_hc_coords = raw_pixel_hc_coords[np.where(~np.isnan(raw_pixel_hc_coords.x))]\n",
    "\n",
    "# # Compute area per pixel while accounting for foreshortening\n",
    "# rho = np.sqrt(pixel_hc_coords.x**2 + pixel_hc_coords.y**2 + pixel_hc_coords.z**2)\n",
    "# pixel_sol_rad_ratios = (pixel_hc_coords.rho/u.solRad).to(u.dimensionless_unscaled)\n",
    "# pixel_angles_to_limb = pixel_sol_rad_ratios*np.pi/2 *u.rad\n",
    "# pixel_areas = A_per_square_px/np.cos(pixel_angles_to_limb.value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed Coordinate Conversion Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(failed_coord_idxs):\n",
    "    failed_hp_coords = detected_hp_coords[failed_coord_idxs]\n",
    "    failed_pixel_pairs = ensemble_map.world_to_pixel(failed_hp_coords)\n",
    "else:\n",
    "    print('No points matched condition')\n",
    "\n",
    "ensemble_map.mask = None\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "failed_point_color = '#1ed950'\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(ensemble_map.data, cmap='magma')\n",
    "if np.any(failed_coord_idxs):\n",
    "    ax.scatter(failed_pixel_pairs.x.value, failed_pixel_pairs.y.value,\n",
    "               color=failed_point_color)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = fig.add_subplot(122, projection=ensemble_map)\n",
    "ensemble_map.plot(axes=ax)\n",
    "ensemble_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(confidence_level):\n",
    "    ax.plot_coord(contour, color='white')\n",
    "\n",
    "if np.any(failed_coord_idxs):\n",
    "    ax.plot_coord(failed_hp_coords, 'o', color=failed_point_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heliographic pixels with too large longitude\n",
    "large_lon_pixel_hg_coords = raw_detected_hg_coords[\n",
    "    np.where(~np.isnan(raw_detected_hg_coords.lon)\n",
    "             & (raw_detected_hg_coords.lon.to(u.deg).value >= 90))\n",
    "]\n",
    "large_lon_pixel_hg_coords\n",
    "# large_lon_pixel_hg_coords[np.argsort(large_lon_pixel_hg_coords.lon.to(u.deg).value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All failed conversion pixels\n",
    "pixel_hg_coords = raw_detected_hg_coords[failed_detect_idxs]\n",
    "pixel_hg_coords\n",
    "\n",
    "# HG\n",
    "# nan_idx = np.argmax(pixel_hg_coords.lon.to(u.deg).value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct B-Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPH_B0 keyword: [deg] Disk center solar latitude at DATE-AVG\n",
    "# Yields a lat offset in HG Stonyhurst coordinates\n",
    "he_map.center\n",
    "he_map.reference_coordinate\n",
    "he_map.center.observer.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth observer HGLT_OBS keyword\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "earth_header['HGLT_OBS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limb Size Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonyhurst_frame = frames.HeliographicStonyhurst(obstime=he_map.date)\n",
    "\n",
    "num_points = 100\n",
    "lon_value = -50 * u.deg\n",
    "lat_value = 0 * u.deg\n",
    "constant_lon = SkyCoord(lon_value, np.linspace(-90, 90, num_points) * u.deg,\n",
    "                        frame=stonyhurst_frame)\n",
    "constant_lat = SkyCoord(np.linspace(-90, 90, num_points) * u.deg, lat_value,\n",
    "                        frame=stonyhurst_frame)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection=he_map)\n",
    "\n",
    "# north = SkyCoord(0 * u.deg, 10 * u.deg, frame=\"heliographic_stonyhurst\")\n",
    "# offset_frame = NorthOffsetFrame(north=north)\n",
    "# overlay = ax.get_coords_overlay(offset_frame)\n",
    "# overlay[0].set_ticks(spacing=30. * u.deg)\n",
    "# overlay.grid(ls='--', color='blue')\n",
    "\n",
    "ax.plot_coord(constant_lon, color=\"lightblue\")\n",
    "ax.plot_coord(constant_lat, color=\"tomato\")\n",
    "# he_map.draw_grid(axes=ax, grid_spacing=10*u.deg)\n",
    "he_map.draw_limb(axes=ax)\n",
    "he_map.plot(axes=ax, vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "pixel_lons = detected_hg_coords.lon.to(u.rad).value\n",
    "pixel_lats = detected_hg_coords.lat.to(u.rad).value - B0.to(u.rad).value\n",
    "pixel_areas = A_per_square_px/(np.cos(pixel_lons)*np.cos(pixel_lats))\n",
    "pixel_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Off Limb Coords: \n",
    "# https://docs.sunpy.org/en/stable/generated/api/sunpy.coordinates.utils.get_limb_coordinates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMB_FACTOR = 1.5\n",
    "\n",
    "# def get_nso_sunpy_map(fits_file):\n",
    "#     \"\"\"Retrieve a Sunpy map with a Helioprojective Cartesian\n",
    "#     coordinate system and the first data array in a SOLIS VSM FITS file.\n",
    "    \n",
    "#     Args\n",
    "#         fits_file: path to FITS file\n",
    "#     Returns\n",
    "#         Sunpy map object.\n",
    "#     \"\"\"\n",
    "with fits.open(fits_file) as hdu_list:\n",
    "    header = hdu_list[-1].header\n",
    "    num_data_arrays = header.get('NAXIS3')\n",
    "    \n",
    "    if not num_data_arrays:\n",
    "        data = hdu_list[-1].data\n",
    "    else:\n",
    "        data = hdu_list[-1].data[0]\n",
    "\n",
    "# # Apply absolute value of coordinate change per pixel such that\n",
    "# # Solar-X is positive\n",
    "# header['CDELT1'] = abs(header['CDELT1'])\n",
    "\n",
    "# Helioprojective Cartesian coordinates must have\n",
    "# arcsec units for further processing. Warning messages\n",
    "# will appear but the map will be produced successfully.\n",
    "if (header['WCSNAME'] == 'Helioprojective-cartesian'\n",
    "    and header['CUNIT1'] != 'arcsec'):\n",
    "    pass\n",
    "    \n",
    "# Heliocentric Cartesian coordinates must have zero\n",
    "# centered coordinates\n",
    "if (header['WCSNAME'] == 'Heliocentric-cartesian (approximate)'\n",
    "    and (header['CRVAL1'] != 0 or header['CRVAL2'] != 0)):\n",
    "    print((f'Failed to convert {fits_file} into a Sunpy map.')\n",
    "            + ('Coordinates were Heliocentric but were not ')\n",
    "            + ('zero centered.'))\n",
    "    pass\n",
    "    \n",
    "# Specify Earth-based observer for solar radius, distance to Sun,\n",
    "# and Heliographic coordinates to avoid warning messages due to\n",
    "# missing keywords\n",
    "earth_hp_coords = frames.Helioprojective(\n",
    "    header['CRVAL1']*u.arcsec, header['CRVAL2']*u.arcsec,\n",
    "    observer='earth', obstime=header['DATE-OBS'],\n",
    ")\n",
    "earth_header = sunpy.map.make_fitswcs_header(data, earth_hp_coords)\n",
    "for earth_coord_key in ['DSUN_OBS', 'HGLN_OBS', 'HGLT_OBS']:\n",
    "    header[earth_coord_key] = earth_header[earth_coord_key]\n",
    "\n",
    "# Enlarge solar radius by a factor to account for larger apparent solar\n",
    "# limb in He I observations\n",
    "header['RSUN_REF'] = (100 + LIMB_FACTOR)/100 * earth_header['RSUN_REF']\n",
    "\n",
    "# Change primary World Coordinate System from Heliocentric Cartesian\n",
    "# to Helioprojective Cartesian for Sunpy to create map\n",
    "if header['WCSNAME'] == 'Heliocentric-cartesian (approximate)':\n",
    "    \n",
    "    # Cartesian coordinate units\n",
    "    coord_u1 = u.Unit(header['CUNIT1'])\n",
    "    coord_u2 = u.Unit(header['CUNIT2'])\n",
    "    \n",
    "    # Convert center pixel coordinates from distance to angle\n",
    "    hc_coords = frames.Heliocentric(\n",
    "        header['CRVAL1']*coord_u1,\n",
    "        header['CRVAL2']*coord_u2, z=0*u.m,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    "    hp_coords = hc_coords.transform_to(earth_hp_coords)\n",
    "    header['CRVAL1'] = hp_coords.Tx.value\n",
    "    header['CRVAL2'] = hp_coords.Ty.value\n",
    "    \n",
    "    # Convert change per pixel from distance to angle\n",
    "    hc_delta_coords = frames.Heliocentric(\n",
    "        header['CDELT1']*coord_u1,\n",
    "        header['CDELT2']*coord_u2, z=0*u.m,\n",
    "        observer='earth', obstime=header['DATE-OBS']\n",
    "    )\n",
    "    hp_delta_coords = hc_delta_coords.transform_to(earth_hp_coords)\n",
    "    header['CDELT1'] = hp_delta_coords.Tx.value\n",
    "    header['CDELT2'] = hp_delta_coords.Ty.value\n",
    "    \n",
    "    # Modify keywords\n",
    "    header['WCSNAME'] = 'Helioprojective-cartesian'\n",
    "    header['CTYPE1'] = 'HPLN-TAN'\n",
    "    header['CTYPE2'] = 'HPLT-TAN'\n",
    "    header['CUNIT1'] = 'arcsec'\n",
    "    header['CUNIT2'] = 'arcsec'\n",
    "\n",
    "    # Remove error causing keywords indicate presence of\n",
    "    # coordinate transformation\n",
    "    header.pop('PC1_1')\n",
    "    header.pop('PC2_2')\n",
    "        \n",
    "he_map = sunpy.map.Map(data, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(he_map.rsun_meters/u.solRad).to(u.dimensionless_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnetogram Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = fig.add_subplot(121, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "for contour in ensemble_map.contour(0):\n",
    "    ax.plot_coord(contour, color='yellow')\n",
    "\n",
    "# Mask out data\n",
    "reprojected_mag_map.mask = (ensemble_map.data < 1)\n",
    "ax = fig.add_subplot(122, projection=reprojected_mag_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-0.1, vmax=0.1, cmap='coolwarm')\n",
    "\n",
    "reprojected_mag_map.mask = None\n",
    "# Red: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Flux: Compare official function with decomposed to investigate failed retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_flux = detect.get_unsigned_open_flux(\n",
    "    ensemble_map, reprojected_mag_map, confidence_level=0\n",
    ")\n",
    "f'{open_flux:.7e} Wb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "\n",
    "if confidence_level <= 0:\n",
    "    confidence_level = 1e-3\n",
    "    \n",
    "A_per_square_px = detect.get_A_per_square_px(ensemble_map)\n",
    "        \n",
    "detected_hg_coords, failed_coord_idxs = detect.get_detected_hg_coords(\n",
    "    ensemble_map, confidence_level\n",
    ")\n",
    "\n",
    "pixel_areas = detect.get_pixel_areas(\n",
    "    ensemble_map, A_per_square_px, detected_hg_coords\n",
    ")\n",
    "\n",
    "# Magnetic field strength per detected pixel\n",
    "# Flip upside down to align Sunpy coordinates and Numpy indices\n",
    "detected_idxs = np.where(np.flipud(ensemble_map.data) >= confidence_level)\n",
    "pixel_B_LOS = reprojected_mag_map.data[detected_idxs]*u.G\n",
    "\n",
    "# Remove pixels with failed coordinate conversion\n",
    "pixel_B_LOS = np.delete(pixel_B_LOS, failed_coord_idxs)\n",
    "\n",
    "# Remove pixels with failed magnetic data retrieval\n",
    "failed_mag_idxs = np.where(np.isnan(pixel_B_LOS))\n",
    "pixel_B_LOS = np.delete(pixel_B_LOS, failed_mag_idxs)\n",
    "\n",
    "\n",
    "pixel_areas = np.delete(pixel_areas, failed_mag_idxs)\n",
    "    \n",
    "unsigned_open_flux = np.sum(np.abs(pixel_B_LOS)*pixel_areas).to(u.Wb)\n",
    "\n",
    "unsigned_open_flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed Magnetic Retrieval Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(failed_mag_idxs):\n",
    "    failed_hp_coords = detected_hp_coords[failed_mag_idxs]\n",
    "    failed_pixel_pairs = ensemble_map.world_to_pixel(failed_hp_coords)\n",
    "else:\n",
    "    print('No points matched condition')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "failed_point_color = '#1ed950'\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(reprojected_mag_map.data, vmin=-50, vmax=50, cmap='gray')\n",
    "if np.any(failed_mag_idxs):\n",
    "    ax.scatter(failed_pixel_pairs.x.value, failed_pixel_pairs.y.value,\n",
    "               color=failed_point_color)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax = fig.add_subplot(122, projection=he_map)\n",
    "reprojected_mag_map.plot(axes=ax, vmin=-50, vmax=50)\n",
    "reprojected_mag_map.draw_grid(axes=ax)\n",
    "for contour in ensemble_map.contour(confidence_level):\n",
    "    ax.plot_coord(contour, color='yellow')\n",
    "\n",
    "if np.any(failed_mag_idxs):\n",
    "    ax.plot_coord(failed_hp_coords, 'o', color=failed_point_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.CDM.fracstat import *\n",
    "\n",
    "def get_fractal_D(img, scale_range):\n",
    "    # Compute fractal dimension\n",
    "    \n",
    "    scales, n_filled = box_counting(img, scale_range, f=1.1)[:2]\n",
    "    if np.any(n_filled == 0):\n",
    "        D = np.nan\n",
    "    else:\n",
    "        fit = power_law(scales, n_filled, scale_range)[0]\n",
    "        D = -fit[0]\n",
    "    \n",
    "    return D\n",
    "    \n",
    "\n",
    "def plot_fractal_D(img, scale_range, title_var, contours=False):\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    if contours:\n",
    "        ax.contour(img, cmap='gray')\n",
    "    \n",
    "    ax.set_title(title_var)\n",
    "    \n",
    "    # Plot full range\n",
    "    img_row_num = img.shape[0]\n",
    "    full_range = [min([5, scale_range[0]]), max(img_row_num, scale_range[1])]\n",
    "    full_scales, full_n_filled = box_counting(img, scale_range=full_range, f=1.1)[:2]\n",
    "    \n",
    "    ax = fig.add_subplot(212)\n",
    "    full_X = full_scales/img_row_num\n",
    "    ax.loglog(full_X, full_n_filled, c='k')\n",
    "    \n",
    "    # Compute fractal dimension in specified range and plot selected range\n",
    "    scales, n_filled = box_counting(img, scale_range, f=1.1)[:2]\n",
    "    X = scales/img_row_num\n",
    "    fit, cov = power_law(X, n_filled)\n",
    "    predict = np.poly1d(fit)\n",
    "    D = -fit[0]\n",
    "    D_err = np.sqrt(cov[0,0])\n",
    "    \n",
    "    X_range = np.array(scale_range)/img_row_num\n",
    "    ax.loglog(X_range, 10**predict(np.log10(X_range)), c='C0', linewidth=2)\n",
    "    ax.vlines(X_range, ymin=0, ymax=1e6, linestyles='--', colors='k')\n",
    "    \n",
    "    ax.set_xlabel(r'$\\epsilon$')\n",
    "    ax.set_ylabel('Box Number Containing Boundary')\n",
    "    ax.set_title(f'Fractal Dimension: {D:.3f} +/- {D_err:.4f}')\n",
    "    ax.set_ylim([1,1e5])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = measure.find_contours(ch_mask_data)\n",
    "ch_boundary_data = np.zeros(ch_mask_data.shape)\n",
    "\n",
    "for contour in contours:\n",
    "    contour = contour.astype(int)\n",
    "    ch_boundary_data[contour[:,0], contour[:,1]] = 1\n",
    "\n",
    "fig = plot_fractal_D(\n",
    "    ch_boundary_data,\n",
    "    scale_range=[10, 500],\n",
    "    title_var=f'SE Disk Radius: {morph_radius_dist} Mm',\n",
    "    contours=True\n",
    ")\n",
    "# plt.savefig(f'{OUTPUT_DIR}Fractal/{percent_of_peak}_{morph_radius_dist}_Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fractal_D(ch_boundary_data, scale_range=[10, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_fractal_D(\n",
    "    ch_mask_data,\n",
    "    scale_range=[10, 500],\n",
    "    # scale_range=[10, np.min(ch_mask_data.shape)],\n",
    "    title_var=f'SE Disk Radius: {morph_radius_dist} Mm'\n",
    ")\n",
    "plt.savefig(f'{OUTPUT_DIR}Fractal/{morph_radius_dist}_Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process Outcomes vs Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80, 90, 100, 110]\n",
    "area_percent_df_by_method_list = []\n",
    "mad_by_thresh_by_method_list = []\n",
    "\n",
    "for pre_process_save_dir in ['v0_1/', 'v0_4/']:\n",
    "    pre_process_save_dir = out_dir + PREPROCESS_DIR + pre_process_save_dir\n",
    "    \n",
    "    area_percent_df = detect.get_thresh_outcome_time_series_dfs(\n",
    "        HE_DATE_LIST, percent_of_peak_list, HE_DIR, pre_process_save_dir\n",
    "    )[1]\n",
    "    area_percent_df_by_method_list.append(area_percent_df)\n",
    "    mad_by_thresh_by_method_list.append(\n",
    "        detect.get_mad_by_confidences(area_percent_df, percent_of_peak_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(percent_of_peak_list))\n",
    "threshold_label_list = [\n",
    "    f'{thresh_level}% of Peak Threshold'\n",
    "    for thresh_level in percent_of_peak_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(8,6))\n",
    "\n",
    "plt.bar(x_ticks - 0.2, mad_by_thresh_by_method_list[0], width=0.2, label='v0.3')\n",
    "plt.bar(x_ticks, mad_by_thresh_by_method_list[1], width=0.2, label='Band Pass')\n",
    "plt.bar(x_ticks + 0.2, mad_by_thresh_by_method_list[2], width=0.2, label='Rescaling')\n",
    "plt.xticks(x_ticks, threshold_label_list, rotation=10)\n",
    "plt.ylabel(f'MAD of Detected Area Percentage (%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Outcomes vs Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare outcomes between confidence levels and/or methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = DETECT_DIR + '_Outcome_Comparison/' + DATE_DIR\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "confidence_level_list = [0, 35, 65, 95]\n",
    "# confidence_level_list = list(range(0,96,5))\n",
    "\n",
    "# version_dirs = ['v0_3/', 'Band_Pass/', 'Rescale/', 'Rescale_Center/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/', 'v0_4/']\n",
    "# version_dirs = ['v0_3/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Single/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Unipolar']\n",
    "version_dirs = ['v0_1', 'v0_2', 'v0_3', 'v0_4', 'v0_5']\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "method_list = ['Bright & Coherent Mask', 'Ensemble', 'Smoothness',\n",
    "               'Consistency', 'Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.9,0.91,0.2)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(0,1,0.05)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Band Pass', 'v0.3 Design + Rescale',\n",
    "# #               'v0.3 Design + Rescale & Center']\n",
    "# method_list = ['v0.1', 'v0.2', 'v0.3', 'v0.4']\n",
    "\n",
    "# cl_dx_list = [-0.1, 0.1]\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Rescale']\n",
    "# # method_list = ['v0.3', 'v0.4']\n",
    "# method_list = ['v0.4 Single', 'v0.4 Ensemble']\n",
    "\n",
    "# cl_dx_list = [-0.2, 0, 0.2]\n",
    "# method_list = ['v0.3', 'v0.3 Design + Rescale', 'v0.4']\n",
    "\n",
    "cmap = colormaps['viridis']\n",
    "color_list = cmap(np.linspace(0, 0.75, len(confidence_level_list)))\n",
    "# cmap = colormaps['plasma_r']\n",
    "# color_list = cmap(np.linspace(0.25, 1, len(confidence_level_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.5 Compute Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_df_by_method_list = []\n",
    "autocorr_by_conf_by_method_list = []\n",
    "mad_by_conf_by_method_list = []\n",
    "norm_mad_by_conf_by_method_list = []\n",
    "\n",
    "\n",
    "for version_dir in version_dirs:\n",
    "    DETECTION_NPY_SAVE_DIR = os.path.join(DETECT_DIR, version_dir, 'Saved_npy_Files/')\n",
    "    \n",
    "    outcome_time_series_dict = detect.get_outcome_time_series_dict(\n",
    "        HE_DATE_LIST, confidence_level_list, DETECTION_NPY_SAVE_DIR\n",
    "    )\n",
    "    area_percent_df_by_method_list.append(\n",
    "        outcome_time_series_dict['area_percent']\n",
    "    )\n",
    "    \n",
    "    autocorr_by_confidences = [\n",
    "        outcome_time_series_dict['area'][cl].autocorr()\n",
    "        for cl in confidence_level_list\n",
    "    ]\n",
    "    autocorr_by_conf_by_method_list.append(autocorr_by_confidences)\n",
    "    out = detect.get_mad_by_confidences(\n",
    "        outcome_time_series_dict['area'], confidence_level_list\n",
    "    )\n",
    "    mad_by_confidences, norm_mad_by_confidences = out\n",
    "    mad_by_conf_by_method_list.append(mad_by_confidences)\n",
    "    norm_mad_by_conf_by_method_list.append(norm_mad_by_confidences)\n",
    "    print(f'Outcomes computed for {version_dir}')\n",
    "\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]\n",
    "autocorr_file = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}.npy'\n",
    "np.save(autocorr_file, np.array(autocorr_by_conf_by_method_list),\n",
    "        allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(confidence_level_list))\n",
    "confidence_label_list = [\n",
    "    f'{confidence_level}% Confidence'\n",
    "    for confidence_level in confidence_level_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(9,6))\n",
    "for mad_by_confidences, cl_dx, method, color in zip(\n",
    "    mad_by_conf_by_method_list, cl_dx_list, method_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, mad_by_confidences, width=0.2,\n",
    "            label=method, color=color)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Method Comparison')\n",
    "plt.xticks(x_ticks, confidence_label_list, rotation=10)\n",
    "plt.ylabel(f'MAD of Detected Area (Mm^2)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = np.arange(len(confidence_level_list))\n",
    "confidence_label_list = [\n",
    "    f'{confidence_level}% Confidence'\n",
    "    for confidence_level in confidence_level_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(9,6))\n",
    "for norm_mad_by_confidences, cl_dx, method, color in zip(\n",
    "    norm_mad_by_conf_by_method_list, cl_dx_list, method_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, norm_mad_by_confidences, width=0.2,\n",
    "            label=method, color=color)\n",
    "    \n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Method Comparison')\n",
    "plt.xticks(x_ticks, confidence_label_list, rotation=10)\n",
    "plt.ylim([0, 50])\n",
    "plt.ylabel(f'Normalized MAD of Detected Area (%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Outcomes on Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "\n",
    "# v0.5.1\n",
    "p_start, p_step, p_num = (70, 10, 5)\n",
    "r_start, r_step, r_num = (8, 4, 4)\n",
    "\n",
    "# # vY\n",
    "# p_start, p_step, p_num = (60, 7.5, 7)\n",
    "# r_start, r_step, r_num = (8, 2, 6)\n",
    "\n",
    "PERCENTS_OF_PEAK = np.arange(p_start, p_start + p_num*p_step, p_step)\n",
    "MORPH_RADII = np.arange(r_start, r_start + r_num*r_step, r_step)\n",
    "\n",
    "# thresh_step = 5\n",
    "# radius_step = 2\n",
    "# PERCENTS_OF_PEAK = np.arange(50,86,thresh_step)\n",
    "# # PERCENTS_OF_PEAK = np.array(range(65,101,thresh_step))\n",
    "# # PERCENTS_OF_PEAK = np.array(range(45,101,thresh_step))\n",
    "# MORPH_RADII = np.arange(14,25,radius_step)\n",
    "\n",
    "print(PERCENTS_OF_PEAK)\n",
    "print(MORPH_RADII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_area_for_date_str_1D(percent_of_peak, he_date_str):\n",
    "#     \"\"\"Retrieve detected area percentages for the specified date.\n",
    "#     \"\"\"\n",
    "#     he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "#     he = detect.pre_process_he_v0_4(he_map.data)\n",
    "#     ch_mask_data = detect.get_ch_mask(\n",
    "#         he, percent_of_peak, MORPH_RADIUS\n",
    "#     )\n",
    "#     ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "#     return detect.get_area(ch_mask_map, 0)[0]\n",
    "\n",
    "\n",
    "def get_area_for_date_str(percent_of_peak, morph_radius, he_date_str):\n",
    "    \"\"\"Retrieve detected area percentages for specified dates.\n",
    "    \"\"\"\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    he = detect.pre_process_v0_4(he_map.data)\n",
    "    ch_mask_data = detect.get_ch_mask(\n",
    "        he, percent_of_peak, morph_radius\n",
    "    )\n",
    "    ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "    return detect.get_open_area(ch_mask_map, 0)\n",
    "    \n",
    "\n",
    "def get_outcomes(percent_of_peak, morph_radius):\n",
    "    \"\"\"Retrieve segmentation map outcomes with specified design\n",
    "    variables over time.\n",
    "    \n",
    "    Args\n",
    "        percent_of_peak_list: list of float percentage values\n",
    "            at which to take threshold\n",
    "        morph_radius_list: list of int pixel number for radius of disk \n",
    "            structuring element in morphological operations\n",
    "    Returns\n",
    "        Array of median area percentage.\n",
    "        Autocorrelation and normalized MAD of the detected area time series.\n",
    "    \"\"\"\n",
    "    print((percent_of_peak, morph_radius), end='  ')\n",
    "    area_tuple_list = [\n",
    "        get_area_for_date_str(percent_of_peak, morph_radius, he_date_str)\n",
    "        for he_date_str in HE_DATE_LIST\n",
    "    ]\n",
    "    area_percent_list = [\n",
    "        area_tuple[0] for area_tuple in area_tuple_list\n",
    "    ]\n",
    "    area_list = [\n",
    "        area_tuple[1] for area_tuple in area_tuple_list\n",
    "    ]\n",
    "    # percent_of_peak = design_vars\n",
    "    # print((design_vars, MORPH_RADIUS))\n",
    "    # area_percent_list = [\n",
    "    #     get_area_for_date_str_1D(percent_of_peak, he_date_str)\n",
    "    #     for he_date_str in HE_DATE_LIST\n",
    "    # ]\n",
    "    # area_percent_list = OPTIMIZER.get_area_list(design_vars)\n",
    "    \n",
    "    area_percent_median = np.median(area_percent_list)\n",
    "    autocorr = pd.Series(area_list).autocorr()\n",
    "    \n",
    "    # Compute normalized MAD of detected area\n",
    "    mad = np.median(np.abs(area_list - np.median(area_list)))\n",
    "    if np.median(area_list) == 0:\n",
    "        norm_mad = 0\n",
    "    else:\n",
    "        norm_mad = mad/np.median(area_list)*100\n",
    "    \n",
    "    return np.array([area_percent_median, autocorr, norm_mad])\n",
    "\n",
    "get_vect_outcomes = np.vectorize(get_outcomes, signature='(),()->(3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Expensive computation: 31 min for 1 month, 48 nodes)\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Optionally overwrite existing files\n",
    "if os.path.isfile(save_file) and not overwrite:\n",
    "    sys.exit(f'{file_date_str} data already exists.')\n",
    "\n",
    "num_nodes = len(PERCENTS_OF_PEAK)*len(PERCENTS_OF_PEAK)\n",
    "print(f'Evaluating outcomes for {num_nodes} nodes')\n",
    "\n",
    "# ~1min/step\n",
    "X, Y = np.meshgrid(PERCENTS_OF_PEAK, MORPH_RADII)\n",
    "outcome_array = get_vect_outcomes(X, Y)\n",
    "\n",
    "np.save(save_file, outcome_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcome maps\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps_L/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "area_percent_median_array_L = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps_R/'\n",
    "save_file = f'{output_dir}{file_date_str}.npy'\n",
    "area_percent_median_array_R = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "\n",
    "area_percent_median_array = np.hstack([\n",
    "    area_percent_median_array_L[:,:4], area_percent_median_array_R\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_outcomes_for_date_str(he_date_str, percent_of_peak, morph_radius_dist):\n",
    "    \"\"\"Retrieve detected area for specified dates.\n",
    "    \"\"\"\n",
    "    pre_process_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "    pre_processed_map = sunpy.map.Map(pre_process_file)\n",
    "\n",
    "    # Obtain segmentation mask, sunpy map, and boundaries --------------------\n",
    "    ch_mask_data = detect.get_ch_mask_list_vY(\n",
    "        pre_processed_map, [percent_of_peak], [morph_radius_dist]\n",
    "    )[0]\n",
    "    ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), pre_processed_map.meta)\n",
    "    \n",
    "    contours = measure.find_contours(ch_mask_data)\n",
    "    ch_boundary_data = np.zeros(ch_mask_data.shape)\n",
    "\n",
    "    for contour in contours:\n",
    "        contour = contour.astype(int)\n",
    "        ch_boundary_data[contour[:,0], contour[:,1]] = 1\n",
    "    \n",
    "    # Compute outcomes -------------------------------------------------------\n",
    "    area_percent, open_area = detect.get_open_area(ch_mask_map, 0)\n",
    "    fractal_D = get_fractal_D(ch_boundary_data, scale_range=[10, 500])\n",
    "    \n",
    "    return area_percent, open_area, fractal_D\n",
    "\n",
    "\n",
    "def get_outcomes_v0_5_1(percent_of_peak, morph_radius_dist):\n",
    "    \"\"\"Retrieve segmentation map outcomes with specified design\n",
    "    variables over time.\n",
    "    \n",
    "    Args\n",
    "        percent_of_peak: float percentage measured from the zero\n",
    "            value up to or beyond the histogram value\n",
    "        morph_radius_dist: float distances in Mm for radius of\n",
    "            disk structuring element in morphological operations\n",
    "    Returns\n",
    "        Array of median area percentage and autocorrelation of the detected\n",
    "            area time series.\n",
    "    \"\"\"\n",
    "    print((percent_of_peak, morph_radius_dist), end='  ')\n",
    "    outcomes_by_dates = [\n",
    "        get_contour_outcomes_for_date_str(\n",
    "            he_date_str, percent_of_peak, morph_radius_dist\n",
    "        )\n",
    "        for he_date_str in HE_DATE_LIST\n",
    "    ]\n",
    "    area_percent_list = [\n",
    "        date_outcomes[0] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    area_list = [\n",
    "        date_outcomes[1] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    fractal_D_list = [\n",
    "        date_outcomes[2] for date_outcomes in outcomes_by_dates\n",
    "    ]\n",
    "    \n",
    "    area_percent_median = np.median(area_percent_list)\n",
    "    autocorr = pd.Series(area_list).autocorr()\n",
    "    fractal_D_median = np.nanmedian(fractal_D_list)\n",
    "    \n",
    "    return np.array([area_percent_median, autocorr, fractal_D_median])\n",
    "\n",
    "get_vect_outcomes_v0_5_1 = np.vectorize(get_outcomes_v0_5_1, signature='(),()->(3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Optionally overwrite existing files\n",
    "if os.path.isfile(save_file) and not overwrite:\n",
    "    sys.exit(f'{file_date_str} data already exists.')\n",
    "\n",
    "num_nodes = len(PERCENTS_OF_PEAK)*len(PERCENTS_OF_PEAK)\n",
    "print(f'Evaluating outcomes for {num_nodes} nodes and {num_maps} dates')\n",
    "\n",
    "# ~1min/step\n",
    "X, Y = np.meshgrid(PERCENTS_OF_PEAK, MORPH_RADII)\n",
    "outcome_array = get_vect_outcomes_v0_5_1(X, Y)\n",
    "\n",
    "np.save(save_file, outcome_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "upper_level = np.ceil(np.max(area_percent_median_array))\n",
    "# levels = [0, 0.1, 0.5]\n",
    "# levels.extend(list(np.linspace()))\n",
    "step = 2\n",
    "levels = np.arange(0,upper_level + step,step)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII,\n",
    "             area_percent_median_array, levels, cmap='plasma')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Median Detected Area Percentage (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Area_fill_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0.5.1 Design\n",
    "percent_of_peak_design = [70, 70, 80, 90]\n",
    "morph_radius_design = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # v0.5.1 KPVT Design\n",
    "# percent_of_peak_design = [80, 80, 90, 100]\n",
    "# morph_radius_design = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# # vY Design 1\n",
    "# percent_of_peak_design = [62, 68, 73, 80]\n",
    "# morph_radius_design = [   11, 13,  8, 10]\n",
    "\n",
    "# # vY Design 2\n",
    "# percent_of_peak_design = [85, 73, 95, 85]\n",
    "# morph_radius_design = [   10, 14, 10, 14]\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "upper_level = np.ceil(np.max(area_percent_median_array))\n",
    "step = 1\n",
    "levels_1 = np.arange(0,5 + step,step)\n",
    "step = 2\n",
    "levels_2 = np.arange(6,upper_level + step,step)\n",
    "levels = np.hstack((levels_1, levels_2))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "cp = plt.contour(PERCENTS_OF_PEAK, MORPH_RADII,\n",
    "                 area_percent_median_array, levels, cmap='plasma')\n",
    "plt.clabel(cp, fontsize=14)\n",
    "\n",
    "plt.title('Median Detected Area Percentage (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "# plt.savefig(f'{output_dir}Area_{file_date_str}.jpg')\n",
    "\n",
    "plt.scatter(percent_of_peak_design, morph_radius_design, color='k')\n",
    "plt.savefig(f'{output_dir}Area_Point_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "d_area_d_thresh = np.diff(area_percent_median_array, axis=1)/p_step\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK[:-1], MORPH_RADII,\n",
    "             d_area_d_thresh, cmap='bone')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('$\\partial(Median\\ Area)/\\partial(Threshold)$ (%/%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Partial_Thresh_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "d_area_d_radius = np.diff(area_percent_median_array, axis=0)/r_step\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII[:-1],\n",
    "             d_area_d_radius, cmap='bone')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('$\\partial(Median\\ Area)/\\partial(Radius)$ (%/px)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Partial_Radius_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_area_d_radius[np.where(MORPH_RADII == 10), np.where(PERCENTS_OF_PEAK == 115)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not available in updated versions\n",
    "norm_mad_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "upper_level = np.ceil(np.max(norm_mad_array))\n",
    "levels = [0, 1, 10]\n",
    "levels.extend(list(np.linspace(15,upper_level,7)))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, norm_mad_array, levels)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Normalized MAD of Detected Area (%)')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')\n",
    "plt.savefig(f'{output_dir}Norm_MAD_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_array = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, -autocorr_array)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Negative Autocorrelation')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "# plt.ylabel('SE Disk Radius (px)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Autocorr_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal_D_bound_median_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "# fractal_D_bound_median_array = np.load(save_file, allow_pickle=True)[:,:,3]\n",
    "lower_level = np.floor(np.nanmin(fractal_D_bound_median_array)*10)/10\n",
    "upper_level = np.ceil(np.nanmax(fractal_D_bound_median_array)*10)/10\n",
    "step = 0.025\n",
    "levels = np.arange(lower_level, upper_level + step, step)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, fractal_D_bound_median_array, levels)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Fractal Dimension')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')\n",
    "plt.savefig(f'{output_dir}Fractal_Dim_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), fractal_D_bound_median_array\n",
    ")\n",
    "f((8,110))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs verification\n",
    "interp_points = 500\n",
    "num_bins = 17\n",
    "\n",
    "area_interp = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), area_percent_median_array\n",
    ")\n",
    "fractal_D_interp = interpolate.RegularGridInterpolator(\n",
    "    (MORPH_RADII, PERCENTS_OF_PEAK), fractal_D_bound_median_array\n",
    ")\n",
    "\n",
    "interp_percents_of_peak = np.linspace(np.min(PERCENTS_OF_PEAK), np.max(PERCENTS_OF_PEAK), interp_points)\n",
    "interp_morph_radii = np.linspace(np.min(MORPH_RADII), np.max(MORPH_RADII), interp_points)\n",
    "XX, YY = np.meshgrid(interp_percents_of_peak, interp_morph_radii)\n",
    "\n",
    "interpolated_area = area_interp((YY, XX))\n",
    "interpolated_fractal_D = fractal_D_interp((YY, XX))\n",
    "\n",
    "bins = np.linspace(np.min(area_percent_median_array), np.max(area_percent_median_array), num_bins)\n",
    "binned_idx_array = np.digitize(interpolated_area, bins) - 1\n",
    "\n",
    "X = np.zeros_like(binned_idx_array, dtype=float)\n",
    "\n",
    "for bin_num in range(num_bins):\n",
    "    bin_idxs = np.where(binned_idx_array == bin_num)\n",
    "    bin_fractal_D = interpolated_fractal_D[bin_idxs]\n",
    "    \n",
    "    if not np.any(bin_fractal_D):\n",
    "        continue\n",
    "\n",
    "    X[bin_idxs] = bin_fractal_D - np.mean(bin_fractal_D)\n",
    "    \n",
    "# X = X\n",
    "\n",
    "bound = max([np.max(X), np.abs(np.min(X))])\n",
    "levels = np.linspace(-bound, bound, 15)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(interp_percents_of_peak, interp_morph_radii, X,\n",
    "             levels, cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Binned Difference Fractal Boundary Dimension')\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (Mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func_v0_5_1(norm_mad_array, area_percent_median_array, mu, M):\n",
    "    penalty = mu*(area_percent_median_array - M)**2 + 1/area_percent_median_array\n",
    "    obj_func_array = norm_mad_array + penalty\n",
    "    return obj_func_array\n",
    "\n",
    "def get_optim_vars(mu, M):\n",
    "    obj_func_array = obj_func(NORM_MAD_ARRAY, AREA_PERCENT_MEDIAN_ARRAY, mu, M)\n",
    "    xi, yi = np.unravel_index(np.argmin(obj_func_array), obj_func_array.shape)\n",
    "    return np.array([PERCENTS_OF_PEAK[yi], MORPH_RADII[xi]])\n",
    "\n",
    "get_vect_optim_vars = np.vectorize(get_optim_vars, signature='(),()->(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area penalty weight\n",
    "mu = 1\n",
    "\n",
    "# Target median area\n",
    "M = 3\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "norm_mad_array = np.load(save_file, allow_pickle=True)[:,:,2]\n",
    "obj_func_array = obj_func_v0_5_1(norm_mad_array, area_percent_median_array, mu, M)\n",
    "\n",
    "z = obj_func_array[~np.isinf(obj_func_array)]\n",
    "lev_exp = np.linspace(\n",
    "    np.log10(z.min()), np.log10(z.max()), 10\n",
    ")\n",
    "levels = np.power(10, lev_exp)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "cs = plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, obj_func_array, levels, \n",
    "                  norm=colors.LogNorm(), cmap='gray')\n",
    "plt.colorbar(cs, format=ticker.FuncFormatter(lambda x, pos: f'{x:.1f}'))\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title(f'Objective Function | $\\mu$: {mu} $M$: {M}%')\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')\n",
    "plt.savefig(f'{output_dir}Obj_Func_Mu{mu}_M{M}_{file_date_str}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func_array[np.where(MORPH_RADII == 20), np.where(PERCENTS_OF_PEAK == 115)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_step = 5\n",
    "M_step = 0.1\n",
    "mu = np.arange(0, 500 + mu_step, mu_step)\n",
    "M = np.arange(2, 6 + M_step, M_step)\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(mu, M)\n",
    "AREA_PERCENT_MEDIAN_ARRAY = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "NORM_MAD_ARRAY = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "optim_var_array = get_vect_optim_vars(X, Y)\n",
    "optim_var_pairs = optim_var_array.reshape((np.prod(optim_var_array.shape[:2]), 2))\n",
    "unique_optim_var_pairs, optim_counts = np.unique(\n",
    "    optim_var_pairs,axis=0, return_counts=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(unique_optim_var_pairs[:,0], unique_optim_var_pairs[:,1], s=250,\n",
    "            c=optim_counts, cmap='turbo')\n",
    "plt.colorbar()\n",
    "plt.xticks(PERCENTS_OF_PEAK)\n",
    "plt.yticks(MORPH_RADII)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Optimal Design Variables with Varied $\\mu,\\ M$')\n",
    "plt.xlabel('Threshold Relative to Mode (%)')\n",
    "plt.ylabel('SE Disk Radius (px)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(mu, M, optim_var_array[:,:,0], cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Optimal Threshold Relative to Mode (%)')\n",
    "plt.xlabel('Penalty Weight $mu$')\n",
    "plt.ylabel('Target Median Area Percentage $M$ (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func_v0_5_2(autocorr_array, area_percent_median_array, mu, M):\n",
    "    C = mu*(area_percent_median_array - M)**2\n",
    "    obj_func_array = -autocorr_array + C\n",
    "    return obj_func_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target median area\n",
    "M = 3\n",
    "\n",
    "output_dir = DETECTION_IMAGE_DIR + 'Outcome_Maps/Obj_Func_v0_5_2/'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "area_percent_median_array = np.load(save_file, allow_pickle=True)[:,:,0]\n",
    "autocorr_array = np.load(save_file, allow_pickle=True)[:,:,1]\n",
    "\n",
    "# Area penalty weight\n",
    "for mu in np.logspace(-5,0,20):\n",
    "    obj_func_array = obj_func_v0_5_2(autocorr_array, area_percent_median_array, mu, M)\n",
    "    if np.min(obj_func_array) < 0:\n",
    "        obj_func_array = obj_func_array + np.abs(np.min(obj_func_array)) + 0.1\n",
    "\n",
    "    z = obj_func_array[~np.isinf(obj_func_array)]\n",
    "    lev_exp = np.linspace(\n",
    "        np.log10(z.min()), np.log10(z.max()), 10\n",
    "    )\n",
    "    levels = np.power(10, lev_exp)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    \n",
    "    cs = plt.contourf(PERCENTS_OF_PEAK, MORPH_RADII, obj_func_array, levels, \n",
    "                    norm=colors.LogNorm(), cmap='gray')\n",
    "    plt.colorbar(cs, format=ticker.FuncFormatter(lambda x, pos: f'{x:.1f}'))\n",
    "\n",
    "    plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "    plt.title(f'Objective Function | $\\mu$: {mu:.2e} $M$: {M}%')\n",
    "    plt.xlabel('Threshold Relative to Mode (%)')\n",
    "    plt.ylabel('SE Disk Radius (px)')\n",
    "    plt.savefig(f'{output_dir}Mu{mu:.6f}_M{M}_{file_date_str}'.replace('.','_') + '.jpg')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f'Mu: {mu:.2e} saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect.write_ensemble_video(output_dir, fps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v0.1 Heat Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CH Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = f'{DETECTION_IMAGE_DIR}Outcome_Maps/{he_date_str}_maps.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CH Mask List (Expensive computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower threshold accepts more and lets morphology carry the load in selection and removal\n",
    "thresh_step = 5\n",
    "radius_step = 1\n",
    "percent_of_peak_list = list(np.arange(70,106,thresh_step))\n",
    "morph_radius_list = list(np.arange(6,21,radius_step))\n",
    "\n",
    "# List of CHS masks for different files with varied parameters\n",
    "all_ch_mask_list = [\n",
    "    detect.get_ch_mask(pre_processed_map_data, percent_of_peak, morph_radius)\n",
    "    for percent_of_peak, morph_radius\n",
    "    in zip(percent_of_peak_list, morph_radius_list)\n",
    "]\n",
    "\n",
    "save_list = [he_date_str, percent_of_peak_list, morph_radius_list, all_ch_mask_list]\n",
    "np.save(save_file, np.array(save_list, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CH Mask List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list = np.load(save_file, allow_pickle=True)\n",
    "date_str = save_list[0]\n",
    "percent_of_peak_list = save_list[1]\n",
    "morph_radius_list = save_list[2]\n",
    "all_ch_mask_list = save_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [f'{percent_of_peak:d}% of Peak | {radius:d}px Radius'\n",
    "              for percent_of_peak in percent_of_peak_list\n",
    "              for radius in morph_radius_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat_map(outcome_list, title, percent_of_peak_list, morph_radius_list, \n",
    "                  color_scale='Magma'):\n",
    "    # Reverse order to facilitate plotting\n",
    "    y_axis_list = morph_radius_list.copy()\n",
    "    y_axis_list.reverse()\n",
    "    \n",
    "    outcome_map = np.flipud(np.reshape(\n",
    "        outcome_list, (len(percent_of_peak_list),len(morph_radius_list))).T)\n",
    "\n",
    "    fig = px.imshow(outcome_map, \n",
    "                    labels=dict(x='Threshold Level as Percent of Peak (%)',\n",
    "                                y='SE Disk Radius (px)'),\n",
    "                    x=percent_of_peak_list, y=y_axis_list,\n",
    "                    aspect='auto', color_continuous_scale=color_scale)\n",
    "    fig.update_layout(title=title, width=700)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def plot_heat_map_band(outcome_list, heat_map_title, lower_bound, upper_bound,\n",
    "                       percent_of_peak_list, morph_radius_list, \n",
    "                       array, all_ch_masks_list, title_list, color_scale='Magma'):\n",
    "    edit_outcome_list = outcome_list.copy()\n",
    "    \n",
    "    # Index list of outcomes within bounds \n",
    "    idx_list = [i for i in range(len(edit_outcome_list)) \n",
    "              if edit_outcome_list[i] >= lower_bound and edit_outcome_list[i] <= upper_bound]\n",
    "    num_ch_masks = len(idx_list)\n",
    "    \n",
    "    if not num_ch_masks:\n",
    "        print('No masks in outcome range')\n",
    "        return\n",
    "    \n",
    "    max_outcome = max(edit_outcome_list)\n",
    "    # Highlight outcomes within bounds\n",
    "    for i in idx_list:\n",
    "        edit_outcome_list[i] = 2*max_outcome\n",
    "\n",
    "    plot_heat_map(edit_outcome_list, heat_map_title,\n",
    "                  percent_of_peak_list, morph_radius_list, color_scale)\n",
    "    \n",
    "    if num_ch_masks == 1:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        ax = fig.add_subplot()\n",
    "        \n",
    "        outcome_idx = idx_list[0]\n",
    "        ax.imshow(array, cmap=plt.cm.afmhot)\n",
    "        ax.contour(all_ch_masks_list[outcome_idx], linewidths=0.5, cmap=plt.cm.gray)\n",
    "        ax.set_title(title_list[outcome_idx], fontsize=18)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=num_ch_masks, figsize=(6*num_ch_masks, 6))\n",
    "        ax = axes.ravel()\n",
    "    \n",
    "        for i in range(num_ch_masks):\n",
    "            outcome_idx = idx_list[i]\n",
    "            ax[i].imshow(array, cmap=plt.cm.afmhot)\n",
    "            ax[i].contour(all_ch_masks_list[outcome_idx], linewidths=0.5, cmap=plt.cm.gray)\n",
    "            ax[i].set_title(title_list[outcome_idx], fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_list = detect.get_px_percent_list(all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Segmented Area Percentage'\n",
    "\n",
    "plot_heat_map(\n",
    "    area_percent_list, heat_map_title, percent_of_peak_list, morph_radius_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(area_percent_list[:-1], bins=30)\n",
    "ax.set_title(f'{date_str} Segmented Area Bins', fontsize=20)\n",
    "ax.set_xlabel('Area Percentage', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Area Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 16\n",
    "upper_bound = 18\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 6.8\n",
    "upper_bound = 7.3\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 5.4\n",
    "upper_bound = 6\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 3.2\n",
    "upper_bound = 3.3\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound}% Segmented Area'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    area_percent_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CH Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ch_list = detect.get_num_CH_list(all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Segmented Hole Number'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map(\n",
    "    num_ch_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(num_ch_list, bins=30, range=(0,25))\n",
    "ax.set_title(f'{date_str} Segmented Hole Number Bins', fontsize=20)\n",
    "ax.set_xlabel('Segemented Hole Number', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Hole Number Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 17\n",
    "upper_bound = 17\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 11\n",
    "upper_bound = 11\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 9\n",
    "upper_bound = 9\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Segmented Holes'\n",
    "color_scale = 'Aggrnyl'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    num_ch_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Tail Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ch_lower_tail_width_list(array, ch_mask_list): \n",
    "    \"\"\"Retrieve the average of the histogram lower tail width across CHs\n",
    "    for each segmentation in a list.\n",
    "    \n",
    "    Args\n",
    "        array: image to process\n",
    "        ch_mask_list: binary coronal holes mask list\n",
    "    Returns\n",
    "        List of mean histogram tail widths of CHs detected in segmentations.\n",
    "    \"\"\"\n",
    "    labeled_ch_list = [ndimage.label(ch_mask)[0]\n",
    "                          for ch_mask in ch_mask_list]\n",
    "    num_ch_list = [get_num_ch(ch_mask) for ch_mask in ch_mask_list]\n",
    "    \n",
    "    # List of average lower tail widths across all CH's of each segmentaion\n",
    "    lower_tail_width_list = []\n",
    "    \n",
    "    count = 0\n",
    "    for labeled_ch_mask, num_ch in zip(labeled_ch_list, num_ch_list):\n",
    "\n",
    "        map_data_by_ch = get_map_data_by_ch(array, labeled_ch_mask, num_ch)\n",
    "    \n",
    "        ch_mask_lower_tail_width_list = get_ch_lower_tail_widths(map_data_by_ch)\n",
    "        \n",
    "        mean_lower_tail_width = np.mean(ch_mask_lower_tail_width_list)\n",
    "        \n",
    "        lower_tail_width_list.append(mean_lower_tail_width)\n",
    "        count = count + 1\n",
    "        \n",
    "    return lower_tail_width_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tail_width_list = detect.get_ch_lower_tail_width_list(he, all_ch_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map(\n",
    "    lower_tail_width_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.hist(lower_tail_width_list, bins=30, range=(10,40))\n",
    "ax.set_title(f'{date_str} Mean CH Tail Width Bins', fontsize=20)\n",
    "ax.set_xlabel('Mean CH Tail Width', fontsize=18)\n",
    "ax.set_ylabel('Number of Masks in Tail Width Bin', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat Map Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 29\n",
    "upper_bound = 29.5\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    lower_tail_width_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 24.5\n",
    "upper_bound = 25\n",
    "\n",
    "heat_map_title = f'{date_str} {lower_bound}-{upper_bound} Mean CH Tail Width'\n",
    "color_scale = 'ice'\n",
    "\n",
    "plot_heat_map_band(\n",
    "    lower_tail_width_list, heat_map_title, lower_bound, upper_bound,\n",
    "    percent_of_peak_list, morph_radius_list, \n",
    "    he, all_ch_mask_list, title_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_title = f'{date_str} Multiplied Metrics'\n",
    "color_scale = 'dense_r'\n",
    "\n",
    "outcome_list = list(np.array(area_percent_list)*np.array(num_ch_list)*np.array(lower_tail_width_list))\n",
    "\n",
    "plot_heat_map(\n",
    "    outcome_list, heat_map_title, percent_of_peak_list, morph_radius_list, color_scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Date Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Histograms\n",
    "\n",
    "Histograms of candidate regions as stratified by confidence level, such as Figure 4 of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1+\n",
    "\n",
    "Requires setting DETECTION_VERSION_DIR = DETECT_DIR + 'v0_5_1_No_Thresh/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes_by_all_date_ch_v0_5_1(cl_list):\n",
    "    \"\"\"Retrieve outcomes per CH in ensemble maps in all datetimes\n",
    "    at specified confidence levels from ensemble maps.\n",
    "    \n",
    "    See get_outcomes for retrieved outcomes.\n",
    "    \n",
    "    Args\n",
    "        cl_list: list of float confidence levels at which\n",
    "            to threshold ensemble maps for computing outcomes\n",
    "    Returns\n",
    "        Dataframes of outcomes by confidence level over time.\n",
    "    \"\"\"\n",
    "    # Dictionaries for outcomes of distinct CHs at varied confidence levels\n",
    "    outcomes_by_all_ch_dict = {}\n",
    "    for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "        outcomes_by_all_ch_dict[outcome_key] = {cl:[] for cl in cl_list}\n",
    "\n",
    "\n",
    "    for he_date_str in HE_DATE_LIST:\n",
    "        \n",
    "        # Extract saved ensemble map array and convert to Sunpy map\n",
    "        ensemble_file = f'{DETECTION_MAP_SAVE_DIR}{he_date_str}_ensemble_map.fits'\n",
    "        ensemble_map = sunpy.map.Map(ensemble_file)\n",
    "        \n",
    "        # Extract saved He I observation\n",
    "        he_fits_file = DATA_FITS_FORMAT.format(\n",
    "            data_dir=HE_DIR, date_str=he_date_str\n",
    "        )\n",
    "        he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "        he_map_data = np.flipud(he_map.data)\n",
    "        \n",
    "        # Extract saved processed magnetogram\n",
    "        mag_date_str = prepare_data.get_nearest_date_str(\n",
    "            MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "        )\n",
    "        reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                                 + f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "        reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "        \n",
    "        # Extract single date outcomes by CH ---------------------------------\n",
    "        # Applied at varied confidence levels, then extending a main\n",
    "        # dictionary with outcomes by CH from all dates\n",
    "        \n",
    "        # List of varied confidence levels for outcomes per CH detected\n",
    "        # at the given or greater confidence levels\n",
    "        outcome_by_ch_dict_by_cl = [\n",
    "            detect.get_outcomes_by_ch(ensemble_map, he_map_data,\n",
    "                                      reprojected_mag_map, cl)\n",
    "            for cl in cl_list\n",
    "        ]\n",
    "        \n",
    "        # Dictionary of outcomes holding dictionaries of\n",
    "        # confidence levels for outcomes per CH\n",
    "        single_date_outcome_dict = {}\n",
    "        for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "            single_date_outcome_dict[outcome_key] = {\n",
    "                cl:outcome_by_ch_dict[outcome_key] for cl, outcome_by_ch_dict\n",
    "                in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "            }\n",
    "        \n",
    "        # Extend main outcomes per CH dictionary by confidence level\n",
    "        for cl in cl_list:\n",
    "            for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "                outcomes_by_all_ch_dict[outcome_key][cl].extend(\n",
    "                    single_date_outcome_dict[outcome_key][cl]\n",
    "                )\n",
    "            \n",
    "    return outcomes_by_all_ch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_list = [50, 75, 90]\n",
    "# cl_list = [0]\n",
    "unipolarity_confidence = True\n",
    "outcomes_by_all_ch_dict = get_outcomes_by_all_date_ch_v0_5_1(cl_list)\n",
    "\n",
    "# Number of candidate regions in 1st confidence level\n",
    "len(outcomes_by_all_ch_dict[detect.OUTCOME_KEY_LIST[0]][cl_list[0]])\n",
    "\n",
    "# Number of total candidate regions, set cl_list=[0]\n",
    "# len(outcomes_by_all_ch_dict[detect.OUTCOME_KEY_LIST[0]][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to v0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcomes_by_all_date_ch(cl_list):\n",
    "    \"\"\"Retrieve outcomes per CH in ensemble maps in all datetimes\n",
    "    at specified confidence levels from ensemble maps.\n",
    "    \n",
    "    See get_outcomes for retrieved outcomes.\n",
    "    \n",
    "    Args\n",
    "        cl_list: list of float confidence levels at which\n",
    "            to threshold ensemble maps for computing outcomes\n",
    "    Returns\n",
    "        Dataframes of outcomes by confidence level over time.\n",
    "    \"\"\"\n",
    "    # Dictionaries for outcomes of distinct CHs at varied confidence levels\n",
    "    area_dict = {cl:[] for cl in cl_list}\n",
    "    lat_dict = {cl:[] for cl in cl_list}\n",
    "    lon_dict = {cl:[] for cl in cl_list}\n",
    "    unsigned_flux_dict = {cl:[] for cl in cl_list}\n",
    "    signed_flux_dict = {cl:[] for cl in cl_list}\n",
    "    mag_skew_dict = {cl:[] for cl in cl_list}\n",
    "    unipolarity_dict = {cl:[] for cl in cl_list}\n",
    "\n",
    "    for he_date_str in HE_DATE_LIST:\n",
    "        \n",
    "        # Extract He I observation\n",
    "        he_file = f'{HE_DIR}{he_date_str}.fts'\n",
    "        he_map = prepare_data.get_nso_sunpy_map(he_file)\n",
    "        if not he_map:\n",
    "            print(f'{he_date_str} He I observation extraction failed.')\n",
    "            continue\n",
    "        \n",
    "        # Extract saved ensemble map\n",
    "        ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "        ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "        ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "        \n",
    "        # Extract saved processed magnetograms\n",
    "        mag_date_str = prepare_data.get_nearest_date_str(\n",
    "            MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "        )\n",
    "        reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                                 + f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "        reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "        \n",
    "        # Outcomes per CH detected at the given or greater\n",
    "        # confidence levels\n",
    "        outcome_by_ch_dict_by_cl = [\n",
    "            detect.get_outcomes_by_ch(ensemble_map, pre_processed_map,\n",
    "                                      reprojected_mag_map, cl)\n",
    "            for cl in cl_list\n",
    "        ]\n",
    "        area_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['area'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        lat_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['cm_lat'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        lon_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['cm_lon'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        unsigned_flux_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['unsigned_flux'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        signed_flux_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['signed_flux'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        mag_skew_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['mag_skew'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        unipolarity_single_date_dict = {\n",
    "            cl:outcome_by_ch_dict['unipolarity'] for cl, outcome_by_ch_dict\n",
    "            in zip(cl_list, outcome_by_ch_dict_by_cl)\n",
    "        }\n",
    "        \n",
    "        # Extend outcomes per CH\n",
    "        for cl in cl_list:\n",
    "            area_dict[cl].extend(\n",
    "                area_single_date_dict[cl]\n",
    "            )\n",
    "            lat_dict[cl].extend(\n",
    "                lat_single_date_dict[cl]\n",
    "            )\n",
    "            lon_dict[cl].extend(\n",
    "                lon_single_date_dict[cl]\n",
    "            )\n",
    "            unsigned_flux_dict[cl].extend(\n",
    "                unsigned_flux_single_date_dict[cl]\n",
    "            )\n",
    "            signed_flux_dict[cl].extend(\n",
    "                signed_flux_single_date_dict[cl]\n",
    "            )\n",
    "            mag_skew_dict[cl].extend(\n",
    "                mag_skew_single_date_dict[cl]\n",
    "            )\n",
    "            unipolarity_dict[cl].extend(\n",
    "                unipolarity_single_date_dict[cl]\n",
    "            )\n",
    "            \n",
    "    return area_dict, lat_dict, lon_dict, \\\n",
    "        unsigned_flux_dict, signed_flux_dict, \\\n",
    "        mag_skew_dict, unipolarity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_list = [0, 35, 65, 95]\n",
    "outcome_dicts = get_outcomes_by_all_date_ch(cl_list)\n",
    "area_dict, lat_dict, lon_dict = outcome_dicts[:3]\n",
    "unsigned_flux_dict, signed_flux_dict = outcome_dicts[3:5]\n",
    "mag_skew_dict, unipolarity_dict = outcome_dicts[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat/Lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine Lat/Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_factor = 0.6\n",
    "\n",
    "total_width = 13.5\n",
    "lon_width = 8\n",
    "\n",
    "# # Uncomment for longitude -----------------------------------------------\n",
    "# lat_or_lon = 'lon'\n",
    "# outcome_dict = outcomes_by_all_ch_dict['cm_lon']\n",
    "# plt.figure(figsize=(lon_width*sz_factor,5*sz_factor), dpi=300)\n",
    "# plt.title('Longitude Histogram')\n",
    "# plt.ylabel('Number of CH Detections')\n",
    "# plt.xlabel(r'sin($\\phi$)')\n",
    "# orientation = 'vertical'\n",
    "# plt.xlim([-1,1])\n",
    "# plt.ylim([0,50])\n",
    "# plt.xticks([-1, -0.5, 0, 0.5, 1])\n",
    "# loc = 'upper left'\n",
    "\n",
    "# Uncomment for latitude -----------------------------------------------\n",
    "lat_or_lon = 'lat'\n",
    "outcome_dict = outcomes_by_all_ch_dict['cm_lat']\n",
    "plt.figure(\n",
    "    figsize=((total_width - lon_width)*sz_factor,5*sz_factor), dpi=300\n",
    ")\n",
    "plt.title('Latitude Histogram')\n",
    "plt.ylabel(r'sin($\\theta$)')\n",
    "plt.xlabel('Number of CH Detections')\n",
    "orientation = 'horizontal'\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([0,70])\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "loc = 'upper right'\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "bins = np.arange(-1,1.01,0.125)\n",
    "for cl, color, linestyle in zip(cl_list, color_list, ['-', '--', '-.']):\n",
    "    sine_angle = np.sin(np.deg2rad(outcome_dict[cl]))\n",
    "    \n",
    "    if unipolarity_confidence:\n",
    "        label = fr'$U \\geq${cl/100}'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        sine_angle, bins, histtype='step',\n",
    "        color='white', edgecolor=color,\n",
    "        linestyle=linestyle, linewidth=2,\n",
    "        orientation=orientation\n",
    "    )\n",
    "    plt.plot([2,3], [2,3], color=color, linestyle=linestyle, label=label)\n",
    "    \n",
    "# Commented out for cm_lat in paper\n",
    "# plt.legend(loc=loc)\n",
    "\n",
    "plt.savefig(\n",
    "    PAPER_PLOT_DIR + f'center-of-mass-hists-{lat_or_lon}.jpeg',\n",
    "    bbox_inches='tight'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_PLOT_DIR = 'paper/paper_plots/2024_08_plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = outcomes_by_all_ch_dict['cm_lon']\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Longitude Histogram')\n",
    "plt.ylabel('Number of CH Detections')\n",
    "plt.xlabel('Longitude (deg)')\n",
    "orientation = 'vertical'\n",
    "plt.xlim([-90,90])\n",
    "plt.ylim([0,50])\n",
    "\n",
    "# plt.ylim([0,120])\n",
    "\n",
    "# outcome_dict = outcomes_by_all_ch_dict['cm_lat']\n",
    "# plt.figure(figsize=(7,6))\n",
    "# plt.title('Latitude Histogram')\n",
    "# plt.ylabel('Latitude (deg)')\n",
    "# plt.xlabel('Number of CH Detections')\n",
    "# orientation = 'horizontal'\n",
    "# plt.ylim([-90,90])\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "\n",
    "bins = np.arange(-90,90.1,10)\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'>= {cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color,\n",
    "        orientation=orientation, label=label\n",
    "    )\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsigned Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = outcomes_by_all_ch_dict['unsigned_flux']\n",
    "\n",
    "bin_min = np.log10(np.min(outcome_dict[0]))\n",
    "bin_max = np.ceil(np.log10(np.max(outcome_dict[0])))\n",
    "bins = 10**(np.linspace(bin_min,bin_max,25))\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xscale('log')\n",
    "plt.title('Unsigned Flux Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unsigned Open Flux (Wb)')\n",
    "\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'{cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color,\n",
    "        orientation=orientation, label=label\n",
    "    )\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unipolarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unipolarity_threshold = 0.5\n",
    "smooth_percentile_bounds = [0, 50, 80]\n",
    "\n",
    "# Outcomes for all candidates (>=0% confidence)\n",
    "cl = 0\n",
    "unipolarity_list = outcomes_by_all_ch_dict['unipolarity'][cl]\n",
    "grad_median_list = outcomes_by_all_ch_dict['grad_median'][cl]\n",
    "\n",
    "# Smoothness percentile by candidate\n",
    "# Mapped in [0,100) and reversed order from gradient median quanitfying roughness\n",
    "smooth_percentiles = 100 - stats.rankdata(grad_median_list)/len(grad_median_list)*100\n",
    "\n",
    "# Stratify candidate CH unipolarity by percentiles of smoothness\n",
    "u_by_smooth_pct_dict = {}\n",
    "    \n",
    "for smooth_pct_bound in smooth_percentile_bounds:\n",
    "    \n",
    "    candidate_u_above_smooth_pct_list = [\n",
    "        unipolarity for unipolarity, smooth_percentile\n",
    "        in zip(unipolarity_list, smooth_percentiles)\n",
    "        if smooth_percentile >= smooth_pct_bound\n",
    "    ]\n",
    "    u_by_smooth_pct_dict[smooth_pct_bound] = candidate_u_above_smooth_pct_list\n",
    "\n",
    "bins = np.arange(0,1.01,0.05)\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(smooth_percentile_bounds)))\n",
    "\n",
    "line_styles = ['-', '--', '-.']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Unipolarity Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unipolarity')\n",
    "plt.xticks([0,0.25,0.5,0.75,1])\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,140])\n",
    "\n",
    "for smooth_pct, color, linestyle in zip(\n",
    "    smooth_percentile_bounds, color_list, line_styles):\n",
    "    \n",
    "    plt.hist(\n",
    "        u_by_smooth_pct_dict[smooth_pct], bins, histtype='step',\n",
    "        color='white', edgecolor=color,\n",
    "        linestyle=linestyle, linewidth=3,\n",
    "        label=f'{smooth_pct}th % Smoothness'\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.vlines([unipolarity_threshold, unipolarity_threshold], ymin=-10, ymax=150,\n",
    "           linestyles=['--'], color='k', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_candidates = len(u_by_smooth_pct_dict[0])\n",
    "\n",
    "u_among_all_candidates = np.array(u_by_smooth_pct_dict[0])\n",
    "num_bipolar_candidates = np.count_nonzero(u_among_all_candidates < 0.5)\n",
    "num_unipolar_candidates = np.count_nonzero(u_among_all_candidates >= 0.5)\n",
    "\n",
    "f'Bipolar candidate fraction: {num_bipolar_candidates/num_candidates*100:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_among_smooth_candidates = np.array(u_by_smooth_pct_dict[50])\n",
    "\n",
    "num_bipolar_smooth_candidates = np.count_nonzero(u_among_smooth_candidates < 0.5)\n",
    "num_unipolar_smooth_candidates = np.count_nonzero(u_among_smooth_candidates >= 0.5)\n",
    "\n",
    "('Bipolar, unsmooth candidate fraction: '\n",
    " f'{num_bipolar_smooth_candidates/num_bipolar_candidates*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Unpolar, smooth candidate fraction: '\n",
    " f'{num_unipolar_smooth_candidates/num_unipolar_candidates*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u_among_all_candidates) - len(u_among_unsmooth_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_dict = unipolarity_dict\n",
    "\n",
    "# bins = np.linspace(0,1,25)\n",
    "bins = np.arange(0,1.01,0.05)\n",
    "\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 1, len(cl_list)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Unipolarity Histogram')\n",
    "plt.ylabel('Number of CH Candidates')\n",
    "plt.xlabel('Unipolarity')\n",
    "\n",
    "for cl, color in zip(cl_list, color_list):\n",
    "    if unipolarity_confidence:\n",
    "        label = f'{cl/100} Unipolarity'\n",
    "    else:\n",
    "        label = f'{cl}th % Smoothness'\n",
    "    \n",
    "    plt.hist(\n",
    "        outcome_dict[cl], bins, color=color, label=label\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level_list = [50, 75, 95]\n",
    "# confidence_level_list = [1, 50, 75, 95]\n",
    "# confidence_level_list = [0, 35, 65, 95]\n",
    "outcome_time_series_dict = detect.get_outcome_time_series_dict_v0_5_1(\n",
    "    HE_DATE_LIST, confidence_level_list, DETECTION_MAP_SAVE_DIR\n",
    ")\n",
    "outcome_time_series_dict['area_percent'][50].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "region_num_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'Region_Number/',\n",
    "    'num_ch', 'viridis', 'Detected CH Number'\n",
    ")\n",
    "px_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Px_Percentage/',\n",
    "    'px_percent', 'plasma', 'Detected Pixel Percentage (%)'\n",
    ")\n",
    "area_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area_Percentage/',\n",
    "    'area_percent', 'plasma', 'Detected Area Percentage (%)'\n",
    ")\n",
    "area_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area/',\n",
    "    'area', 'plasma', 'Detected Area (Mm^2)'\n",
    ")\n",
    "out_dir, outcome_key, cmap, ylabel = area_percent_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to display Heliographic reprojected maps. Leave as False\n",
    "hg_reproject = False\n",
    "\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for euv_date_str in EUV_DATE_LIST[:1]:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}EUV{euv_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'EUV {euv_date_str} comparison already exists.'))\n",
    "        continue\n",
    "    \n",
    "    he_date_str = prepare_data.get_latest_date_str(\n",
    "        HE_DATE_LIST, selected_date_str=euv_date_str\n",
    "    )\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    if hg_reproject:\n",
    "        fig = plt.figure(figsize=(22, 10))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    plot_detection.plot_he_neutral_lines_euv_v0_5_1(\n",
    "        fig, he_date_str, mag_date_str, euv_date_str,\n",
    "        nrows=2, hg_reproject=False\n",
    "    )\n",
    "    \n",
    "    if hg_reproject:\n",
    "        ax = fig.add_subplot(2, 7, (8, 14))\n",
    "    else:\n",
    "        ax = fig.add_subplot(2, 3, (4, 6))\n",
    "    \n",
    "    plot_detection.plot_outcome_df_vs_time(\n",
    "        ax, outcome_time_series_dict[outcome_key], he_date_str, cmap,\n",
    "        ylabel, #ylim=[0,3.75]\n",
    "    )\n",
    "    \n",
    "    # # Save plot\n",
    "    # plt.savefig(comparison_img_file)\n",
    "    # plt.close(fig)\n",
    "    # print(f'{euv_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level_list = [1, 50, 75, 95]\n",
    "# confidence_level_list = [0, 35, 65, 95]\n",
    "outcome_time_series_dict = detect.get_outcome_time_series_dict(\n",
    "    HE_DATE_LIST, confidence_level_list, DETECTION_SAVE_DIR\n",
    ")\n",
    "outcome_time_series_dict['area_percent'][50].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "region_num_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'Region_Number/',\n",
    "    'num_ch', 'viridis', 'Detected CH Number'\n",
    ")\n",
    "px_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Px_Percentage/',\n",
    "    'px_percent', 'plasma', 'Detected Pixel Percentage (%)'\n",
    ")\n",
    "area_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area_Percentage/',\n",
    "    'area_percent', 'plasma', 'Detected Area Percentage (%)'\n",
    ")\n",
    "area_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area/',\n",
    "    'area', 'plasma', 'Detected Area (Mm^2)'\n",
    ")\n",
    "out_dir, outcome_key, cmap, ylabel = area_percent_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for euv_date_str in EUV_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}EUV{euv_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'EUV {euv_date_str} comparison already exists.'))\n",
    "        continue\n",
    "    \n",
    "    he_date_str = prepare_data.get_latest_date_str(\n",
    "        HE_DATE_LIST, selected_date_str=euv_date_str\n",
    "    )\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    # Extract saved ensemble map array and convert to Sunpy map\n",
    "    ensemble_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map_data = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    ensemble_map = sunpy.map.Map(np.flipud(ensemble_map_data), he_map.meta)\n",
    "    ensemble_map.plot_settings['cmap'] = colormaps['magma']\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plot_detection.plot_he_neutral_lines_euv_comparison(\n",
    "        fig, he_date_str, mag_date_str, euv_date_str,\n",
    "        ROTATED_MAG_SAVE_DIR, nrows=2\n",
    "    )\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))\n",
    "    plot_detection.plot_outcome_df_vs_time(\n",
    "        ax, outcome_time_series_dict[outcome_key], he_date_str, cmap, ylabel\n",
    "    )\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(comparison_img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{euv_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_time_series_dict = detect.get_outcome_time_series_dict_v0_1(\n",
    "    HE_DATE_LIST, DETECTION_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "region_num_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'Region_Number/',\n",
    "    'num_ch', 'viridis', 'Detected CH Number'\n",
    ")\n",
    "px_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Px_Percentage/',\n",
    "    'px_percent', 'plasma', 'Detected Pixel Percentage (%)'\n",
    ")\n",
    "area_percent_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area_Percentage/',\n",
    "    'area_percent', 'plasma', 'Detected Area Percentage (%)'\n",
    ")\n",
    "area_settings = (\n",
    "    DETECTION_IMAGE_DIR + 'EUV_Area/',\n",
    "    'area', 'plasma', 'Detected Area (Mm^2)'\n",
    ")\n",
    "out_dir, outcome_key, cmap, ylabel = area_percent_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for euv_date_str in EUV_DATE_LIST[:1]:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    comparison_img_file = f'{out_dir}EUV{euv_date_str}.jpg'\n",
    "    if os.path.isfile(comparison_img_file) and not overwrite:\n",
    "        print((f'EUV {euv_date_str} comparison already exists.'))\n",
    "        continue\n",
    "\n",
    "    he_date_str = prepare_data.get_latest_date_str(\n",
    "        HE_DATE_LIST, selected_date_str=euv_date_str\n",
    "    )\n",
    "    \n",
    "    # Extract He I observation\n",
    "    he_map = prepare_data.get_nso_sunpy_map(HE_DIR + he_date_str + '.fts')\n",
    "    if not he_map:\n",
    "        print(f'{he_date_str} He I observation extraction failed.')\n",
    "        continue\n",
    "    \n",
    "    # Extract He I observation for mask base and convert to Sunpy map\n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    raw_he = prepare_data.get_image_from_fits(he_fits_file)\n",
    "    he_base_data = np.where(raw_he == raw_he[0,0], np.NaN, raw_he)\n",
    "    he_base_map = sunpy.map.Map(np.flipud(he_base_data), he_map.meta)\n",
    "    \n",
    "    # Extract saved single mask array and convert to Sunpy map\n",
    "    mask_file = f'{DETECTION_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    mask_data = np.load(mask_file, allow_pickle=True)[-1]\n",
    "    mask_map = sunpy.map.Map(np.flipud(mask_data), he_map.meta)\n",
    "    mask_map.plot_settings['cmap'] = colormaps['gray']\n",
    "    \n",
    "    euv_map = sunpy.map.Map(EUV_DIR + euv_date_str + '.fts')\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    plot_detection.plot_he_map(fig, (2, 3, 1), he_map, he_date_str)\n",
    "    \n",
    "    # Plot He I observation with overlayed detection contours\n",
    "    ax = fig.add_subplot(232, projection=he_map)\n",
    "    he_base_map.plot(axes=ax, vmin=-100, vmax=100, title=he_date_str,\n",
    "                     cmap='afmhot')\n",
    "    for contour in mask_map.contour(0):\n",
    "        ax.plot_coord(contour, color='black', linewidth=1)\n",
    "    \n",
    "    plot_detection.plot_euv_map(fig, (2, 3, 3), euv_map, euv_date_str)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))\n",
    "    plot_detection.plot_outcome_series_vs_time(\n",
    "        ax, outcome_time_series_dict[outcome_key], he_date_str, cmap,\n",
    "        ylabel, ylim=[0,3.75]\n",
    "    )\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(comparison_img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{euv_date_str} map comparison saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Time Series Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level_list = [1, 50, 75, 95]\n",
    "num_ch_df, area_percent_df, px_percent_df = detect.get_outcome_time_series_dfs(\n",
    "    HE_DATE_LIST[:5], confidence_level_list, DETECTION_NPY_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = px_percent_df\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_detection.plot_outcome_df_vs_time(\n",
    "    ax, outcome_df, he_date_str, cmap='plasma', ylabel='Detected Pixel Percentage (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = area_percent_df\n",
    "\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_detection.plot_outcome_df_vs_time(\n",
    "    ax, outcome_df, he_date_str, cmap='bone', ylabel='Detected Area Percentage (%)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram Moments vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_stat_list = []\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map_data = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    peak_counts_val = detect.get_peak_counts_loc(\n",
    "        pre_processed_map_data, bins_as_percent=False\n",
    "    )\n",
    "    hist_stat_list.append(\n",
    "        [peak_counts_val, np.nanstd(pre_processed_map_data)]\n",
    "    )\n",
    "\n",
    "# Convert to dataframes\n",
    "datetime_list = [datetime.strptime(he_date_str, DICT_DATE_STR_FORMAT)\n",
    "                 for he_date_str in HE_DATE_LIST]\n",
    "hist_df = pd.DataFrame(\n",
    "    hist_stat_list, columns=['Peak', 'StDev'],\n",
    "    index=datetime_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Histogram_Moments/'\n",
    "cmap = 'plasma'\n",
    "ylabel = 'Histogram Moments'\n",
    "\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    img_file = f'{out_dir}{he_date_str}.jpg'\n",
    "    if os.path.isfile(img_file) and not overwrite:\n",
    "        print((f'He {he_date_str} map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    hist, edges = detect.get_hist(pre_processed_map,\n",
    "                                         bins_as_percent=False)\n",
    "    \n",
    "    ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(231)\n",
    "    ax.set_title(he_date_str)\n",
    "    ax.imshow(pre_processed_map, cmap=plt.cm.gray)\n",
    "    \n",
    "    ax = fig.add_subplot(232)\n",
    "    ax.set_title('Semilog Histogram')\n",
    "    ax.semilogy(edges[1:], hist)\n",
    "    if 'Rescale' in DETECTION_VERSION_DIR:\n",
    "        ax.set_xlim([-1.3, 1.1])\n",
    "        ax.set_ylim([1E2, 5E4])\n",
    "    else:\n",
    "        ax.set_xlim([-110, 110])\n",
    "        ax.set_ylim([1E1, 5E4])\n",
    "    \n",
    "    ax = fig.add_subplot(233)\n",
    "    ax.imshow(ensemble_map, cmap=plt.cm.magma)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))\n",
    "    datetimes = hist_df.index\n",
    "    ax.plot(hist_df['StDev'], label='Standard Deviation', linewidth=3)\n",
    "    ax.plot(hist_df['Peak'], label='Mode', linewidth=3)\n",
    "    \n",
    "    # Vertical line for datetime indicator\n",
    "    vline_datetime = datetime.strptime(he_date_str, DICT_DATE_STR_FORMAT)\n",
    "    min_moment = min(hist_df.min())\n",
    "    max_moment = max(hist_df.max())\n",
    "    ax.vlines(x=[vline_datetime, vline_datetime], ymax=2*max_moment, ymin=0,\n",
    "              colors='k', linestyles='dashed')\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    ax.set_xlim([datetimes[0], datetimes[-1]])\n",
    "    if 'Rescale' in DETECTION_VERSION_DIR:\n",
    "        ax.set_ylim([0.4, 0.8])\n",
    "    else:\n",
    "        ax.set_ylim([0.9*min_moment, 1.1*max_moment])\n",
    "    \n",
    "    ax.legend(reverse=True)\n",
    "\n",
    "    plt.savefig(img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{he_date_str} map saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Process Outcomes vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak_list = [80, 90, 100, 110]\n",
    "num_ch_df, area_percent_df, area_df, px_percent_df = detect.get_thresh_outcome_time_series_dfs(\n",
    "    HE_DATE_LIST, percent_of_peak_list, HE_DIR, PREPROCESS_NPY_SAVE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "out_dir = DETECTION_IMAGE_DIR + 'Thresh_Area_Percentage/'\n",
    "outcome_df = area_percent_df\n",
    "cmap = 'plasma'\n",
    "ylabel = 'Detected Area Percentage (%)'\n",
    "\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for he_date_str in HE_DATE_LIST[24:25]:\n",
    "    \n",
    "    # Optionally overwrite existing files\n",
    "    img_file = f'{out_dir}{he_date_str}.jpg'\n",
    "    if os.path.isfile(img_file) and not overwrite:\n",
    "        print((f'He {he_date_str} map already exists.'))\n",
    "        continue\n",
    "    \n",
    "    pre_process_file = (PREPROCESS_NPY_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.npy')\n",
    "    pre_processed_map = np.load(pre_process_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    hist, edges = detect.get_hist(pre_processed_map,\n",
    "                                         bins_as_percent=False)\n",
    "    \n",
    "    ensemble_file = f'{DETECTION_NPY_SAVE_DIR}{he_date_str}_ensemble_map.npy'\n",
    "    ensemble_map = np.load(ensemble_file, allow_pickle=True)[-1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    ax = fig.add_subplot(231)\n",
    "    ax.set_title(he_date_str)\n",
    "    ax.imshow(pre_processed_map, cmap=plt.cm.gray)\n",
    "    \n",
    "    ax = fig.add_subplot(232)\n",
    "    ax.set_title('Semilog Histogram')\n",
    "    ax.semilogy(edges[1:], hist)\n",
    "    if 'Rescale/' in DETECTION_VERSION_DIR:\n",
    "        ax.set_ylim([1E2, 5E4])\n",
    "    else:\n",
    "        ax.set_xlim([-110, 110])\n",
    "        ax.set_ylim([1E1, 5E4])\n",
    "    \n",
    "    ax = fig.add_subplot(233)\n",
    "    ax.imshow(ensemble_map, cmap=plt.cm.magma)\n",
    "    \n",
    "    ax = fig.add_subplot(2, 3, (4, 6))    \n",
    "    plot_detection.plot_thresh_outcome_vs_time(\n",
    "        ax, outcome_df, he_date_str, cmap, ylabel)\n",
    "\n",
    "    plt.savefig(img_file)\n",
    "    plt.close(fig)\n",
    "    print(f'{he_date_str} map saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Comparison\n",
    "\n",
    "Compare outcomes between versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare outcomes between confidence levels and/or methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = DETECT_DIR + '_Outcome_Comparison/' + DATE_DIR\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# confidence_level_list = [0, 35, 65, 95]\n",
    "confidence_level_list = [0, 50, 80]\n",
    "# confidence_level_list = list(range(0,96,5))\n",
    "\n",
    "# version_dirs = ['v0_3/', 'Band_Pass/', 'Rescale/', 'Rescale_Center/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/']\n",
    "# version_dirs = ['v0_3/', 'Rescale/', 'v0_4/']\n",
    "# version_dirs = ['v0_3/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Single/', 'v0_4/']\n",
    "# version_dirs = ['v0_4_Unipolar']\n",
    "# version_dirs = ['v0_1', 'v0_2', 'v0_3', 'v0_4', 'v0_5']\n",
    "version_dirs = ['v0_1', 'v0_2', 'v0_4', 'v0_5']\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "# method_list = ['Bright & Coherent Mask', 'Ensemble', 'Smoothness',\n",
    "#                'Consistency', 'Unipolarity']\n",
    "\n",
    "cl_dx_list = np.arange(-0.2,0.21,0.2)\n",
    "method_list = ['Single Preliminary Mask', 'Area Ensemble',\n",
    "               'Smoothness Ensemble', 'Unipolarity Ensemble']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.9,0.91,0.2)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(0,1,0.05)\n",
    "# method_list = ['Unipolarity']\n",
    "\n",
    "# cl_dx_list = np.arange(-0.3,0.31,0.2)\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Band Pass', 'v0.3 Design + Rescale',\n",
    "# #               'v0.3 Design + Rescale & Center']\n",
    "# method_list = ['v0.1', 'v0.2', 'v0.3', 'v0.4']\n",
    "\n",
    "# cl_dx_list = [-0.1, 0.1]\n",
    "# # method_list = ['v0.3', 'v0.3 Design + Rescale']\n",
    "# # method_list = ['v0.3', 'v0.4']\n",
    "# method_list = ['v0.4 Single', 'v0.4 Ensemble']\n",
    "\n",
    "# cl_dx_list = [-0.2, 0, 0.2]\n",
    "# method_list = ['v0.3', 'v0.3 Design + Rescale', 'v0.4']\n",
    "\n",
    "# cmap = colormaps['viridis']\n",
    "cmap = colormaps['bone_r']\n",
    "color_list = cmap(np.linspace(0.25, 0.9, len(confidence_level_list)))\n",
    "# cmap = colormaps['plasma_r']\n",
    "# color_list = cmap(np.linspace(0.25, 1, len(confidence_level_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.2-v0.5 Compute Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_df_by_method_list = []\n",
    "autocorr_by_conf_by_method_list = []\n",
    "mad_by_conf_by_method_list = []\n",
    "norm_mad_by_conf_by_method_list = []\n",
    "\n",
    "\n",
    "for version_dir in version_dirs:\n",
    "    detection_save_dir = os.path.join(DETECT_DIR, version_dir, 'Saved_npy_Files/')\n",
    "    \n",
    "    outcome_time_series_dict = detect.get_outcome_time_series_dict(\n",
    "        HE_DATE_LIST, confidence_level_list, detection_save_dir\n",
    "    )\n",
    "    area_percent_df_by_method_list.append(\n",
    "        outcome_time_series_dict['area_percent']\n",
    "    )\n",
    "    \n",
    "    autocorr_by_confidences = [\n",
    "        outcome_time_series_dict['area'][cl].autocorr()\n",
    "        for cl in confidence_level_list\n",
    "    ]\n",
    "    autocorr_by_conf_by_method_list.append(autocorr_by_confidences)\n",
    "    out = detect.get_mad_by_confidences(\n",
    "        outcome_time_series_dict['area'], confidence_level_list\n",
    "    )\n",
    "    mad_by_confidences, norm_mad_by_confidences = out\n",
    "    mad_by_conf_by_method_list.append(mad_by_confidences)\n",
    "    norm_mad_by_conf_by_method_list.append(norm_mad_by_confidences)\n",
    "    print(f'Outcomes computed for {version_dir}')\n",
    "\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]\n",
    "autocorr_file = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}.npy'\n",
    "np.save(autocorr_file, np.array(autocorr_by_conf_by_method_list),\n",
    "        allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v0.5.1 Compute Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_percent_df_by_method_list = []\n",
    "autocorr_by_conf_by_method_list = []\n",
    "\n",
    "\n",
    "for version_dir in version_dirs:\n",
    "    if 'v0_5_1' in version_dir:\n",
    "        detection_save_dir = os.path.join(\n",
    "            DETECT_DIR, version_dir, 'Saved_fits_Files/'\n",
    "        )\n",
    "        outcome_time_series_dict = detect.get_outcome_time_series_dict_v0_5_1(\n",
    "            HE_DATE_LIST, confidence_level_list, detection_save_dir\n",
    "        )\n",
    "    else:\n",
    "        detection_save_dir = os.path.join(\n",
    "            DETECT_DIR, version_dir, 'Saved_npy_Files/'\n",
    "        )\n",
    "        outcome_time_series_dict = detect.get_outcome_time_series_dict(\n",
    "            HE_DATE_LIST, confidence_level_list, detection_save_dir\n",
    "        )\n",
    "    \n",
    "    area_percent_df_by_method_list.append(\n",
    "        outcome_time_series_dict['area_percent']\n",
    "    )\n",
    "    \n",
    "    autocorr_by_confidences = [\n",
    "        outcome_time_series_dict['area'][cl].autocorr()\n",
    "        for cl in confidence_level_list\n",
    "    ]\n",
    "    autocorr_by_conf_by_method_list.append(autocorr_by_confidences)\n",
    "    print(f'Outcomes computed for {version_dir}')\n",
    "\n",
    "descript_list = version_dirs + [f'cl{cl}' for cl in confidence_level_list]\n",
    "autocorr_file = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}.npy'\n",
    "np.save(autocorr_file, np.array(autocorr_by_conf_by_method_list),\n",
    "        allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Variable Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0\n",
    "area_percent_df = area_percent_df_by_method_list[confidence_level]\n",
    "median_area_percent_by_cl = [\n",
    "    np.median(area_percent_df[cl]) for cl in confidence_level_list\n",
    "]\n",
    "\n",
    "x_ticks = np.arange(len(method_list))\n",
    "plt.figure(1, figsize=(9,6))\n",
    "\n",
    "# Loop over confidence levels to plot bars for all methods at once\n",
    "for median_area_percent, cl_dx, color in zip(\n",
    "    median_area_percent_by_cl, cl_dx_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, median_area_percent, width=0.05, color=color)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Design Variable Sweep')\n",
    "plt.ylabel('Median Detected Area Percentage (%)')\n",
    "plt.xlabel('Unipolarity Threshold')\n",
    "plt.xlim([-0.025,1.025])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_file_name = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}'\n",
    "autocorrs_by_cl_by_method = np.load(autocorr_file_name + '.npy', allow_pickle=True)\n",
    "autocorrs_by_method_by_cl = autocorrs_by_cl_by_method.T\n",
    "\n",
    "x_ticks = np.arange(len(method_list))\n",
    "\n",
    "plt.figure(1, figsize=(9,6))\n",
    "\n",
    "# Loop over confidence levels to plot bars for all methods at once\n",
    "for autocorrs_by_method, cl_dx, color in zip(\n",
    "    autocorrs_by_method_by_cl, cl_dx_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, autocorrs_by_method, width=0.05,\n",
    "            color=color)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Design Variable Sweep')\n",
    "plt.ylabel(f'Autocorrelation')\n",
    "plt.xlabel('Unipolarity Threshold')\n",
    "plt.xlim([-0.025,1.025])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation by Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_file_name = f'{out_dir}Autocorr_comp_{\"_\".join(descript_list)}'\n",
    "autocorrs_by_cl_by_method = np.load(autocorr_file_name + '.npy', allow_pickle=True)\n",
    "autocorrs_by_method_by_cl = autocorrs_by_cl_by_method.T\n",
    "\n",
    "x_ticks = np.arange(len(method_list))\n",
    "confidence_label_list = [\n",
    "    f'{confidence_level}% Confidence'\n",
    "    for confidence_level in confidence_level_list\n",
    "]\n",
    "\n",
    "plt.figure(1, figsize=(10,6))\n",
    "\n",
    "# Loop over confidence levels to plot bars for all methods at once\n",
    "for autocorrs_by_method, cl_dx, confidence, color in zip(\n",
    "    autocorrs_by_method_by_cl, cl_dx_list, confidence_label_list, color_list):\n",
    "    plt.bar(x_ticks + cl_dx, autocorrs_by_method, width=0.2,\n",
    "            label=confidence, color=color)\n",
    "\n",
    "plt.suptitle(DATE_RANGE_SUPTITLE)\n",
    "plt.title('Method Comparison')\n",
    "plt.xticks(x_ticks, method_list)\n",
    "plt.ylabel(f'Time Series Autocorrelation')\n",
    "\n",
    "# plt.ylim([0, 0.8])\n",
    "# plt.ylim([-0.1, 0.8])\n",
    "# plt.axhline(0, color='k', linestyle='--')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(autocorr_file_name + '.png')\n",
    "plt.close()\n",
    "print(f'{autocorr_file_name.split(\"/\")[-1]} method comparison saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrs_by_method_by_cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH Labels\n",
    "\n",
    "Saved in: output/Labels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, make_scorer, recall_score, confusion_matrix\n",
    ")\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "LDA_FILE_NAME = 'v1_1_LDA_model.pkl'\n",
    "DISK_VAL = -0.5\n",
    "UNLABELED_VAL = 1.5\n",
    "CH_LABEL_DIR = OUTPUT_DIR + 'Labels/'\n",
    "CH_LABEL_FILE_FORMAT = CH_LABEL_DIR + '{he_date_str}.dat'\n",
    "LABEL_FILE_GLOB_PATTERN = CH_LABEL_DIR + '*.dat'\n",
    "LABEL_CMAP = colors.ListedColormap(\n",
    "    ['black', 'indianred', 'gold', 'forestgreen', 'gray']\n",
    ")\n",
    "\n",
    "cmap_object = colormaps['tab20']\n",
    "black_start_cmap = np.vstack(([0,0,0,1], cmap_object(np.arange(0,20,1))))\n",
    "NUMBERED_CMAP = colors.ListedColormap(black_start_cmap)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "LABEL_OUTCOME_FILE = (\n",
    "    CH_LABEL_DIR + f'{datetime.today().strftime(DICT_DATE_STR_FORMAT)}_outcomes.csv'\n",
    ")\n",
    "LABEL_ID_LIST = [0,0.5,1]\n",
    "NUM_MASKS = 2\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "LABEL_NAME_LIST = ['False', 'Marginal', 'True']\n",
    "LABEL_COLOR_LIST = ['indianred', 'gold', 'forestgreen']\n",
    "MARKER_SIZE_RANGE = [10,20]\n",
    "\n",
    "CUSTOMDATA_COLS = [\n",
    "    'he_date_str', 'mask_idx',\n",
    "    'grad_median', 'unipolarity', 'cm_foreshort',\n",
    "    'cm_lat', 'cm_lon', 'all_cand_idx', 'area'\n",
    "]\n",
    "HOVER_TEMPLATE = (\n",
    "    '%{customdata[0]} | Mask %{customdata[1]}<br>' +\n",
    "    'GM: %{customdata[2]:.3f} | U: %{customdata[3]:.2f} | ' +\n",
    "    'F: %{customdata[4]:.2f}<br>' +\n",
    "    'Lat: %{customdata[5]:.1f} deg | ' +\n",
    "    'Lon: %{customdata[6]:.1f} deg<br>' +\n",
    "    'Idx: %{customdata[7]}'\n",
    "    + '<br>Area: %{customdata[8]:.1e} Mm^2<br>'\n",
    ")\n",
    "\n",
    "def get_numbered_mask_on_disk(pre_processed_map_data, ch_mask):\n",
    "    \"\"\"Retrieve 2D array with numbered candidate regions and a removed\n",
    "    background.\n",
    "    \"\"\"\n",
    "    # Array with number labels per distinct CH\n",
    "    numbered_mask, _ = ndimage.label(ch_mask)\n",
    "    \n",
    "    # Remove background to display CHs on disk\n",
    "    numbered_mask_on_disk = np.where(\n",
    "        np.isnan(pre_processed_map_data), np.nan, numbered_mask\n",
    "    )\n",
    "    return numbered_mask_on_disk\n",
    "\n",
    "\n",
    "def get_labeled_mask_on_disk(pre_processed_map_data, ch_mask,\n",
    "                             ch_label_list, mask_idx, allow_unlabeled):\n",
    "    \"\"\"Retrieve 2D array with candidate regions labeled 0, 0.5, or 1\n",
    "    and a removed background.\n",
    "    \"\"\"\n",
    "    # Array with number labels per distinct CH and number of labels\n",
    "    numbered_mask, num_candidates = ndimage.label(ch_mask)\n",
    "    \n",
    "    # Remove background to display CHs on disk\n",
    "    numbered_mask_on_disk = np.where(\n",
    "        np.isnan(pre_processed_map_data), np.nan, numbered_mask\n",
    "    )\n",
    "    \n",
    "    if allow_unlabeled:\n",
    "        # Extend label list for unlabeled CHs\n",
    "        ch_label_list = (\n",
    "            ch_label_list[:num_candidates]\n",
    "            + [UNLABELED_VAL]*(num_candidates - len(ch_label_list))\n",
    "        )\n",
    "    else:\n",
    "        num_labeled = len([ch_label for ch_label in ch_label_list\n",
    "                           if ch_label != 2])\n",
    "        assert num_labeled >= num_candidates, \\\n",
    "            f'Candidates in mask {mask_idx} have been left unlabeled. ' \\\n",
    "            f'There are {num_candidates} candidates.'\n",
    "    \n",
    "    # Initialize and iterate to assign labels to CHs\n",
    "    labeled_mask_on_disk = np.where(\n",
    "        numbered_mask_on_disk == 0, DISK_VAL, numbered_mask_on_disk\n",
    "    )\n",
    "    for ch_idx in range(num_candidates):\n",
    "        labeled_mask_on_disk = np.where(\n",
    "            labeled_mask_on_disk == ch_idx + 1,\n",
    "            ch_label_list[ch_idx], labeled_mask_on_disk\n",
    "        )\n",
    "    return labeled_mask_on_disk\n",
    "\n",
    "\n",
    "def plot_labeled_chs(he_date_str, pre_processed_map_data, smooth_mag_map_data,\n",
    "                     ch_label_dict, ch_mask_list, numbered_mode):\n",
    "    \"\"\"Retrieve numbered (1-num_candidates) or labeled (True, Marginal, False)\n",
    "    candidate regions from CH masks, then plot them in a row.\n",
    "    \"\"\"\n",
    "    percent_of_peak_list = ch_label_dict['percent_of_peak_list']\n",
    "    morph_radius_list = ch_label_dict['morph_radius_list']\n",
    "    \n",
    "    image_size = 5\n",
    "    num_cols = len(ch_mask_list)\n",
    "    fig = plt.figure(figsize=((image_size+1.5)*num_cols, image_size))\n",
    "    fig.suptitle(he_date_str, fontsize=16)\n",
    "\n",
    "    for mask_idx, ch_mask in enumerate(ch_mask_list):\n",
    "\n",
    "        ax = fig.add_subplot(1, num_cols, mask_idx + 1)\n",
    "        ax.contour(smooth_mag_map_data, levels=0, colors='y')\n",
    "        \n",
    "        if numbered_mode:\n",
    "            numbered_mask_on_disk = get_numbered_mask_on_disk(\n",
    "                pre_processed_map_data, ch_mask\n",
    "            )\n",
    "            im = ax.imshow(numbered_mask_on_disk, NUMBERED_CMAP,\n",
    "                           interpolation='nearest', vmin=-0.5, vmax=20.5)\n",
    "            fig.colorbar(im, ticks=np.arange(0,20.5,4))\n",
    "        else:\n",
    "            ch_label_list = ch_label_dict['label_list_dict'][f'mask_{mask_idx}']\n",
    "            labeled_mask_on_disk = get_labeled_mask_on_disk(\n",
    "                pre_processed_map_data, ch_mask, ch_label_list,\n",
    "                mask_idx, allow_unlabeled=True\n",
    "            )\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                labeled_mask_on_disk, LABEL_CMAP, interpolation='nearest',\n",
    "                vmin=DISK_VAL - 0.25, vmax=UNLABELED_VAL + 0.25\n",
    "            )\n",
    "            cb = fig.colorbar(\n",
    "                im, fraction=0.05, ticks=np.linspace(DISK_VAL, UNLABELED_VAL, 5)\n",
    "            )\n",
    "            cb.ax.set_yticklabels(\n",
    "                ['Disk', '0: False', '0.5: Marginal', '1: True', 'Unlabeled']\n",
    "            )\n",
    "        \n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        ax.axes.yaxis.set_ticks([])\n",
    "        ax.set_title(\n",
    "            (f'{percent_of_peak_list[mask_idx]:d}% of Mode Threshold | '\n",
    "             f'{morph_radius_list[mask_idx]:d}Mm SE Disk Radius')\n",
    "        )\n",
    "\n",
    "\n",
    "def save_ch_label_dict(pre_processed_map_data, he_date_str, ch_mask_list,\n",
    "                       ch_label_dict):\n",
    "    \"\"\"Save a dictionary with information on candidate region labels from \n",
    "    m number of masks to file.\n",
    "    The dictionary contains 4 key, value pairs:\n",
    "        'percent_of_peak_list': list of length m with mask percent_of_peak\n",
    "            design variable values\n",
    "        'morph_radius_list': list of length m with mask morph_radius\n",
    "            design variable values\n",
    "        'label_list_dict': dict with m key, value pairs:\n",
    "            'mask_{i}': list of 0 (False), 0.5 (Marginal), or 1 (True)\n",
    "        'labeled_data_dict': dict with m key, value pairs:\n",
    "            'mask_{i}': 2D array with 0, 0.5, or 1 label values in candidate\n",
    "            regions, -0.5 on disk background, and NaN off-disk.\n",
    "    \"\"\"\n",
    "    # Array with number labels per distinct CH and number of labels\n",
    "    for mask_idx, ch_mask in enumerate(ch_mask_list):\n",
    "        ch_label_list = ch_label_dict['label_list_dict'][f'mask_{mask_idx}']\n",
    "        labeled_mask_on_disk = get_labeled_mask_on_disk(\n",
    "            pre_processed_map_data, ch_mask, ch_label_list, mask_idx,\n",
    "            allow_unlabeled=False\n",
    "        )\n",
    "        ch_label_dict['labeled_data_dict'][f'mask_{mask_idx}'] = labeled_mask_on_disk\n",
    "        \n",
    "    # Save dictionary\n",
    "    if not os.path.isdir(CH_LABEL_DIR):\n",
    "        os.makedirs(CH_LABEL_DIR)\n",
    "    \n",
    "    ch_label_file_name = CH_LABEL_FILE_FORMAT.format(\n",
    "        he_date_str=he_date_str\n",
    "    )\n",
    "    with open(ch_label_file_name, 'wb') as ch_label_file:\n",
    "        pickle.dump(\n",
    "            ch_label_dict, ch_label_file, protocol=pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    \n",
    "    print(f'{he_date_str} Label Dictionary Saved')\n",
    "        \n",
    "    return ch_label_dict\n",
    "\n",
    "\n",
    "def plot_3d_feature_space(labeled_cand_df, camera_eye, gray_out_inspected=False):\n",
    "    \"\"\"Display candidate regions in a 3D feature space, sized by area and colored\n",
    "    by label or, optionally, greyed if it has been marked as inspected.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot candidate region points ----------------------------------------------\n",
    "    for label_id, label, color in zip(\n",
    "        LABEL_ID_LIST, LABEL_NAME_LIST, LABEL_COLOR_LIST):\n",
    "        \n",
    "        label_id_df = labeled_cand_df[labeled_cand_df['label_id'] == label_id]\n",
    "        label_id_df = label_id_df.reset_index(drop=True)\n",
    "        \n",
    "        if gray_out_inspected:\n",
    "            marker_colors = ['gray' if label_id_df['inspected?'][i] else color\n",
    "                            for i in range(len(label_id_df))]\n",
    "        else:\n",
    "            marker_colors = color\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=label_id_df['smooth_quantile'],\n",
    "                # x=label_id_df['grad_median'],\n",
    "                y=label_id_df['unipolarity'],\n",
    "                z=label_id_df['cm_foreshort'], name=label,\n",
    "                mode='markers', marker_color=marker_colors,\n",
    "                marker_size=np.interp(\n",
    "                    label_id_df['area'],\n",
    "                    [np.min(label_id_df['area']), np.max(label_id_df['area'])],\n",
    "                    MARKER_SIZE_RANGE\n",
    "                ),\n",
    "                customdata=label_id_df[CUSTOMDATA_COLS],\n",
    "                hovertemplate=HOVER_TEMPLATE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout -------------------------------------------------------------\n",
    "    min_grad_median = np.min(labeled_cand_df['grad_median'])\n",
    "    max_grad_median = np.max(labeled_cand_df['grad_median'])\n",
    "    \n",
    "    # Plot ideal T/F lines\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[1, 1],\n",
    "            # x=[min_grad_median, min_grad_median],\n",
    "            y=[1,1], z=[-1,2],\n",
    "            mode='lines', name='Smooth, Unipolar',\n",
    "            line=dict(width=10, color='lightseagreen')\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[0, 0],\n",
    "            # x=[max_grad_median, max_grad_median],\n",
    "            y=[0,0], z=[-1,2],\n",
    "            mode='lines', name='Unsmooth, Bipolar',\n",
    "            line=dict(width=10, color='crimson')\n",
    "        )\n",
    "    )\n",
    "    axis_range = [-0.1, 1.1]\n",
    "    fig.update_layout(\n",
    "        # width=600, height=500,\n",
    "        width=800, height=700,\n",
    "        margin=dict(l=20, r=0, b=20, t=20),\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='Smoothness Quantile', range=axis_range),\n",
    "            # xaxis_title='Gradient Median',\n",
    "            yaxis=dict(title='Unipolarity', range=axis_range),\n",
    "            zaxis=dict(title='Near Disk Center', range=axis_range),\n",
    "            camera_eye=camera_eye, aspectmode='cube',\n",
    "        )\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_roc_rates(confidence_series, y, confidence_thresh):\n",
    "    \"\"\"Compute true and false positive rates with sklearn for an ROC curve\n",
    "    \"\"\"\n",
    "    pred = np.where(np.array(confidence_series) > confidence_thresh, 1, 0)\n",
    "    cm = confusion_matrix(y, pred, labels=[0, 1]\n",
    "    )\n",
    "    true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
    "    true_pos_rate = true_pos/(true_pos + false_neg)\n",
    "    false_pos_rate = false_pos/(false_pos + true_neg)\n",
    "    \n",
    "    return [true_pos_rate, false_pos_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Outcomes\n",
    "\n",
    "of Labeled Candidates Across All Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df_file = CH_LABEL_DIR + '2024_06_06__20_29_outcomes.csv'\n",
    "LDA_FILE_NAME = 'v1_1_LDA_model.pkl'\n",
    "\n",
    "labeled_cand_df = pd.read_csv(labeled_cand_df_file)\n",
    "\n",
    "# Track candidates that have been inspected\n",
    "inspected_all_cand_idxs = [\n",
    "    7, 11, 63, 66, 68, 70, 83, 101, 104, 120, 135, 142,\n",
    "    151, 159, 189, 191, 192, 196, 211, 220, 244, 247, 261,\n",
    "    258, 265, 274, 276, 277, 298, 309, 316, 323,\n",
    "]\n",
    "\n",
    "labeled_cand_df['inspected?'] = [False for _ in range(len(labeled_cand_df))]\n",
    "labeled_cand_df.loc[inspected_all_cand_idxs,'inspected?'] = True\n",
    "\n",
    "labeled_cand_df['smooth_quantile'] = (\n",
    "    1 - stats.rankdata(labeled_cand_df['grad_median'])/len(labeled_cand_df)\n",
    ")\n",
    "classify_cand_df = labeled_cand_df[labeled_cand_df['label_id'] != 0.5]\n",
    "num_cands = classify_cand_df.shape[0]\n",
    "\n",
    "len(labeled_cand_df[labeled_cand_df['inspected?']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old Label Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df = pd.read_csv(\n",
    "    CH_LABEL_DIR + '2024_05_30__12_51_outcomes.csv'\n",
    ")\n",
    "\n",
    "# Track candidates that have been inspected\n",
    "inspected_all_cand_idxs = [\n",
    "    11, 63, 66, 68, 70,\n",
    "    83, 101, 104, 120, 135, 142,\n",
    "    192,\n",
    "    196, 211, 220, 244, 247,\n",
    "    261, 274, 276, 277,\n",
    "    309, 323,\n",
    "]\n",
    "\n",
    "labeled_cand_df['inspected?'] = [False for _ in range(len(labeled_cand_df))]\n",
    "labeled_cand_df.loc[inspected_all_cand_idxs,'inspected?'] = True\n",
    "\n",
    "labeled_cand_df['smooth_quantile'] = (\n",
    "    1 - stats.rankdata(labeled_cand_df['grad_median'])/len(labeled_cand_df)\n",
    ")\n",
    "classify_cand_df = labeled_cand_df[labeled_cand_df['label_id'] != 0.5]\n",
    "num_cands = classify_cand_df.shape[0]\n",
    "\n",
    "len(labeled_cand_df[labeled_cand_df['inspected?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df = pd.read_csv(\n",
    "    CH_LABEL_DIR + '2024_05_05__07_45_outcomes.csv'\n",
    ")\n",
    "\n",
    "# Track candidates that have been inspected\n",
    "inspected_all_cand_idxs = [\n",
    "    11, 54, 65, 67,\n",
    "    81, 100, 103, 119, 140, 195, 213, 220, 243,\n",
    "    260, 269, 275,\n",
    "    308, 322\n",
    "]\n",
    "\n",
    "labeled_cand_df['inspected?'] = [False for _ in range(len(labeled_cand_df))]\n",
    "labeled_cand_df.loc[inspected_all_cand_idxs, 'inspected?'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Labels\n",
    "\n",
    "1. Load pre-processed maps\n",
    "2. Obtain preliminary segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 period, aimed for every 15-20 days\n",
    "# 0:  2012_04_01__17_03 18 days     15: 2012_04_19__17_56   17 days\n",
    "# 30: 2012_05_06__18_33 16 days     45: 2012_05_22__18_04   19 days\n",
    "# 60: 2012_06_10__17_44 16 days     75: 2012_06_26__16_51   11 days\n",
    "# 80: 2012_07_07__17_24 23 days     90: 2012_07_30__20_19   8 days\n",
    "# 95: 2012_08_07__21_40 17 days     100: 2012_08_24__22_54\n",
    "\n",
    "# 2015 period, every 15-20 days\n",
    "# 0: 2015_01_04__20_30  16 days     1: 2015_01_20__20_25    14 days\n",
    "# 3: 2015_02_03__18_31  7 days      9: 2015_02_10__18_45    17 days\n",
    "# 13: 2015_02_27__20_39 17 days     14: 2015_03_15__18_04   15 days\n",
    "# 16: 2015_03_31__18_13 18 days     17: 2015_04_18__17_22   38 days\n",
    "# 19: 2015_05_26__20_21 11 days     20: 2015_06_06__16_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_date_str = HE_DATE_LIST[0]\n",
    "next_he_date = '2012_08_08__00_00'\n",
    "he_date_str = prepare_data.get_latest_date_str(\n",
    "    HE_DATE_LIST, selected_date_str=next_he_date\n",
    ")\n",
    "\n",
    "mag_date_str = prepare_data.get_nearest_date_str(\n",
    "    MAG_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Aggressive/Conservative Masks from v1.0 SOLIS Design\n",
    "percent_of_peak_list = [70, 90]\n",
    "morph_radius_list = [   15, 13] # Mm\n",
    "\n",
    "# # v1.0 KPVT Design\n",
    "# percent_of_peak_list = [85, 85, 95, 105]\n",
    "# morph_radius_list = [   15, 17, 13, 13] # Mm\n",
    "\n",
    "# Extract FITS file pre-processed map\n",
    "pre_process_fits_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "pre_processed_map = sunpy.map.Map(pre_process_fits_file)\n",
    "pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "smooth_mag_map_data = np.flipud(reprojected_smooth_map.data)\n",
    "\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "# See save_ch_label_dict docstring\n",
    "ch_label_dict = {\n",
    "    'percent_of_peak_list': percent_of_peak_list,\n",
    "    'morph_radius_list': morph_radius_list,\n",
    "    'label_list_dict': {\n",
    "        'mask_0': None,\n",
    "        'mask_1': None\n",
    "    },\n",
    "    'labeled_data_dict': {\n",
    "        'mask_0': None,\n",
    "        'mask_1': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot numbered CHs, specify labels, and plot labeled CHs to confirm correct labeling\n",
    "   - Analyze Current Design Option: Label CHs from 2 of 4 masks\n",
    "   - Characterize CHs in He I: Vary design variables to get rough boundaries for all present CHs?\n",
    "2. Save ch_label_dict for a single date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to plot CHs with discrete colors to indicate number.\n",
    "#   False will plot CHs with False, Marginal, True, or Unlabeled colors\n",
    "numbered_mode = True\n",
    "\n",
    "ch_label_dict['label_list_dict']['mask_0'] = [\n",
    "    2, 2, 2, 2, # blue, orange\n",
    "    2, 2, 2, 2, # green, red\n",
    "    2, 2, 2, 2, # purple, brown\n",
    "    2, 2, 2, 2, # pink, grey\n",
    "]\n",
    "ch_label_dict['label_list_dict']['mask_1'] = [\n",
    "    2, 2, 2, 2, # blue, orange\n",
    "    2, 2, 2, 2, # green, red\n",
    "    2, 2, 2, 2, # purple, brown\n",
    "    2, 2, 2, 2, # pink, grey\n",
    "]\n",
    "# numbered_mode = False\n",
    "\n",
    "plot_labeled_chs(\n",
    "    he_date_str, pre_processed_map_data, smooth_mag_map_data,\n",
    "    ch_label_dict, ch_mask_list, numbered_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean slate\n",
    "ch_label_dict['label_list_dict']['mask_0'] = [\n",
    "    2, 2, 2, 2, # blue, orange\n",
    "    2, 2, 2, 2, # green, red\n",
    "    2, 2, 2, 2, # purple, brown\n",
    "    2, 2, 2, 2, # pink, grey\n",
    "]\n",
    "ch_label_dict['label_list_dict']['mask_1'] = [\n",
    "    2, 2, 2, 2, # blue, orange\n",
    "    2, 2, 2, 2, # green, red\n",
    "    2, 2, 2, 2, # purple, brown\n",
    "    2, 2, 2, 2, # pink, grey\n",
    "]\n",
    "# numbered_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_label_dict = save_ch_label_dict(\n",
    "    pre_processed_map_data, he_date_str, ch_mask_list, ch_label_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute All Date Outcomes\n",
    "\n",
    "Requires all dates to be selected in settings.py\n",
    "\n",
    "TODO: Switch inspected cand idxs to persistent identifier if new candidates will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dates for which candidate regions have been labeled\n",
    "label_file_list = glob.glob(LABEL_FILE_GLOB_PATTERN)\n",
    "LABEL_DATE_LIST = [label_file.split('/')[-1].split('.')[0]\n",
    "                            for label_file in label_file_list]\n",
    "LABEL_DATE_LIST.sort()\n",
    "\n",
    "len(LABEL_DATE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries of candidate outcomes and metadata\n",
    "outcomes_by_cand_dict = {}\n",
    "for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "    outcomes_by_cand_dict[outcome_key] = []\n",
    "\n",
    "meta_by_cand_dict = {\n",
    "    'he_date_str': [],\n",
    "    'mask_idx': [],\n",
    "    'label_id': []\n",
    "}\n",
    "\n",
    "print(f'Computed outcomes for candidate regions on:')\n",
    "\n",
    "for he_date_str in LABEL_DATE_LIST:\n",
    "    # Extract saved data for computing outcomes ------------------------------\n",
    "    # Extract He I map data\n",
    "    he_fits_file = DATA_FITS_FORMAT.format(\n",
    "        data_dir=HE_DIR, date_str=he_date_str\n",
    "    )\n",
    "    he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "    he_map_data = np.flipud(he_map.data)\n",
    "\n",
    "    # Extract differentially rotated magnetogram map\n",
    "    mag_date_str = prepare_data.get_nearest_date_str(\n",
    "        MAG_DATE_LIST, selected_date_str=he_date_str\n",
    "    )\n",
    "    reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                            f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "    reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "    \n",
    "    # Extract label dictionary from file\n",
    "    ch_label_file_name = CH_LABEL_FILE_FORMAT.format(\n",
    "        he_date_str=he_date_str\n",
    "    )\n",
    "    with open(ch_label_file_name, 'rb') as ch_label_file:\n",
    "        ch_label_dict = pickle.load(ch_label_file)\n",
    "    labeled_data_dict = ch_label_dict['labeled_data_dict']\n",
    "    \n",
    "    # Compute candidate region outcomes for each mask and label ID ----------\n",
    "    for mask_idx in range(NUM_MASKS):\n",
    "        # Extract 2D array with labeled candidate regions\n",
    "        labeled_mask_on_disk = labeled_data_dict[f'mask_{mask_idx}']\n",
    "        \n",
    "        for label_id in LABEL_ID_LIST:\n",
    "            # 2D array with candidate regions of the specified label ID\n",
    "            label_id_mask = np.where(labeled_mask_on_disk == label_id, 1, 0)\n",
    "\n",
    "            label_id_map = sunpy.map.Map(\n",
    "                np.flipud(label_id_mask), he_map.meta\n",
    "            )\n",
    "            mask_label_id_cand_outcomes_dict = detect.get_outcomes_by_ch(\n",
    "                label_id_map, he_map_data, reprojected_mag_map,\n",
    "                confidence_level=0\n",
    "            )\n",
    "            \n",
    "            # Unsure how to get candidate number from ndimage.label(ch_mask)\n",
    "            # in global mask for helping reference rainbow plots\n",
    "            \n",
    "            # Track metadata of candidates being iterated over\n",
    "            num_candidates = len(\n",
    "                mask_label_id_cand_outcomes_dict[detect.OUTCOME_KEY_LIST[0]]\n",
    "            )\n",
    "            meta_by_cand_dict['he_date_str'].extend(\n",
    "                [he_date_str for _ in range(num_candidates)]\n",
    "            )\n",
    "            meta_by_cand_dict['mask_idx'].extend(\n",
    "                [mask_idx for _ in range(num_candidates)]\n",
    "            )\n",
    "            meta_by_cand_dict['label_id'].extend(\n",
    "                [label_id for _ in range(num_candidates)]\n",
    "            )\n",
    "            \n",
    "            # Extend main dictionary with each outcome\n",
    "            for outcome_key in detect.OUTCOME_KEY_LIST:\n",
    "                outcomes_by_cand_dict[outcome_key].extend(\n",
    "                    mask_label_id_cand_outcomes_dict[outcome_key]\n",
    "                )\n",
    "    \n",
    "    print(he_date_str, end='\\t')\n",
    "\n",
    "labeled_cand_dict = outcomes_by_cand_dict.copy()\n",
    "labeled_cand_dict.update(meta_by_cand_dict)\n",
    "labeled_cand_df = pd.DataFrame(labeled_cand_dict)\n",
    "\n",
    "labeled_cand_df['all_cand_idx'] = labeled_cand_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df.to_csv(LABEL_OUTCOME_FILE, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Saved Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_date_str = HE_DATE_LIST[19]\n",
    "next_he_date = '2015_01_21__00_00'\n",
    "he_date_str = prepare_data.get_latest_date_str(\n",
    "    HE_DATE_LIST, selected_date_str=next_he_date\n",
    ")\n",
    "\n",
    "mag_date_str = prepare_data.get_nearest_date_str(\n",
    "    MAG_DATE_LIST, selected_date_str=he_date_str\n",
    ")\n",
    "\n",
    "# Extract pre-processed He I map\n",
    "pre_process_fits_file = (PREPROCESS_MAP_SAVE_DIR + he_date_str\n",
    "                        + '_pre_processed_map.fits')\n",
    "pre_processed_map = sunpy.map.Map(pre_process_fits_file)\n",
    "pre_processed_map_data = np.flipud(pre_processed_map.data)\n",
    "\n",
    "# Extract saved data for plotting labels -----------------------------\n",
    "\n",
    "# Extract saved processed magnetogram\n",
    "reprojected_smooth_file = (f'{ROTATED_MAG_SAVE_DIR}Mag{mag_date_str}'\n",
    "                           f'_He{he_date_str}_smooth.fits')\n",
    "reprojected_smooth_map = sunpy.map.Map(reprojected_smooth_file)\n",
    "smooth_mag_map_data = np.flipud(reprojected_smooth_map.data)\n",
    "\n",
    "# Extract label dictionary from file ---------------------------------\n",
    "ch_label_file_name = CH_LABEL_FILE_FORMAT.format(\n",
    "    detection_image_dir=DETECTION_IMAGE_DIR, he_date_str=he_date_str\n",
    ")\n",
    "with open(ch_label_file_name, 'rb') as ch_label_file:\n",
    "    ch_label_dict = pickle.load(ch_label_file)\n",
    "    \n",
    "# Recreate masks which were labeled ----------------------------------\n",
    "percent_of_peak_list =  ch_label_dict['percent_of_peak_list']\n",
    "morph_radius_list =  ch_label_dict['morph_radius_list']\n",
    "ch_mask_list = detect.get_ch_mask_list_v0_5_1(\n",
    "    pre_processed_map, percent_of_peak_list, morph_radius_list\n",
    ")\n",
    "\n",
    "plot_labeled_chs(\n",
    "    he_date_str, pre_processed_map_data, smooth_mag_map_data,\n",
    "    ch_label_dict, ch_mask_list, numbered_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downselect candidates from a single mask to those with a label among True/Marginal/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 1\n",
    "mask_idx = 0\n",
    "\n",
    "# 2D array with candidate regions labeled 0, 0.5, or 1 and a removed background\n",
    "labeled_mask_on_disk = ch_label_dict['labeled_data_dict'][f'mask_{mask_idx}']\n",
    "\n",
    "# 2D array with candidate regions of the specified label ID\n",
    "label_id_mask = np.where(labeled_mask_on_disk == label_id, 1, 0)\n",
    "\n",
    "label_id_map = sunpy.map.Map(np.flipud(label_id_mask), pre_processed_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111, projection=label_id_map)\n",
    "label_id_map.plot(axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute outcomes of down selected candidate regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_outcome = 'unipolarity'\n",
    "sort_outcome = None\n",
    "\n",
    "# Extract saved data for computing outcomes -----------------------------\n",
    "# Extract He I map data\n",
    "he_fits_file = DATA_FITS_FORMAT.format(\n",
    "    data_dir=HE_DIR, date_str=he_date_str\n",
    ")\n",
    "he_map = prepare_data.get_nso_sunpy_map(he_fits_file)\n",
    "he_map_data = np.flipud(he_map.data)\n",
    "\n",
    "# Extract differentially rotated magnetogram map\n",
    "reprojected_fits_file = (f'{ROTATED_MAG_SAVE_DIR}'\n",
    "                         f'Mag{mag_date_str}_He{he_date_str}.fits')\n",
    "reprojected_mag_map = sunpy.map.Map(reprojected_fits_file)\n",
    "\n",
    "# Compute outcomes by CH and retrieve binary masks for distinct CHs -----\n",
    "outcome_by_ch_dict = detect.get_outcomes_by_ch(\n",
    "    label_id_map, he_map_data, reprojected_mag_map, confidence_level=0\n",
    ")\n",
    "individual_ch_list = detect.get_map_data_by_ch(\n",
    "    label_id_map.data, label_id_map.data\n",
    ")\n",
    "\n",
    "# Optionally sort from greatest to least ---------------------------------\n",
    "if sort_outcome is not None:\n",
    "    sorted_idxs = np.flip(np.argsort(outcome_by_ch_dict[sort_outcome]))\n",
    "\n",
    "    sorted_outcome_by_ch_dict = {}\n",
    "    for key, outcome_by_ch in zip(outcome_by_ch_dict, outcome_by_ch_dict.values()):\n",
    "        sorted_outcome_by_ch_dict[key] = [outcome_by_ch[i] for i in sorted_idxs]\n",
    "    \n",
    "    outcome_by_ch_dict = sorted_outcome_by_ch_dict\n",
    "\n",
    "    individual_ch_list = [individual_ch_list[i] for i in sorted_idxs]\n",
    "\n",
    "\n",
    "[print(f'Grad: {grad_median:.4f}', end='\\t')\n",
    " for grad_median in outcome_by_ch_dict['grad_median']]\n",
    "print()\n",
    "[print(f'U: {unipolarity:.4f}', end='\\t')\n",
    " for unipolarity in outcome_by_ch_dict['unipolarity']]\n",
    "print()\n",
    "[print(f'Foreshort: {cm_foreshort:.2f}', end='\\t')\n",
    " for cm_foreshort in outcome_by_ch_dict['cm_foreshort']]\n",
    "print()\n",
    "[print(f'{area:.1e} Mm^2', end='\\t')\n",
    " for area in outcome_by_ch_dict['area']]\n",
    "print()\n",
    "[print(f'Lat: {cm_lat:.1f} deg', end='\\t')\n",
    " for cm_lat in outcome_by_ch_dict['cm_lat']]\n",
    "print()\n",
    "[print(f'Lon: {cm_lon:.1f} deg', end='\\t')\n",
    " for cm_lon in outcome_by_ch_dict['cm_lon']]\n",
    "print()\n",
    "[print(f'{signed_flux:.4e} Mx', end='\\t')\n",
    " for signed_flux in outcome_by_ch_dict['signed_flux']]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_idx = 3\n",
    "\n",
    "grad_median = outcome_by_ch_dict['grad_median'][ch_idx]\n",
    "unipolarity = outcome_by_ch_dict['unipolarity'][ch_idx]\n",
    "cm_foreshort = outcome_by_ch_dict['cm_foreshort'][ch_idx]\n",
    "\n",
    "title = f'GM: {grad_median:.3f} | U: {unipolarity:.2f} | F: {cm_foreshort:.2f}'\n",
    "\n",
    "selected_ch_map_data = individual_ch_list[ch_idx]\n",
    "selected_ch_map_data = np.where(\n",
    "    np.isnan(selected_ch_map_data), -100,\n",
    "    selected_ch_map_data\n",
    ")\n",
    "selected_ch_map = sunpy.map.Map(selected_ch_map_data, he_map.meta)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.suptitle(he_date_str)\n",
    "\n",
    "ax = fig.add_subplot(projection=label_id_map)\n",
    "label_id_map.plot(axes=ax, title=title)\n",
    "label_id_map.draw_grid(axes=ax)\n",
    "for contour in selected_ch_map.contour(0):\n",
    "    ax.plot_coord(contour, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Saved Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = 0\n",
    "numbered_mode = True\n",
    "\n",
    "# Comment to ID region number to correct/Uncomment to correct region label\n",
    "numbered_mode = False\n",
    "ch_label_dict['label_list_dict'][f'mask_{mask_idx}'][region_idx] = new_label\n",
    "\n",
    "plot_labeled_chs(\n",
    "    he_date_str, pre_processed_map_data, smooth_mag_map_data,\n",
    "    ch_label_dict, ch_mask_list, numbered_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify region number\n",
    "region_num_on_colorbar = 6\n",
    "region_idx = region_num_on_colorbar - 1\n",
    "\n",
    "# Verify by checking original label\n",
    "ch_label_dict['label_list_dict'][f'mask_{mask_idx}'][region_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_label_dict = save_ch_label_dict(\n",
    "    pre_processed_map_data, he_date_str, ch_mask_list, ch_label_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Dates with Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_cand_df[labeled_cand_df['inspected?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_M = len(labeled_cand_df[labeled_cand_df['label_id'] == 0.5])\n",
    "num_not_M = len(labeled_cand_df[labeled_cand_df['label_id'] != 0.5])\n",
    "f'Percent of Regions Labeled Marginal: {num_M/num_not_M*100:.2f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera_loc=dict(x=0.1, y=2.6, z=0.2) # Distance vs Grad Median\n",
    "camera_loc=dict(x=0.1, y=-2.6, z=0.2) # Distance vs Smoothness\n",
    "\n",
    "# camera_loc=dict(x=0.1, y=0, z=2.6) # Grad Median vs Unipolarity\n",
    "# camera_loc=dict(x=0, y=-0.1, z=2.6) # Smoothness vs Unipolarity\n",
    "\n",
    "# camera_loc=dict(x=2.6, y=-0.1, z=0.2) # Distance vs Unipolarity\n",
    "# camera_loc=dict(x=1.4, y=2, z=0.3) # Decision\n",
    "\n",
    "fig = plot_3d_feature_space(\n",
    "    labeled_cand_df, camera_eye=camera_loc,\n",
    "    gray_out_inspected=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'smooth_quantile'\n",
    "\n",
    "# Y-axis by area sum of regions in bin or by count\n",
    "y_sum = 'area'\n",
    "# y_sum = None\n",
    "\n",
    "ascend = True\n",
    "\n",
    "sorted_df = labeled_cand_df.sort_values(by='label_id', ascending=ascend)\n",
    "color_list = LABEL_COLOR_LIST.copy()\n",
    "\n",
    "if not ascend:\n",
    "    color_list.reverse()\n",
    "\n",
    "fig = px.histogram(\n",
    "    sorted_df, x=outcome, y=y_sum,\n",
    "    color='label_id', marginal='box',\n",
    "    hover_data=sorted_df.columns, barmode='overlay',\n",
    "    color_discrete_sequence=color_list,\n",
    "    nbins=20\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave candidates labeled as marginal out of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varied Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.75\n",
    "outcomes_for_features = ['unipolarity', 'smooth_quantile']\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])\n",
    "\n",
    "num_train_cands = int(train_fraction*num_cands)\n",
    "X_train = X[:num_train_cands, :]\n",
    "y_train = y[:num_train_cands]\n",
    "X_test = X[num_train_cands:, :]\n",
    "y_test = y[num_train_cands:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "svc = svm.LinearSVC(C=0.05)\n",
    "svc_C10 = svm.LinearSVC(C=50)\n",
    "\n",
    "estimators = [lda, qda, svc, svc_C10]\n",
    "estimator_names = ['LDA', 'QDA', 'SVM C=0.05', 'SVM C=50']\n",
    "\n",
    "for estimator in estimators:\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_thresh = 0\n",
    "\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, ax_grid = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*6, nrows*5))\n",
    "axs = ax_grid.ravel()\n",
    "cmap = colors.ListedColormap(['tab:red', 'tab:green'])\n",
    "\n",
    "for estimator, estimator_name, ax in zip(estimators, estimator_names, axs):\n",
    "    \n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator, X, plot_method='contourf', ax=ax, cmap='RdYlGn',\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator, X, plot_method='contour', ax=ax, alpha=1.0,\n",
    "        levels=[decision_thresh],\n",
    "    )\n",
    "    for X_plot, y_plot, alpha in zip(\n",
    "        [X_train, X_test], [y_train, y_test], [0.3, 1]):\n",
    "        \n",
    "        y_pred = estimator.predict(X_plot)\n",
    "        X_right, y_right = X_plot[y_plot == y_pred], y_plot[y_plot == y_pred]\n",
    "        X_wrong, y_wrong = X_plot[y_plot != y_pred], y_plot[y_plot != y_pred]\n",
    "        \n",
    "        ax.scatter(\n",
    "            X_right[:, 0], X_right[:, 1], c=y_right, s=20,\n",
    "            cmap=cmap, alpha=alpha\n",
    "        )\n",
    "        ax.scatter(\n",
    "            X_wrong[:, 0], X_wrong[:, 1], c=y_wrong, s=30,\n",
    "            cmap=cmap, marker='x', alpha=alpha\n",
    "        )\n",
    "    \n",
    "    ax.set_title(estimator_name)\n",
    "    ax.set_xlim([-0.1,1.1])\n",
    "    ax.set_ylim([-0.1,1.1])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "for i in range(nrows):\n",
    "    ax_grid[i,0].set_ylabel('Smoothness Quantile')\n",
    "for i in range(ncols):\n",
    "    ax_grid[nrows-1,i].set_xlabel('Unipolarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance & Sklearn Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_for_features = detect.OUTCOME_KEY_LIST\n",
    "outcomes_for_features.append('smooth_quantile')\n",
    "outcomes_for_features = ['unipolarity', 'smooth_quantile', 'cm_foreshort']\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])\n",
    "\n",
    "lda.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(outcomes_for_features)\n",
    "importance = np.abs(lda.coef_).flatten()\n",
    "\n",
    "fig = plt.figure(figsize=(9,5))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title('Feature importances via coefficients')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_for_features = ['unipolarity', 'smooth_quantile', 'cm_foreshort']\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])\n",
    "X_train = X[:num_train_cands, :]\n",
    "y_train = y[:num_train_cands]\n",
    "X_test = X[num_train_cands:, :]\n",
    "y_test = y[num_train_cands:]\n",
    "\n",
    "neg_label = 0\n",
    "pos_label = 1\n",
    "\n",
    "\n",
    "def fpr_score(y, y_pred, neg_label, pos_label):\n",
    "    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])\n",
    "    tn, fp, _, _ = cm.ravel()\n",
    "    return fp / (tn + fp)\n",
    "\n",
    "def fnr_score(y, y_pred, neg_label, pos_label):\n",
    "    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])\n",
    "    _, _, fn, tp = cm.ravel()\n",
    "    return fn / (fn + tp)\n",
    "\n",
    "tpr_score = recall_score  # TPR and recall are the same metric\n",
    "scoring = {\n",
    "    'fpr': make_scorer(fpr_score, neg_label=neg_label, pos_label=pos_label),\n",
    "    'tpr': make_scorer(tpr_score, pos_label=pos_label),\n",
    "    'fnr': make_scorer(fnr_score, neg_label=neg_label, pos_label=pos_label),\n",
    "}\n",
    "\n",
    "vanilla_model = LinearDiscriminantAnalysis()\n",
    "vanilla_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "# from sklearn.model_selection import FixedThresholdClassifier\n",
    "\n",
    "# fixed_model = FixedThresholdClassifier(\n",
    "#     estimator=vanilla_model, threshold=0.5\n",
    "# ).fit(X_train, y_train)\n",
    "\n",
    "tuned_model = TunedThresholdClassifierCV(\n",
    "    estimator=vanilla_model,\n",
    "    # scoring=scoring['fnr'],\n",
    "    store_cv_results=True,  # necessary to inspect all results\n",
    ")\n",
    "tuned_model.fit(X_train, y_train)\n",
    "print(f\"{tuned_model.best_threshold_=:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring['fnr'](tuned_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7, 7))\n",
    "color_list = ('tab:blue', 'tab:orange')\n",
    "\n",
    "for idx, (model, color) in enumerate(zip([vanilla_model, tuned_model], color_list)):\n",
    "    decision_threshold = getattr(model, 'best_threshold_', 0.5)\n",
    "    if idx == 1:\n",
    "        RocCurveDisplay.from_estimator(\n",
    "            model, X_test, y_test, pos_label=pos_label, ax=ax, name='LDA',\n",
    "            plot_chance_level=True,\n",
    "        )\n",
    "    ax.plot(\n",
    "        scoring['fpr'](model, X_test, y_test),\n",
    "        scoring['tpr'](model, X_test, y_test),\n",
    "        marker='o',\n",
    "        markersize=10,\n",
    "        color=color,\n",
    "        label=f'Cut-off point at probability of {decision_threshold:.2f}',\n",
    "    )\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n",
    "\n",
    "fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(11, 5))\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "RocCurveDisplay.from_estimator(\n",
    "    lda, X_test, y_test, ax=ax_roc, name='LDA',\n",
    "    plot_chance_level=True\n",
    ")\n",
    "decision_threshold = getattr(model, 'best_threshold_', 0.5)\n",
    "# ax_roc.plot(\n",
    "#     scoring['fpr'](lda, X_test, y_test),\n",
    "#     scoring['tpr'](lda, X_test, y_test),\n",
    "#     marker='o',\n",
    "#     markersize=10,\n",
    "#     # color=color,\n",
    "#     label=f'Cut-Off Probability of {decision_threshold:.2f}',\n",
    "# )\n",
    "ax_roc.legend()\n",
    "\n",
    "DetCurveDisplay.from_estimator(lda, X_test, y_test, ax=ax_det, name='LDA',)\n",
    "# ax_det.plot(\n",
    "#     scoring['fpr'](lda, X_test, y_test),\n",
    "#     scoring['fnr'](lda, X_test, y_test),\n",
    "#     marker='o',\n",
    "#     markersize=10,\n",
    "#     # color=color,\n",
    "#     label=f'Cut-Off Probability of {decision_threshold:.2f}',\n",
    "# )\n",
    "ax_det.legend()\n",
    "\n",
    "ax_roc.set_title('Receiver Operating Characteristic (ROC) curves')\n",
    "ax_det.set_title('Detection Error Tradeoff (DET) curves')\n",
    "\n",
    "ax_roc.grid(linestyle='--')\n",
    "ax_det.grid(linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.479 # 2012 only\n",
    "# train_fraction = 0.75\n",
    "num_train_cands = int(train_fraction*num_cands)\n",
    "\n",
    "classify_cand_df.iloc[:num_train_cands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcomes_for_features = ['unipolarity', 'smooth_quantile']\n",
    "# ylabel = 'Smoothness Quantile'\n",
    "outcomes_for_features = ['unipolarity', 'grad_median']\n",
    "ylabel = 'Gradient Median'\n",
    "\n",
    "# outcomes_for_features = ['unipolarity', 'cm_foreshort']\n",
    "# ylabel = 'Near Disk Center'\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_thresh = 0\n",
    "\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, ax_grid = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*6, nrows*5))\n",
    "axs = ax_grid.ravel()\n",
    "cmap = colors.ListedColormap(['tab:red', 'tab:green'])\n",
    "\n",
    "for train_fraction, ax in zip(np.arange(0.15, 0.8, 0.2), axs):\n",
    "    num_train_cands = int(train_fraction*num_cands)\n",
    "    \n",
    "    X_train = X[:num_train_cands, :]\n",
    "    y_train = y[:num_train_cands]\n",
    "    X_test = X[num_train_cands:, :]\n",
    "    y_test = y[num_train_cands:]\n",
    "    \n",
    "    lda.fit(X_train, y_train)\n",
    "    \n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        lda, X, plot_method='contourf', ax=ax, cmap='RdYlGn',\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        lda, X, plot_method='contour', ax=ax, alpha=1.0,\n",
    "        levels=[decision_thresh],\n",
    "    )\n",
    "    for X_plot, y_plot, alpha in zip(\n",
    "        [X_train, X_test], [y_train, y_test], [0.3, 1]):\n",
    "        \n",
    "        y_pred = lda.predict(X_plot)\n",
    "        X_right, y_right = X_plot[y_plot == y_pred], y_plot[y_plot == y_pred]\n",
    "        X_wrong, y_wrong = X_plot[y_plot != y_pred], y_plot[y_plot != y_pred]\n",
    "        \n",
    "        ax.scatter(\n",
    "            X_right[:, 0], X_right[:, 1], c=y_right, s=20,\n",
    "            cmap=cmap, alpha=alpha\n",
    "        )\n",
    "        ax.scatter(\n",
    "            X_wrong[:, 0], X_wrong[:, 1], c=y_wrong, s=30,\n",
    "            cmap=cmap, marker='x', alpha=alpha\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f'{int(train_fraction*100)}% of Data for Training')\n",
    "    ax.set_xlim([-0.1,1.1])\n",
    "    # ax.set_ylim([-0.1,1.1])\n",
    "    ax.set_ylim([0.8,6])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "for i in range(nrows):\n",
    "    ax_grid[i,0].set_ylabel(ylabel)\n",
    "\n",
    "for i in range(ncols):\n",
    "    ax_grid[nrows-1,i].set_xlabel('Unipolarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.479 # 2012 only\n",
    "# train_fraction = 0.9\n",
    "# outcomes_for_features = ['unipolarity', 'smooth_quantile', 'cm_foreshort']\n",
    "outcomes_for_features = ['unipolarity', 'grad_median', 'cm_foreshort']\n",
    "# outcomes_for_features = ['unipolarity', 'grad_median']\n",
    "\n",
    "X = np.array([classify_cand_df[outcome] for outcome in outcomes_for_features]).T\n",
    "y = np.array(classify_cand_df['label_id'])\n",
    "\n",
    "num_train_cands = int(train_fraction*num_cands)\n",
    "X_train = X[:num_train_cands, :]\n",
    "y_train = y[:num_train_cands]\n",
    "X_test = X[num_train_cands:, :]\n",
    "y_test = y[num_train_cands:]\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lda.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LDA object for future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_FILE_NAME = 'v1_1_LDA_model.pkl'\n",
    "\n",
    "with open(LDA_FILE_NAME, 'wb') as lda_file:\n",
    "    pickle.dump(lda, lda_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probability near 1 for perfectly unipolar and smooth CH,\n",
    "# either on disk center or limb\n",
    "perfect_ch_on_limb = lda.predict_proba(np.array([[1, 0, 0]]))[:,1][0]\n",
    "perfect_ch_on_disk_center = lda.predict_proba(np.array([[1, 0, 1]]))[:,1][0]\n",
    "print(f'Limb {perfect_ch_on_limb:.10f}, Center {perfect_ch_on_disk_center:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts probability 1 beyond U, GM, F bounds\n",
    "extreme_val = 1e0\n",
    "# extreme_val = 1e6\n",
    "impossible_ch = lda.predict_proba(\n",
    "    np.array([[extreme_val, -extreme_val, -extreme_val]])\n",
    ")[:,1][0]\n",
    "print(f'{impossible_ch - 1:.4e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_label_types = np.array(\n",
    "    [labeled_cand_df[outcome] for outcome in outcomes_for_features]\n",
    ").T\n",
    "v1_1_pred_probabilities = lda.predict_proba(X_all_label_types)[:,1]\n",
    "v1_1_log_probabilities = lda.predict_log_proba(X_all_label_types)[:,1]\n",
    "labeled_cand_df['v1_1_pred_probability'] = v1_1_pred_probabilities\n",
    "labeled_cand_df['v1_1_log_probability'] = v1_1_log_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome = 'v1_1_pred_probability'\n",
    "# outcome = 'v1_1_log_probability'\n",
    "# outcome = 'unipolarity'\n",
    "outcome = 'smooth_quantile'\n",
    "\n",
    "# Y-axis by area sum of regions in bin or by count\n",
    "y_sum = 'area'\n",
    "# y_sum = None\n",
    "\n",
    "ascend = True\n",
    "\n",
    "sorted_df = labeled_cand_df.sort_values(by='label_id', ascending=ascend)\n",
    "color_list = LABEL_COLOR_LIST.copy()\n",
    "\n",
    "if not ascend:\n",
    "    color_list.reverse()\n",
    "\n",
    "fig = px.histogram(\n",
    "    sorted_df, x=outcome, y=y_sum,\n",
    "    color='label_id', marginal='box',\n",
    "    hover_data=sorted_df.columns, barmode='overlay',\n",
    "    color_discrete_sequence=color_list,\n",
    "    nbins=20\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skspatial.objects import Plane\n",
    "\n",
    "coeffs = lda.coef_.flatten()\n",
    "\n",
    "lda_plane = Plane(point=-lda.intercept_/coeffs, normal=coeffs)\n",
    "lda_plane.cartesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "lda_plane.plot_3d(ax, alpha=0.5)\n",
    "ax.set_xlim([0,1])\n",
    "ax.set_ylim([1,6])\n",
    "ax.set_zlim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cand_features = X[0]\n",
    "x, y, z = new_cand_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(new_cand_features, lda.coef_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = new_cand_features\n",
    "a, b, c = lda.coef_.flatten()\n",
    "(a*x + b* y + c * z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_distance(new_cand_features, lda, a, b, c, d): \n",
    "    \n",
    "    x, y, z = new_cand_features\n",
    "    a, b, c = lda.coef_.flatten()\n",
    "    d = abs((a*x + b*y + c*z + d)) \n",
    "    e = (math.sqrt(a * a + b * b + c * c))\n",
    "    print(\"Perpendicular distance is\", d/e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No threshold for confidence\n",
    "v1_0_confidence_series = classify_cand_df['unipolarity'].copy()\n",
    "\n",
    "# # Threshold for confidence\n",
    "# unipolarity_threshold = 0.5\n",
    "# v1_0_confidence_series = (\n",
    "#     (classify_cand_df['unipolarity'] - unipolarity_threshold)\n",
    "#     /(1 - unipolarity_threshold)\n",
    "# )\n",
    "# v1_0_confidence_series[v1_0_confidence_series < 0] = 0\n",
    "\n",
    "# Descriptive statistics of STRIDE v1.0 detected regions at any confidence level\n",
    "classify_cand_df['unipolarity'][v1_0_confidence_series > 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect properties of T/N P/F categories with Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_0_ch_detected_series = v1_0_confidence_series > unipolarity_threshold\n",
    "v1_0_true_detect_series = (\n",
    "    v1_0_ch_detected_series == (classify_cand_df['label_id'] > 0)\n",
    ")\n",
    "\n",
    "true_pos_series = (v1_0_true_detect_series & v1_0_ch_detected_series)\n",
    "true_neg_series = (v1_0_true_detect_series & ~v1_0_ch_detected_series)\n",
    "false_pos_series = (~v1_0_true_detect_series & v1_0_ch_detected_series)\n",
    "false_neg_series = (~v1_0_true_detect_series & ~v1_0_ch_detected_series)\n",
    "\n",
    "# true_pos_rate = true_pos_series.sum()/(true_pos_series.sum() + false_neg_series.sum())\n",
    "# false_pos_rate = false_pos_series.sum()/(false_pos_series.sum() + true_neg_series.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of correctly STRIDE v1.0 detected regions\n",
    "classify_cand_df['unipolarity'][v1_0_true_detect_series].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = true_pos_series\n",
    "# filt = true_neg_series\n",
    "# filt = false_pos_series\n",
    "# filt = false_neg_series\n",
    "classify_cand_df['unipolarity'][filt].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Curve\n",
    "\n",
    "Pre-run LDA and v1.0 Predict sections\n",
    "- COSPAR dataset scatter\n",
    "- STRIDE U in 0-0.5, STRIDE U in 0.5-1, STRIDE v1.1 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_probabilities = np.arange(0,1.05,0.1)\n",
    "\n",
    "v1_0_sub_thresh_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_0_confidence_series, y, confidence_thresh)\n",
    "     for confidence_thresh in np.linspace(0,0.5,500)]\n",
    ")\n",
    "v1_0_thresh_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_0_confidence_series, y, confidence_thresh)\n",
    "     for confidence_thresh in np.linspace(0.5,1,500)]\n",
    ")\n",
    "v1_0_scatter_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_0_confidence_series, y, confidence_thresh)\n",
    "     for confidence_thresh in scatter_probabilities]\n",
    ")\n",
    "# true_pos_rates = roc_rates_array[:,0]\n",
    "# false_pos_rates = roc_rates_array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_1_probabilities = np.linspace(0,1,500)\n",
    "\n",
    "v1_1_test_probabilities = lda.predict_proba(X_test)[:,1]\n",
    "v1_1_test_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_1_test_probabilities, y_test, probability_thresh)\n",
    "     for probability_thresh in v1_1_probabilities]\n",
    ")\n",
    "v1_1_scatter_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_1_test_probabilities, y_test, probability_thresh)\n",
    "     for probability_thresh in scatter_probabilities]\n",
    ")\n",
    "\n",
    "v1_1_train_probabilities = lda.predict_proba(X_train)[:,1]\n",
    "v1_1_train_roc_rates = np.array(\n",
    "    [get_roc_rates(v1_1_train_probabilities, y_train, probability_thresh)\n",
    "     for probability_thresh in v1_1_probabilities]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_roc_rates = v1_0_scatter_roc_rates\n",
    "scatter_roc_rates = v1_1_scatter_roc_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6.3,6.3), dpi=150)\n",
    "# plt.figure(figsize=(7,7), dpi=150)\n",
    "\n",
    "plt.title(\n",
    "    f'2012 Train: {y_train.size} Candidates | 2015 Test: {y_test.size} Candidates'\n",
    ")\n",
    "# plt.title(f'{train_fraction} Training Fraction | {y_train.size}-{y_test.size} Train-Test')\n",
    "# plt.title('Using Gradient Median')\n",
    "\n",
    "\n",
    "# plt.plot([0,1], [0,1], linestyle='--', color='k', label='Random Classifier')\n",
    "\n",
    "# v1.1 -------------------------------------------------------------------------\n",
    "plt.plot(\n",
    "    v1_1_train_roc_rates[:,1], v1_1_train_roc_rates[:,0], \n",
    "    label=r'STRIDE v1.1 Training Data', color='peru', \n",
    "    linewidth=1\n",
    ")\n",
    "plt.plot(\n",
    "    v1_1_test_roc_rates[:,1], v1_1_test_roc_rates[:,0], \n",
    "    label=r'STRIDE v1.1 Test Data', color='peru', \n",
    "    linewidth=2\n",
    ")\n",
    "# # v1.0 ---------------------------------------------------------------------------\n",
    "# plt.plot(\n",
    "#     v1_0_thresh_roc_rates[:,1], v1_0_thresh_roc_rates[:,0], \n",
    "#     label=r'STRIDE v1.0 $U \\in [0.5,1]$', color='peru',\n",
    "#     linewidth=2, linestyle='--'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     v1_0_sub_thresh_roc_rates[:,1], v1_0_sub_thresh_roc_rates[:,0], \n",
    "#     label=r'STRIDE v1.0 $U \\in [0,0.5)$', color='peru',\n",
    "#     linewidth=2, linestyle=':'\n",
    "# )\n",
    "\n",
    "# Scatter ------------------------------------------------------------------------\n",
    "plt.scatter(\n",
    "    scatter_roc_rates[:,1], scatter_roc_rates[:,0],\n",
    "    color='peru', marker='s'\n",
    ")\n",
    "for prob, xi, yi in zip(\n",
    "        scatter_probabilities, scatter_roc_rates[:,1], scatter_roc_rates[:,0]\n",
    "    ):\n",
    "    \n",
    "    plt.annotate(\n",
    "        f'{prob:.1f}', xy=(xi, yi), xycoords='data', \n",
    "        xytext=(3, -12),    # Corner zoom\n",
    "        # xytext=(-20, 8),  # Unzoomed\n",
    "        textcoords='offset points'\n",
    "    )\n",
    "\n",
    "# Corner COSPAR ------------------------------------------------------------------\n",
    "plt.scatter(0.21,0.77,label='35% EUV Threshold', color='k', marker='d')\n",
    "plt.scatter(0,0.78,label='CHIMERA')\n",
    "plt.scatter(0.04,0.84,label='SPOCA (o:Base | ^:HEK)')\n",
    "plt.scatter(0.25,0.87,marker='^',color='tab:orange')    # SPOCA-HEK\n",
    "plt.scatter(0.07,0.83,label='ACWE  (o:03     | ^:  04)')\n",
    "plt.scatter(0.29,0.86,marker='^',color='tab:green')     # ACWE-04\n",
    "plt.scatter(0.13,0.85,label='CRONNOS')\n",
    "plt.scatter(0.21,0.79,label='CHARM')\n",
    "plt.scatter(0.35,0.82,label='CNN193')\n",
    "plt.scatter(0.38,0.98,label='WWWBCS')\n",
    "\n",
    "# Corner Zoom ---------------------------------------------------------------------\n",
    "plt.xlim([-0.01,0.45])\n",
    "plt.ylim([0.55,1.01])\n",
    "\n",
    "# # Unzoomed ------------------------------------------------------------------------\n",
    "# plt.xlim([-0.05,1.05])\n",
    "# plt.ylim([-0.05,1.05])\n",
    "\n",
    "# Unzoomed with scatter------------------------------------------------------------------------\n",
    "# plt.xlim([-0.08,1.08])\n",
    "# plt.ylim([-0.08,1.08])\n",
    "\n",
    "# # All COSPAR ---------------------------------------------------------------------\n",
    "# plt.scatter(0.49,0.99,label='CHMAP')\n",
    "# plt.scatter(0.54,0.86,label='CHIPS')\n",
    "# plt.scatter(0.62,0.87,label='SYNCH')\n",
    "# plt.scatter(0.85,0.92,label='CHORTLE', marker='s')\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right', reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4s/map * 30 maps/step = 150s/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 10\n",
    "\n",
    "he_map = prepare_data.get_nso_sunpy_map(HE_DIR + HE_DATE_LIST[0] + '.fts')\n",
    "he = detect.pre_process_v0_4(he_map.data)\n",
    "ch_mask_data = detect.get_ch_mask(\n",
    "    he, percent_of_peak, morph_radius\n",
    ")\n",
    "plt.imshow(ch_mask_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "class Optimizer:\n",
    "    \n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def get_area_list(self, design_vars):\n",
    "        \"\"\"Retrieve detected area percentages for specied dates.\n",
    "        \"\"\"\n",
    "        percent_of_peak, morph_radius = design_vars\n",
    "        area_percent_list = []\n",
    "\n",
    "        for he_date_str in HE_DATE_LIST:\n",
    "            he_map = prepare_data.get_nso_sunpy_map(\n",
    "                HE_DIR + he_date_str + '.fts'\n",
    "            )\n",
    "            he = detect.pre_process_v0_4(he_map.data)\n",
    "            ch_mask_data = detect.get_ch_mask(\n",
    "                he, percent_of_peak, morph_radius\n",
    "            )\n",
    "            ch_mask_map = sunpy.map.Map(np.flipud(ch_mask_data), he_map.meta)\n",
    "            area_percent = detect.get_open_area(ch_mask_map, 0)[0]\n",
    "            area_percent_list.append(area_percent)\n",
    "            \n",
    "        return area_percent_list\n",
    "\n",
    "# Initiate optimizer object with cached area percent list\n",
    "OPTIMIZER = Optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 90\n",
    "morph_radius = 10\n",
    "MORPH_RADIUS = 10\n",
    "MAX_AREA_PERCENT = 4\n",
    "MIN_AREA_PERCENT = 1\n",
    "design_vars = (percent_of_peak, morph_radius)\n",
    "\n",
    "\n",
    "def obj_func():\n",
    "    \"\"\"Objective function for persistence optimization.\n",
    "    Penalizes normalized MAD of detected area\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def constraints(design_vars):\n",
    "    \"\"\"Constraints for persistence optimization.\n",
    "    \"\"\"\n",
    "    area_percent_list = [6, 5, 4, 9]\n",
    "    # OPTIMIZER.get_area_list(design_vars)\n",
    "    mean_area_percent = np.mean(area_percent_list)\n",
    "    \n",
    "    return (mean_area_percent - MIN_AREA_PERCENT,\n",
    "            MAX_AREA_PERCENT - mean_area_percent)\n",
    "    # return np.array([mean_area_percent - MIN_AREA_PERCENT],\n",
    "    #                  MAX_AREA_PERCENT - mean_area_percent])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.get_area_list(design_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_area_list(design_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_func(90, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints(design_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLSQP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.show_options(solver='minimize', method='SLSQP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize.rosen([0.5, 0, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_vars = (percent_of_peak, morph_radius)\n",
    "ineq_cons = ({\n",
    "    'type': 'ineq',\n",
    "    'fun' : constraints,\n",
    "    # 'args': (design_vars,)\n",
    "})\n",
    "# Unconstrained\n",
    "res = optimize.minimize(\n",
    "    obj_func, percent_of_peak, method='SLSQP',\n",
    "    options={'disp': True, 'finite_diff_rel_step': [0.1]},\n",
    ")\n",
    "# Constrained\n",
    "# res = optimize.minimize(\n",
    "#     obj_func, design_vars, args=(optimizer,), method='SLSQP', constraints=ineq_cons,\n",
    "#     options={'ftol': 1e-9, 'disp': True}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_peak = 100\n",
    "design_vars = (percent_of_peak, morph_radius)\n",
    "res = optimize.minimize(\n",
    "    obj_func, percent_of_peak, method='BFGS',\n",
    "    options={'disp': True, 'xrtol': 0.01, 'eps': 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res\n",
    "res1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSO CH Estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Level 2 Products: **Stage/Level2/**\n",
    "\n",
    "10830i equivalent width: **svsm_e3100_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "10830i intensity: **svsm_i3000_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "6302l magnetogram: **svsm_m1100_S2_yyyymmdd_hhmm.fts.gz**\n",
    "\n",
    "6302l intensity: **svsm_i1000_S2_yyyymmdd_hhmm.fts.gz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_he_eqw_fits_path = NSO_INPUT_DIR + 'svsm_e3100_S2_20140626_1419.fts'\n",
    "\n",
    "raw_he_intensity_fits_path = NSO_INPUT_DIR + 'svsm_i3000_S2_20140626_1419.fts'\n",
    "\n",
    "raw_magnetogram_fits_path = NSO_INPUT_DIR + 'svsm_m1100_S2_20140626_1444.fts'\n",
    "\n",
    "raw_mag_intensity_fits_path = NSO_INPUT_DIR + 'svsm_i1000_S2_20140626_1444.fts'\n",
    "\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    raw_he_eqw_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_he_intensity_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_magnetogram_fits_path, header_list=['IMTYPE'],\n",
    "    print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    raw_mag_intensity_fits_path, header_list=['IMTYPE'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Level 3 Products: **Stage/Level3/**\n",
    "He I EqW Maps List: **hDataList.txt**\n",
    "- 'Text list of available 10830 He EqW low-res sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic He I EqW maps\n",
    "\n",
    "Magnetogram Maps List: **mDataList.txt**\n",
    "- 'Text list of available 6301.5 low-res sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic magnetograms\n",
    "\n",
    "CH Maps List: **oDataList.txt**\n",
    "- 'Text list of available 10830 solar wind source sine-latlon heliographic maps'\n",
    "- Produced by **mk_datalist**\n",
    "  - Args: heliographic CH maps\n",
    "\n",
    "Carrington Rotation CH Images\n",
    "- '10830 solar wind sine-latlon daily synoptic map plots'\n",
    "- High-Res: **chsh.jpg**\n",
    "- Med-Res: **chsm.jpg**\n",
    "- Low-Res: **chsl.jpg**\n",
    "- Produced by **plot_lev3_map**\n",
    "  - Args: 1 carrington rotation CH map, CH maps list\n",
    "\n",
    "Unincluded He I Observation Image?: **svsm_o10mr_S3_{yyyymmdd}_{hhmm}.jpg**\n",
    "- Produced by **mk_obsimg**\n",
    "  - Args: 1 sky frame disk CH map, CH maps list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Maps: **../single/{yyyy}/**\n",
    "\n",
    "Sine-LatLon He I EQW: **svsm_e31hr_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 He EqW high-res sine-latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 He I EQW file, L2 He I continuum intensity file\n",
    "\n",
    "LatLon He I EQW: **svsm_e31lr_L3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 He EqW low-res latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 He I EQW file, L2 He I continuum intensity file\n",
    "\n",
    "Sine-LatLon Magnetogram: **svsm_m11hr_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '6301.5 high-res sine-latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 magnetogram file, L2 6302l magnetogram intensity file\n",
    "\n",
    "LatLon Magnetogram: **svsm_m11lr_L3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '6301.5 low-res latlon heliographic map'\n",
    "- Produced by **mk_synimg**\n",
    "  - Args: L2 magnetogram file, L2 6302l magnetogram intensity file\n",
    "\n",
    "Sine-LatLon CH Map: **svsm_o1083_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 solar wind source sine-latlon heliographic map'\n",
    "- Produced by **mk_holeimg**\n",
    "  - Args: processed He I EqW, He I EqW maps list, processed magnetogram, magnetogram maps list\n",
    "\n",
    "Sky Frame Disk CH Map: **svsmgo1083_B3_{yyyymmdd}_{hhmm}.fts.gz**\n",
    "- '10830 solar wind source sky frame heliocentric map'\n",
    "- Produced by **mk_dchimg**\n",
    "  - Args: 1 disk CH map, disk CH maps list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31hr_B3_20140606_1746.fts'\n",
    "he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31hr_B3_20140626_1419.fts'\n",
    "# he_eqw_fits_path = NSO_SINGLE_DIR + 'svsm_e31lr_L3_20140626_1419.fts'\n",
    "\n",
    "# mag_fits_path = NSO_SINGLE_DIR + 'svsm_m11hr_B3_20140606_1605.fts'\n",
    "mag_fits_path = NSO_SINGLE_DIR + 'svsm_m11hr_B3_20140626_1444.fts'\n",
    "\n",
    "# E: Empty file\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140601_1836.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140603_1704.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140606_1755.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140612_1438.fts'\n",
    "# E ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140620_1711.fts'\n",
    "ch_fits_map = NSO_SINGLE_DIR + 'svsm_o1083_B3_20140626_1428.fts'\n",
    "\n",
    "# E sky_ch_fits_path = NSO_SINGLE_DIR + 'svsmgo1083_B3_20140606_1755.fts'\n",
    "sky_ch_fits_path = NSO_SINGLE_DIR + 'svsmgo1083_B3_20140626_1428.fts'\n",
    "\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    he_eqw_fits_path, header_list=['IMTYPE', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    mag_fits_path, header_list=['IMTYPE', 'COMMENT1', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray, plt.cm.gray],\n",
    "    # print_header=True\n",
    ")\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    ch_fits_map, header_list=['IMGTYP01', 'IMGTYP02', 'IMGTYP03'],\n",
    "    print_header=True\n",
    ")\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    sky_ch_fits_path, header_list=['COMMENT2'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NSO Processed EQW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = '~/Desktop/out_solarstrm_data/' + 'svsm_e31lr_L3_20140626_1419.fts'\n",
    "im_list = plot_detection.plot_raw_fits_content(\n",
    "    fits, header_list=['IMTYPE', 'COMMENT2'],\n",
    "    cmaps=[plt.cm.gray, plt.cm.gray, plt.cm.gray],\n",
    "    print_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2014_06_26__00_00'\n",
    "\n",
    "raw_nso_eqw = NSO_EQW_DICT[date_str]\n",
    "nso_eqw_nan = detect.pre_process_eqw_v0_1(raw_nso_eqw)[2]\n",
    "\n",
    "titles = ['EQW', 'EQW NaN']\n",
    "plot_detection.plot_hists(\n",
    "    [raw_nso_eqw, nso_eqw_nan], titles, semilogy=True\n",
    ")\n",
    "\n",
    "lower_bounds = [-1600,  0,  0]\n",
    "upper_bounds = [0,      250, 500]\n",
    "plot_detection.plot_thresholds(\n",
    "    nso_eqw_nan, bounds=[lower_bounds, upper_bounds], \n",
    "    bounds_as_percent=False, threshold_type='band'\n",
    ")\n",
    "plot_detection.plot_thresholds(\n",
    "    nso_eqw_nan, bounds=[75, 85, 100], bounds_as_percent=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Maps: **../merged/carr-daily/**\n",
    "Carrington Rotation CH Map: **svsm_o31hr_B3_cr{RRRR}_{DDD}.fts.gz**\n",
    "- 'Solar wind source high-res sine-latlon daily synoptic map'\n",
    "- Produced by **create_crmap**\n",
    "  - Args: 27 most recent disk CH maps, disk CH maps list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synoptic_ch_fits_path = NSO_MERGED_DIR + 'svsm_o31hr_B3_cr2152_275.fts'\n",
    "\n",
    "plot_detection.plot_raw_fits_content(\n",
    "    synoptic_ch_fits_path,\n",
    "    # header_list=['DATE', 'CARR01', 'IMTYPE'],\n",
    "    header_list=['IMGTYP01', 'IMGTYP02', 'IMGTYP03', 'IMGTYP04'],\n",
    "    # print_header=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm on Pre-Processed EQW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2014_06_26__00_00'\n",
    "\n",
    "raw_nso_eqw = NSO_EQW_DICT[date_str]\n",
    "nso_eqw_nan = detect.pre_process_eqw_v0_1(raw_nso_eqw)[2]\n",
    "\n",
    "percent_of_peak_list = [80,85,90]\n",
    "radius_list = [6]\n",
    "\n",
    "ensemble_map, holes_mask_list, confidence_list = detect.get_ensemble_v0_3(\n",
    "    nso_eqw_nan, percent_of_peak_list, radius_list)\n",
    "\n",
    "plot_detection.plot_ensemble(\n",
    "    nso_eqw_nan, ensemble_map, confidence_list, holes_mask_list\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSO Carrington Map Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work\n",
    "percent_of_peak_list = [80,90,100]\n",
    "radius_list = [11,13,15]\n",
    "\n",
    "for he_date_str in list(reversed(HE_DATE_LIST)):\n",
    "    raw_he = HE_FITS_DICT[date_str][0]\n",
    "    he = detect.pre_process_v0_1(raw_he)[0]\n",
    "    \n",
    "    ensemble_map = detect.get_ensemble(\n",
    "        he, percent_of_peak_list, radius_list\n",
    "    )[0]\n",
    "\n",
    "    euv = EUV_DICT[he_date_str]\n",
    "\n",
    "    plot_ensemble_comparison(he, he_date_str, ensemble_map, euv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ch_map(date_str_list, cr_str, ch_map_dict):\n",
    "    \"\"\"Plot NSO detected CH Carrington map.\n",
    "    \"\"\"\n",
    "    # Display selected column number corresponding to date list\n",
    "    selected_datetime_list = [\n",
    "        datetime.strptime(\n",
    "            date_str, DICT_DATE_STR_FORMAT)\n",
    "        for date_str in date_str_list\n",
    "    ]\n",
    "    selected_cr_list = [\n",
    "        carrington_rotation_number(selected_datetime)\n",
    "        for selected_datetime in selected_datetime_list\n",
    "    ]\n",
    "    \n",
    "    cr_str_list = cr_str.split('_')\n",
    "    cr_num_list = [float(cr_str) for cr_str in cr_str_list]\n",
    "    \n",
    "    cr_range = cr_num_list[-1] - cr_num_list[0]\n",
    "    cr_percent_list = [\n",
    "        (selected_cr - cr_num_list[0])/cr_range\n",
    "        for selected_cr in selected_cr_list\n",
    "    ]\n",
    "    \n",
    "    ch_map = ch_map_dict[cr_str]\n",
    "    rows, cols = ch_map.shape\n",
    "    \n",
    "    selected_col_list = [\n",
    "        cols - cr_percent*cols\n",
    "        for cr_percent in cr_percent_list\n",
    "    ]\n",
    "    \n",
    "    print('Selected Date Columns:')\n",
    "    for date_str, selected_col in zip(\n",
    "        date_str_list, selected_col_list):\n",
    "        print(f'{date_str}: {selected_col:.1f}px \\t', end='')\n",
    "\n",
    "    # Prepare the figure and axes with map projection\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_title(f'CR{cr_str}', fontsize=20)\n",
    "    \n",
    "    ax.imshow(ch_map, extent=[0,cols, rows, 0])\n",
    "    ax.vlines(x=selected_col_list, ymin=rows, ymax=0, linestyles='dashed',\n",
    "              colors='black')\n",
    "\n",
    "\n",
    "def rename_all_gong(gong_dir):\n",
    "    \"\"\"Rename all GONG magnetogram FITS files to include observation date in title\"\"\"\n",
    "    glob_pattern = gong_dir + '*.fits'\n",
    "    \n",
    "    fits_path_list = glob.glob(glob_pattern)\n",
    "    \n",
    "    for fits_path in fits_path_list:\n",
    "        gong_fits = fits.open(fits_path)\n",
    "        \n",
    "        gong_fits_header_keys = list(gong_fits[0].header.keys())\n",
    "                \n",
    "        # Pass to next FITS file if header information is missing\n",
    "        if 'CAR_ROT' not in gong_fits_header_keys:\n",
    "            continue\n",
    "        \n",
    "        # Carrington Rotation\n",
    "        CR_str = f'CR{gong_fits[0].header[\"CAR_ROT\"]}'\n",
    "        \n",
    "        gong_fits.close()\n",
    "            \n",
    "        os.rename(fits_path, gong_dir + CR_str + '.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_str = '2151.0342_2152.1035'\n",
    "\n",
    "plot_ch_map(list(reversed(HE_DATE_LIST)), cr_str, CH_MAP_DICT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-debugger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "9702f0bff29bacff409d5ed2ffa7f0a67aa5aa939df8fc4f21a3e6487ad9172c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
